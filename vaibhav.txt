nd;
and then define MAPPING as we would define type LIST (of elementtype) in
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (26 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
procedure MAKENULL ( var M: MAPPING );
var
i: domaintype;
begin
for i := firstvalue to lastvalue do
M[i] := undefined
end; { MAKENULL }
procedure ASSIGN ( var M: MAPPING;
d: domaintype; r: rangetype );
begin
M[d] := r
end; { ASSIGN }
function COMPUTE ( var M: MAPPING;
d: domaintype; var r: rangetype ): boolean;
begin
if M[d] = undefined then
return (false)
else begin
r := M[d];
return (true)
end
end; { COMPUTE }
Fig. 2.23. Array implementation of mappings.
whatever implementation of lists we choose. The three mapping commands are
defined in terms of commands on type LIST in Fig. 2.24.
2.6 Stacks and Recursive Procedures
One important application of stacks is in the implementation of recursive procedures
in programming languages. The run-time organization for a programming language
is the set of data structures used to represent the values of the program variables
during program execution. Every language that, like Pascal, allows recursive
procedures, uses a stack of activation records to record the values for all the
variables belonging to each active procedure of a program. When a procedure P is
called, a new activation record for P is placed on the stack, regardless of whether
there is already another activation record for P on the stack. When P returns, its
activation record must be on top of the stack, since P cannot return until all
procedures it has called have returned to P. Thus, we may pop the activation record
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (27 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
for this call of P to cause control to return to the point at which P was called (that
point, known as the return address, was placed in P's activation record when the call
to P
procedure MAKENULL ( var M: MAPPING );
{ same as for list }
procedure ASSIGN ( var M: MAPPING;
d: domaintype; r: rangetype );
var
x: elementtype; { the pair (d, r) }
p: position; { used to go from first to last position on list M }
begin
x.domain := d;
x.range := r;
p := FIRST(M);
while p <> END(M) do
if RETRIEVE(p, M).domain = d then
DELETE(p, M) { remove element with domain d }
else
p := NEXT(p, M);
INSERT(x, FIRST(M), M) { put (d, r) at front of list }
end; { ASSIGN }
function COMPUTE ( var M: MAPPING;
d: domaintype; var r: rangetype ): boolean;
var
p: position;
begin
p := FIRST(M);
while p <> END(M) do begin
if RETRIEVE(p, M).domain = d then begin
r := RETRIEVE(p, M).range;
return (true)
end;
p := NEXT(p, M)
end;
return (false) { if d is not in domain }
end; { COMPUTE }
Fig. 2.24. Mapping implementation in terms of lists.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (28 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
was made).
Recursion simplifies the structure of many programs. In some languages,
however, procedure calls are much more costly than assignment statements, so a
program may run faster by a large constant factor if we eliminate recursive procedure
calls from it. We do not advocate that recursion or other procedure calls be
eliminated habitually; most often the structural simplicity is well worth the running
time. However, in the most frequently executed portions of programs, we may wish
to eliminate recursion, and it is the purpose of this discussion to illustrate how
recursive procedures can be converted to nonrecursive ones by the introduction of a
user-defined stack.
Example 2.3. Let us consider recursive and nonrecursive solutions to a simplified
version of the classic knapsack problem in which we are given target t and a
collection of positive integer weights w1, w2 , . . . , wn. We are asked to determine
whether there is some selection from among the weights that totals exactly t. For
example, if t = 10, and the weights are 7, 5, 4, 4, and 1, we could select the second,
third, and fifth weights, since 5+4+ 1 = 10.
The image that justifies the name "knapsack problem" is that we wish to carry on
our back no more than t pounds, and we have a choice of items with given weights to
carry. We presumably find the items' utility to be proportional to their weight,† so
we wish to pack our knapsack as closely to the target weight as we can.
In Fig. 2.25 we see a function knapsack that operates on an array
weights : array [l..n] of integer.
A call to knapsack(s, i) determines whether there is a collection of the elements in
weight[i] through weight[n] that sums to exactly s, and prints these weights if so. The
first thing knapsack does is determine if it can respond immediately. Specifically, if s
= 0, then the empty set of weights is a solution. If s < 0, there can be no solution, and
if s > 0 and i > n, then we are out of weights to consider and therefore cannot find a
sum equal to s.
If none of these cases applies, then we simply call knapsack(s-wi, i + 1) to see if
there is a solution that includes wi. If there is such a solution, then the total problem
is solved, and the solution includes wi, so we print it. If there is no solution, then we
call knapsack(s, i + 1) to see if there is a solution that does not use wi.
Elimination of Tail Recursion
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (29 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
Often, we can eliminate mechanically the last call a procedure makes to itself. If a
procedure P(x) has, as its last step, a call to P(y), then we can replace the call to P(y)
by an assignment x := y, followed by a jump to the beginning of the code for P. Here,
y could be an expression, but x must be a parameter passed by value, so its value is
stored in a location private to this call to P ‡ P could have more than one parameter,
of course, and if so, they are each treated exactly as x and y above.
This change works because rerunning P with the new value of x has exactly the
same effect as calling P(y) and then returning from that call.
function knapsack ( target: integer; candidate: integer ): boolean;
begin
(1) if target = 0 then
(2) return (true)
(3) else if (target < 0) or (candidate > n) then
(4) return (false)
else { consider solutions with and without candidate }
(5) if knapsack(target - weights[candidate], candidate + 1)
then
begin
(6) writeln(weights[candidate]);
(7) return (true)
end
else { only possible solution is without candidate }
(8) return (knapsack(target, candidate + 1))
end; { knapsack }
Fig. 2.25. Recursive knapsack solution.
Notice that the fact that some of P's local variables have values the second time
around is of no consequence. P could not use any of those values, or had we called
P(y) as originally intended, the value used would not have been defined.
Another variant of tail recursion is illustrated by Fig. 2.25, where the last step of
the function knapsack just returns the result of calling itself with other parameters. In
such a situation, again provided the parameters are passed by value (or by reference
if the same parameter is passed to the call), we can replace the call by assignments to
the parameters and a jump to the beginning of the function. In the case of Fig. 2.25,
we can replace line (8) by
candidate:= candidate + 1;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (30 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
goto beginning
where beginning stands for a label to be assigned to statement (1). Note no change to
target is needed, since it is passed intact as the first parameter. In fact, we could
observe that since target has not changed, the tests of statements (1) and (3)
involving target are destined to fail, and we could instead skip statements (1) and
(2), test only for candidate > n at line (3), and proceed directly to line (5).
Complete Recursion Elimination
The tail recursion elimination procedure removes recursion completely only when
the recursive call is at the end of the procedure, and the call has the correct form.
There is a more general approach that converts any recursive procedure (or function)
into a nonrecursive one, but this approach introduces a user-defined stack. In general,
a cell of this stack will hold:
1. The current values of the parameters of the procedure;
2. The current values of any local variables of the procedure; and
3. An indication of the return address, that is, the place to which control returns
when the current invocation of the procedure is done.
In the case of the function knapsack, we can do something simpler. First, observe
that whenever we make a call (push a record onto the stack), candidate increases by
1. Thus, we can keep candidate as a global variable, incrementing it by one every
time we push the stack and decreasing it by one when we pop.
A second simplification we can make is to keep a modified "return address" on
the stack. Strictly speaking, the return address for this function is either a place in
some other procedure that calls knapsack, or the call at line (5), or the call at line (8).
We can represent these three conditions by a "status," which has one of three values:
1. none, indicating that the call is from outside the function knapsack,
2. included, indicating the call at line (5), which includes weights[candidate] in
the solution, or
3. excluded, indicating the call at line (8), which excludes weights[candidate].
If we store this status symbol as the return address, then we can treat target as a
global variable. When changing from status none to included, we subtract
weights[candidate] from target, and we add it back in when changing from status
included to excluded. To help represent the effect of the knapsack's return indicating
whether a solution has been found, we use a global winflag. Once set to true, winflag
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (31 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
remains true and causes the stack to be popped and those weights with status
included to be printed. With these modifications, we can declare our stack to be a list
of statuses, by
type
statuses = (none, included, excluded);
STACK = { suitable declaration for stack of statuses }
Figure 2.26 shows the resulting nonrecursive procedure knapsack operating on an
array weights as before. Although this procedure may be faster than the original
function knapsack, it is clearly longer and more difficult to understand. For this
reason, recursion elimination should be used only when speed is very important.
procedure knapsack ( target: integer );
var
candidate: integer;
winflag: boolean;
S: STACK;
begin
candidate := 1;
winflag := false;
MAKENULL(S);
PUSH(none, S); { initialize stack to consider weights[1] }
repeat
if winflag then begin
{ pop stack, printing weights included in solution }
if TOP(S) = included then
writeln(weights[candidate]);
candidate := candidate - 1;
POP(S)
end
else if target = 0 then begin { solution found }
winflag := true;
candidate := candidate - 1;
POP(S)
end
else if (((target < 0) and (TOP(S) = none))
or (candidate > n)) then begin
{ no solution possible with choices made }
candidate := candidate - 1;
POP(S)
end
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (32 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
else { no resolution yet; consider status of current candidate }
if TOP(S) = none then begin { first try including candidate }
target := target - weights[candidate];
candidate := candidate + 1;
POP(S); PUSH(included, S); PUSH(none, S)
end
else if TOP(S) = included then begin { try excluding candidate }
target := target + weights[candidate];
candidate := candidate + 1;
POP(S); PUSH(excluded, S); PUSH(none, S)
end
else begin { TOP(S) = excluded; give up on current choice }
POP(S);
candidate := candidate - 1
end
until EMPTY(S)
end; { knapsack }
Fig. 2.26. Nonrecursive knapsack procedure.
Exercises
2.1
Write a program to print the elements on a list. Throughout these
exercises use list operations to implement your programs.
2.2
Write programs to insert, delete, and locate an element on a sorted list
using
a. array,
b. pointer, and
c. cursor implementations of lists.
What is the running time of each of your programs?
2.3
Write a program to merge
a. two sorted lists,
b. n sorted lists.
2.4 Write a program to concatenate a list of lists.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (33 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
2.5
Suppose we wish to manipulate polynomials of the form p(x) = c1xe1 +
c2xe2 + . . . + cnxen, where e1 > e2 > . . . > en ³ 0. Such a polynomial can
be represented by a linked list in which each cell has three fields: one for
the coefficient ci, one for the exponent ei, and one for the pointer to the
next cell. Write a program to differentiate polynomials represented in this
manner.
2.6
Write programs to add and multiply polynomials of the form in Exercise
2.5. What is the running time of your programs as a function of the
number of terms?
*2.7
Suppose we declare cells by
type
celltype = record
bit: 0..1;
next: ­ celltype
end;
A binary number b1b2 . . . bn, where each bi is 0 or 1, has numerical value
. This number can be represented by the list b1, b2 , . . . , bn.
That list, in turn, can be represented as a linked list of cells of type
celltype. Write a procedure increment(bnumber) that adds one to the
binary number pointed to by bnumber. Hint: Make increment recursive.
2.8
Write a procedure to interchange the elements at positions p and
NEXT(p) in a singly linked list.
*2.9
The following procedure was intended to remove all occurrences of
element x from list L. Explain why it doesn't always work and suggest a
way to repair the procedure so it performs its intended task.
procedure delete ( x: elementtype; var L: LIST );
var
p: position;
begin
p := FIRST(L);
while p <> END(L) do begin
if RETRIEVE(p, L) = x then
DELETE(p, L);
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (34 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
p := NEXT(p, L)
end
end; { delete }
2.10
We wish to store a list in an array A whose cells consist of two fields,
data to store an element and position to give the (integer) position of the
element. An integer last indicates that A[1] through A[last] are used to
hold the list. The type LIST can be defined by
type
LIST = record
last: integer;
elements: array[1..maxlength] of record
data: elementtype;
position: integer
end
end;
Write a procedure DELETE(p, L) to remove the element at position p.
Include all necessary error checks.
2.11
Suppose L is a LIST and p, q, and r are positions. As a function of n, the
length of list L, determine how many times the functions FIRST, END,
and NEXT are executed by the following program.
p := FIRST(L);
while p <> END(L) do begin
q := p;
while q <> END(L) do begin
q := NEXT(q, L);
r := FIRST(L);
while r <> q do
r := NEXT(r, L)
end;
p := NEXT(p, L)
end;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (35 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
2.12
Rewrite the code for the LIST operations assuming a linked list
representation, but without a header cell. Assume true pointers are used
and position 1 is represented by nil.
2.13 Add the necessary error checks in the procedure of Fig. 2.12.
2.14
Another array representation of lists is to insert as in Section 2.2, but
when deleting, simply replace the deleted element by a special value
"deleted," which we assume does not appear on lists otherwise. Rewrite
the list operations to implement this strategy. What are the advantages
and disadvantages of the approach compared with our original array
representation of lists?
2.15
Suppose we wish to use an extra bit in queue records to indicate whether
a queue is empty. Modify the declarations and operations for a circular
queue to accommodate this feature. Would you expect the change to be
worthwhile?
2.16
A dequeue (double-ended queue) is a list from which elements can be
inserted or deleted at either end. Develop array, pointer, and cursor
implementations for a dequeue.
2.17
Define an ADT to support the operations ENQUEUE, DEQUEUE, and
ONQUEUE. ONQUEUE(x) is a function returning true or false
depending on whether x is or is not on the queue.
2.18
How would one implement a queue if the elements that are to be placed
on the queue are arbitrary length strings? How long does it take to
enqueue a string?
2.19
Another possible linked-list implementation of queues is to use no header
cell, and let front point directly to the first cell. If the queue is empty, let
front = rear = nil. Implement the queue operations for this
representation. How does this implementation compare with the list
implementation given for queues in Section 2.4 in terms of speed, space
utilization, and conciseness of the code?
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (36 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
2.20
A variant of the circular queue records the position of the front element
and the length of the queue.
a. Is it necessary in this implementation to limit the length of a
queue to maxlength - 1?
b. Write the five queue operations for this implementation.
c. Compare this implementation with the circular queue
implementation of Section 2.4.
2.21
It is possible to keep two stacks in a single array, if one grows from
position 1 of the array, and the other grows from the last position. Write a
procedure PUSH(x, S) that pushes element x onto stack S, where S is one
or the other of these two stacks. Include all necessary error checks in
your procedure.
2.22
We can store k stacks in a single array if we use the data structure
suggested in Fig. 2.27, for the case k = 3. We push and pop from each
stack as suggested in connection with Fig. 2.17 in Section 2.3. However,
if pushing onto stack i causes TOP(i) to equal BOTTOM(i-1), we first
move all the stacks so that there is an appropriate size gap between each
adjacent pair of stacks. For example, we might make the gaps above all
stacks equal, or we might make the gap above stack i proportional to the
current size of stack i (on the theory that larger stacks are likely to grow
sooner, and we want to postpone as long as possible the next
reorganization).
a. On the assumption that there is a procedure reorganize to call
when stacks collide, write code for the five stack operations.
b. On the assumption that there is a procedure makenewtops that
computes newtop[i], the "appropriate" position for the top of stack
i, for 1 £ i £ k, write the procedure reorganize. Hint. Note that
stack i could move up or down, and it is necessary to move stack i
before stack j if the new position of stack j overlaps the old
position of stack i. Consider stacks 1, 2 , . . . , k in order, but keep
a stack of "goals," each goal being to move a particular stack. If
on considering stack i, we can move it safely, do so, and then
reconsider the stack whose number is on top of the goal stack. If
we cannot safely move stack i, push i onto the goal stack.
c. What is an appropriate implementation for the goal stack in (b)?
Do we really need to keep it as a list of integers, or will a more
succinct representation do?
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (37 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
d. Implement makenewtops in such a way that space above each
stack is proportional to the current size of that stack.
e. What modifications of Fig. 2.27 are needed to make the
implementation work for queues? For general lists?
2.23
Modify the implementations of POP and ENQUEUE in Sections 2.3 and
2.4 to return the element removed from the stack or queue. What
modifications must be made if the element type is not a type that can be
returned by a function?
2.24
Use a stack to eliminate recursion from the following procedures.
a. function comb ( n, m: integer ): integer;
{ computes (
) assuming 0 £ m £ n and n
³ 1 }
begin
if (n = 1) or (m = 0) or (m = n)
then
return (1)
else
return (comb(n-1, m) + comb(n-1, m-1))
end; { comb }
Fig. 2.27. Many stacks in one array.
b. procedure reverse ( var L: LIST );
{ reverse list L }
var
x: elementtype;
begin
if not EMPTY(L) then begin
x := RETRIEVE(FIRST(L), L);
DELETE(FIRST(L), L);
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (38 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
reverse(L);
INSERT(x, END(L), L)
end
end; { reverse }
*2.25
Can we eliminate the tail recursion from the programs in Exercise 2.24?
If so, do it.
Bibliographic Notes
Knuth [1968] contains additional information on the implementation of lists, stacks,
and queues. A number of programming languages, such as LISP and SNOBOL,
support lists and strings in a convenient manner. See Sammet [1969], Nicholls
[1975], Pratt [1975], or Wexelblat [1981] for a history and description of many of
these languages.
† Strictly speaking, the type is "LIST of elementtype." However, the
implementations of lists we propose do not depend on what elementtype is; indeed, it
is that independence that justifies the importance we place on the list concept. We
shall use "LIST" rather than "LIST of elementtype," and similarly treat other ADT's
that depend on the types of elements.
† In this case, if we eliminate records that are "the same" we might wish to check
that the names and addresses are also equal; if the account numbers are equal but the
other fields are not, two people may have inadvertently gotten the same account
number. More likely, however, is that the same subscriber appears on the list more
than once with distinct account numbers and slightly different names and/or
addresses. In such cases, it is difficult to eliminate all duplicates.
† Even though L is not modified, we pass L by reference because frequently it will be
a big structure and we don't want to waste time copying it.
† Making the header a complete cell simplifies the implementation of list operation
in Pascal. We can use pointers for headers if we are willing to implement our
operations so they do insertions and deletions at the beginning of a list in a special
way. See the discussion under cursor-based implementation of lists in this section.
† Of course, there are many situations where we would like p to continue to
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (39 of 40) [1.7.2001 18:59:00]
Data Structures and Algorithms: CHAPTER 2: Basic Abstract DataTypes
represent the position of c.
† Incidentally, it is common practice to make the header of a doubly linked list be a
cell that effectively "completes the circle." That is, the header's previous field points
to the last cell and next points to the first cell. In this manner, we need not check for
nil pointers in Fig. 2.14.
† Note that "consecutive" must be taken in circular sense. That is, a queue of length
four could occupy the last two and first two positions of the array, for example.
† For example, firstvalue = 'A' and lastvalue = 'Z' if domaintype is 'A'..'Z'.
† In the "real" knapsack problem, we are given utility values as well as weights and
are asked to maximize the utility of the items carried, subject to a weight constraint.
‡ Alternatively, x could be passed by reference if y is x.
Table of Contents Go to Chapter 3
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1202.htm (40 of 40) [1.7.2001 18:59:00]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_1.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_1.gif [1.7.2001 18:59:08]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_2.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_2.gif [1.7.2001 18:59:34]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_3.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_3.gif [1.7.2001 18:59:42]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_4.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_4.gif [1.7.2001 18:59:47]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_5.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_5.gif [1.7.2001 18:59:58]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_9.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_9.gif [1.7.2001 19:00:09]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_10.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_10.gif [1.7.2001 19:00:21]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_11.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_11.gif [1.7.2001 19:00:32]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_12.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig1_12.gif [1.7.2001 19:00:35]
Data Structures and Algorithms: CHAPTER 3: Trees
Trees
A tree imposes a hierarchical structure on a collection of items. Familiar examples of
trees are genealogies and organization charts. Trees are used to help analyze
electrical circuits and to represent the structure of mathematical formulas. Trees also
arise naturally in many different areas of computer science. For example, trees are
used to organize information in database systems and to represent the syntactic
structure of source programs in compilers. Chapter 5 describes applications of trees
in the representation of data. Throughout this book, we shall use many different
variants of trees. In this chapter we introduce the basic definitions and present some
of the more common tree operations. We then describe some of the more frequently
used data structures for trees that can be used to support these operations efficiently.
3.1 Basic Terminology
A tree is a collection of elements called nodes, one of which is distinguished as a
root, along with a relation ("parenthood") that places a hierarchical structure on the
nodes. A node, like an element of a list, can be of whatever type we wish. We often
depict a node as a letter, a string, or a number with a circle around it. Formally, a tree
can be defined recursively in the following manner.
1. A single node by itself is a tree. This node is also the root of the tree.
2. Suppose n is a node and T1, T2, . . . , Tk are trees with roots n1, n2, . . . , nk,
respectively. We can construct a new tree by making n be the parent of nodes
n1, n2, . . . , nk. In this tree n is the root and T1, T2, . . . , Tk are the subtrees of
the root. Nodes n1, n2, . . . , nk are called the children of node n.
Sometimes, it is convenient to include among trees the null tree, a "tree" with no
nodes, which we shall represent by L.
Example 3.1. Consider the table of contents of a book, as suggested by Fig. 3.1(a).
This table of contents is a tree. We can redraw it in the manner shown in Fig. 3.1(b).
The parent-child relationship is depicted by a line. Trees are normally drawn topdown
as in Fig. 3.1(b), with the parent above the child.
The root, the node called "Book," has three subtrees with roots corresponding to
the chapters C1, C2, and C3. This relationship is represented by the lines downward
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (1 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
from Book to C1, C2, and C3. Book is the parent of C1, C2, and C3, and these three
nodes are the children of Book.
Fig. 3.1. A table of contents and its tree representation.
The third subtree, with root C3, is a tree of a single node, while the other two
subtrees have a nontrivial structure. For example, the subtree with root C2 has three
subtrees, corresponding to the sections s2.1, s2.2, and s2.3; the last two are one-node
trees, while the first has two subtrees corresponding to the subsections s2.1.1 and
s2.1.2.
Example 3.1 is typical of one kind of data that is best represented as a tree. In this
example, the parenthood relationship stands for containment; a parent node is
comprised of its children, as Book is comprised of C1, C2, and C3. Throughout this
book we shall encounter a variety of other relationships that can be represented by
parenthood in trees.
If n1, n2, . . . , nk is a sequence of nodes in a tree such that ni is the parent of ni+1
for 1 £ i < k, then this sequence is called a path from node n1 to node nk. The length
of a path is one less than the number of nodes in the path. Thus there is a path of
length zero from every node to itself. For example, in Fig. 3.1 there is a path of
length two, namely (C2, s2.1, s2.1.2) from C2 to s2.1.2.
If there is a path from node a to node b, then a is an ancestor of b, and b is a
descendant of a. For example, in Fig. 3.1, the ancestors of s2.1, are itself, C2, and
Book, while its descendants are itself, s2.1.1, and s2.1.2. Notice that any node is both
an ancestor and a descendant of itself.
An ancestor or descendant of a node, other than the node itself, is called a proper
ancestor or proper descendant, respectively. In a tree, the root is the only node with
no proper ancestors. A node with no proper descendants is called a leaf. A subtree of
a tree is a node, together with all its descendants.
The height of a node in a tree is the length of a longest path from the node to a
leaf. In Fig. 3.1 node C1 has height 1, node C2 height 2, and node C3 height 0. The
height of a tree is the height of the root. The depth of a node is the length of the
unique path from the root to that node.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (2 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
The Order of Nodes
The children of a node are usually ordered from left-to-right. Thus the two trees of
Fig. 3.2 are different because the two children of node a appear in a different order in
the two trees. If we wish explicitly to ignore the order of children, we shall refer to a
tree as an unordered tree.
Fig. 3.2. Two distinct (ordered) trees.
The "left-to-right" ordering of siblings (children of the same node) can be
extended to compare any two nodes that are not related by the ancestor-descendant
relationship. The relevant rule is that if a and b are siblings, and a is to the left of b,
then all the descendants of a are to the left of all the descendants of b.
Example 3.2. Consider the tree in Fig. 3.3. Node 8 is to the right of node 2, to the
left of nodes 9, 6, 10, 4, and 7, and neither left nor right of its ancestors 1, 3, and 5.
Fig. 3.3. A tree.
A simple rule, given a node n, for finding those nodes to its left and those to its
right, is to draw the path from the root to n. All nodes branching off to the left of this
path, and all descendants of such nodes, are to the left of n. All nodes and
descendants of nodes branching off to the right are to the right of n.
Preorder, Postorder, and Inorder
There are several useful ways in which we can systematically order all nodes of a
tree. The three most important orderings are called preorder, inorder and postorder;
these orderings are defined recursively as follows.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (3 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
l If a tree T is null, then the empty list is the preorder, inorder and postorder
listing of T.
l If T consists a single node, then that node by itself is the preorder, inorder,
and postorder listing of T.
Otherwise, let T be a tree with root n and subtrees T1, T2, . . . , Tk, as suggested in
Fig. 3.4.
Fig. 3.4. Tree T.
1. The preorder listing (or preorder traversal) of the nodes of T is the root n of
T followed by the nodes of T1 in preorder, then the nodes of T2 in preorder,
and so on, up to the nodes of Tk in preorder.
2. The inorder listing of the nodes of T is the nodes of T1 in inorder, followed by
node n, followed by the nodes of T2, . . . , Tk, each group of nodes in inorder.
3. The postorder listing of the nodes of T is the nodes of T1 in postorder, then
the nodes of T2 in postorder, and so on, up to Tk, all followed by node n.
Figure 3.5(a) shows a sketch of a procedure to list the nodes of a tree in preorder.
To make it a postorder procedure, we simply reverse the order of steps (1) and (2).
Figure 3.5(b) is a sketch of an inorder procedure. In each case, we produce the
desired ordering of the tree by calling the appropriate procedure on the root of the
tree.
Example 3.3. Let us list the tree of Fig. 3.3 in preorder. We first list 1 and then call
PREORDER on the first subtree of 1, the subtree with root 2. This subtree is a single
node, so we simply list it. Then we proceed to the second subtree of 1, the tree rooted
at 3. We list 3, and then call PREORDER on the first subtree of 3. That call results in
listing 5, 8, and 9, in that order.
procedure PREORDER ( n: node );
begin
(1) list n;
(2) for each child c of n, if any, in order from the left do
PREORDER(c)
end; { PREORDER }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (4 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
(a) PREORDER procedure.
procedure INORDER ( n: node );
begin
if n is a leaf then
list n;
else begin
INORDER(leftmost child of n);
list n;
for each child c of n, except for the leftmost,
in order from the left do
INORDER(c)
end
end; { INORDER }
(b) INORDER procedure.
Fig. 3.5. Recursive ordering procedures.
Continuing in this manner, we obtain the complete preorder traversal of Fig. 3.3: 1,
2, 3, 5, 8, 9, 6, 10, 4, 7.
Similarly, by simulating Fig. 3.5(a) with the steps reversed, we can discover that
the postorder of Fig. 3.3 is 2, 8, 9, 5, 10, 6, 3, 7, 4, 1. By simulating Fig. 3.5(b), we
find that the inorder listing of Fig. 3.3 is 2, 1, 8, 5, 9, 3, 10, 6, 7, 4.
A useful trick for producing the three node orderings is the following. Imagine we
walk around the outside of the tree, starting at the root, moving counterclockwise,
and staying as close to the tree as possible; the path we have in mind for Fig. 3.3 is
shown in Fig. 3.6.
For preorder, we list a node the first time we pass it. For postorder, we list a node
the last time we pass it, as we move up to its parent. For inorder, we list a leaf the
first time we pass it, but list an interior node the second time we pass it. For example,
node 1 in Fig. 3.6 is passed the first time at the beginning, and the second time while
passing through the "bay" between nodes 2 and 3. Note that the order of the leaves in
the three orderings is
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (5 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
Fig. 3.6. Traversal of a tree.
always the same left-to-right ordering of the leaves. It is only the ordering of the
interior nodes and their relationship to the leaves that vary among the three.
Labeled Trees and Expression Trees
Often it is useful to associate a label, or value, with each node of a tree, in the same
spirit with which we associated a value with a list element in the previous chapter.
That is, the label of a node is not the name of the node, but a value that is "stored" at
the node. In some applications we shall even change the label of a node, while the
name of a node remains the same. A useful analogy is tree:list = label:element =
node:position.
Example 3.4. Figure 3.7 shows a labeled tree representing the arithmetic expression
(a+b) * (a+c), where n1, . . . , n7 are the names of the nodes, and the labels, by
convention, are shown next to the nodes. The rules whereby a labeled tree represents
an expression are as follows:
1. Every leaf is labeled by an operand and consists of that operand alone. For
example, node n4 represents the expression a.
2. Every interior node n is labeled by an operator. Suppose n is labeled by a
binary operator q, such as + or *, and that the left child represents expression
E1 and the right child E2. Then n represents expression (E1) q (E2). We may
remove the parentheses if they are not necessary.
For example, node n2 has operator +, and its left and right children represent the
expressions a and b, respectively. Therefore, n2 represents (a)+(b), or just a+b. Node
n1 represents (a+b)*(a+c), since * is the label at n1, and a+b and a+c are the
expressions represented by n2 and n3, respectively.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (6 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
Fig. 3.7. Expression tree with labels.
Often, when we produce the preorder, inorder, or postorder listing of a tree, we
prefer to list not the node names, but rather the labels. In the case of an expression
tree, the preorder listing of the labels gives us what is known as the prefix form of an
expression, where the operator precedes its left operand and its right operand. To be
precise, the prefix expression for a single operand a is a itself. The prefix expression
for (E1) q (E2), with q a binary operator, is qP1P2, where P1 and P2 are the prefix
expressions for E1 and E2. Note that no parentheses are necessary in the prefix
expression, since we can scan the prefix expression qP1P2 and uniquely identify P1
as the shortest (and only) prefix of P1P2 that is a legal prefix expression.
For example, the preorder listing of the labels of Fig. 3.7 is *+ab+ac. The prefix
expression for n2, which is +ab, is the shortest legal prefix of +ab+ac.
Similarly, a postorder listing of the labels of an expression tree gives us what is
known as the postfix (or Polish) representation of an expression. The expression (E1)
q (E2) is represented by the postfix expression P1P2q, where P1 and P2 are the
postfix representations of E1 and E2, respectively. Again, no parentheses are
necessary in the postfix representation, as we can deduce what P2 is by looking for
the shortest suffix of P1P2 that is a legal postfix expression. For example, the postfix
expression for Fig. 3.7 is ab+ac+*. If we write this expression as P1P2*, then P2 is
ac+, the shortest suffix of ab+ac+ that is a legal postfix expression.
The inorder traversal of an expression tree gives the infix expression itself, but
with no parentheses. For example, the inorder listing of the labels of Fig. 3.7 is a+b *
a+c. The reader is invited to provide an algorithm for traversing an expression tree
and producing an infix expression with all needed pairs of parentheses.
Computing Ancestral Information
The preorder and postorder traversals of a tree are useful in obtaining ancestral
information. Suppose postorder(n) is the position of node n in a post-order listing of
the nodes of a tree. Suppose desc(n) is the number of proper descendants of node n.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (7 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
For example, in the tree of Fig. 3.7 the postorder numbers of nodes n2, n4, and n5 are
3, 1, and 2, respectively.
The postorder numbers assigned to the nodes have the useful property that the
nodes in the subtree with root n are numbered consecutively from postorder(n) -
desc(n) to postorder(n). To test if a vertex x is a descendant of vertex y, all we need
do is determine whether
postorder(y) - desc(y) £ postorder(x) £ postorder(y).
A similar property holds for preorder.
3.2 The ADT TREE
In Chapter 2, lists, stacks, queues, and mappings were treated as abstract data types
(ADT's). In this chapter trees will be treated both as ADT's and as data structures.
One of our most important uses of trees occurs in the design of implementations for
the various ADT's we study. For example, in Section 5.1, we shall see how a "binary
search tree" can be used to implement abstract data types based on the mathematical
model of a set, together with operations such as INSERT, DELETE, and MEMBER
(to test whether an element is in a set). The next two chapters present a number of
other tree implementations of various ADT's.
In this section, we shall present several useful operations on trees and show how
tree algorithms can be designed in terms of these operations. As with lists, there are a
great variety of operations that can be performed on trees. Here, we shall consider
the following operations:
1. PARENT(n, T). This function returns the parent of node n in tree T. If n is the
root, which has no parent, L is returned. In this context, L is a "null node,"
which is used as a signal that we have navigated off the tree.
2. LEFTMOST_CHILD(n, T) returns the leftmost child of node n in tree T, and
it returns L if n is a leaf, which therefore has no children.
3. RIGHT_SIBLING(n, T) returns the right sibling of node n in tree T, defined
to be that node m with the same parent p as n such that m lies immediately to
the right of n in the ordering of the children of p. For example, for the tree in
Fig. 3.7, LEFTMOST_CHILD(n2) = n4; RIGHT_SIBLING(n4) = n5, and
RIGHT_SIBLING (n5) = L.
4. LABEL(n, T) returns the label of node n in tree T. We do not, however,
require labels to be defined for every tree.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (8 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
5. CREATEi(v, T1, T2, . . . , Ti) is one of an infinite family of functions, one for
each value of i = 0, 1, 2, . . .. CREATEi makes a new node r with label v and
gives it i children, which are the roots of trees T1, T2, . . . , Ti, in order from
the left. The tree with root r is returned. Note that if i = 0, then r is both a leaf
and the root.
6. ROOT(T) returns the node that is the root of tree T, or L if T is the null tree.
7. MAKENULL(T) makes T be the null tree.
Example 3.5. Let us write both recursive and nonrecursive procedures to take a tree
and list the labels of its nodes in preorder. We assume that there are data types node
and TREE already defined for us, and that the data type TREE is for trees with labels
of the type labeltype. Figure 3.8 shows a recursive procedure that, given node n, lists
the labels of the subtree rooted at n in preorder. We call PREORDER(ROOT(T)) to
get a preorder listing of tree T.
procedure PREORDER ( n: node );
{ list the labels of the descendants of n in preorder }
var
c: node;
begin
print(LABEL(n, T));
c := LEFTMOST_CHILD(n, T);
while c <> L do
begin
PREORDER(c);
c := RIGHT_SIBLING(c, T)
end
end; { PREORDER }
Fig. 3.8. A recursive preorder listing procedure.
We shall also develop a nonrecursive procedure to print a tree in preorder. To find
our way around the tree, we shall use a stack S, whose type STACK is really "stack
of nodes." The basic idea underlying our algorithm is that when we are at a node n,
the stack will hold the path from the root to n, with the root at the bottom of the stack
and node n at the top.†
One way to perform a nonrecursive preorder traversal of a tree is given by the
program NPREORDER shown in Fig. 3.9. This program has two modes of
operation. In the first mode it descends down the leftmost unexplored path in the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (9 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
tree, printing and stacking the nodes along the path, until it reaches a leaf.
The program then enters the second mode of operation in which it retreats back
up the stacked path, popping the nodes of the path off the stack, until it encounters a
node on the path with a right sibling. The program then reverts back to the first mode
of operation, starting the descent from that unexplored right sibling.
The program begins in mode one at the root and terminates when the stack
becomes empty. The complete program is shown in Fig. 3.9.
3.3 Implementations of Trees
In this section we shall present several basic implementations for trees and discuss
their capabilities for supporting the various tree operations introduced in Section 3.2.
An Array Representation of Trees
Let T be a tree in which the nodes are named 1, 2, . . . , n. Perhaps the simplest
representation of T that supports the PARENT operation is a linear array A in which
entry A[i] is a pointer or a cursor to the parent of node i. The root of T can be
distinguished by giving it a null pointer or a pointer to itself as parent. In Pascal,
pointers to array elements are not feasible, so we shall have to use a cursor scheme
where A[i] = j if node j is the parent of node i, and A[i] = 0 if node i is the root.
This representation uses the property of trees that each node has a unique parent.
With this representation the parent of a node can be found in constant time. A path
going up the tree, that is, from node to parent to parent, and so on, can be traversed
in time proportional to the number of nodes on the path. We can also support the
LABEL operator by adding another array L, such that L[i] is the label of node i, or by
making the elements of array A be records consisting of an integer (cursor) and a
label.
Example 3.6. The tree of Fig. 3.10(a) has the parent representation given by the
array A shown in Fig. 3.10(b).
The parent pointer representation does not facilitate operations that require childof
information. Given a node n, it is expensive to determine the children of n, or the
height of n. In addition, the parent pointer representation does not specify the order
of the children of a node. Thus, operations like LEFTMOST_CHILD and
RIGHT_SIBLING are not well defined. We could impose an artificial order, for
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (10 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
example, by numbering the children of each node after numbering the parent, and
numbering the children in
procedure NPREORDER ( T: TREE );
{ nonrecursive preorder traversal of tree T }
var
m: node; { a temporary }
S: STACK; { stack of nodes holding path from the root
to the parent TOP(S) of the "current" node m }
begin
{ initialize }
MAKENULL(S);
m := ROOT(T);
while true do
if m < > L then begin
print(LABEL(m, T));
PUSH(m, S);
{ explore leftmost child of m }
m := LEFTMOST_CHILD(m, T)
end
else begin
{ exploration of path on stack
is now complete }
if EMPTY(S) then
return;
{ explore right sibling of node
on top of stack }
m := RIGHT_SIBLING(TOP(S), T);
POP(S)
end
end; { NPREORDER }
Fig. 3.9. A nonrecursive preorder procedure.
increasing order from left to right. On that assumption, we have written the function
RIGHT_SIBLING in Fig. 3.11, for types node and TREE that are defined as follows:
type
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (11 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
node = integer;
TREE = array [1..maxnodes] of node;
For this implementation we assume the null node L is represented by 0.
Fig. 3.10. A tree and its parent pointer representation.
function RIGHT_SIBLING ( n: node; T: TREE ): node;
{ return the right sibling of node n in tree T }
var
i, parent: node;
begin
parent: = T[n];
for i := n + 1 to maxnodes do
{ search for node after n with same parent }
if T[i] = parent then
return (i);
return (0) { null node will be returned
if no right sibling is ever found }
end; { RIGHT_SIBLING }
Fig. 3.11. Right sibling operation using array representation.
Representation of Trees by Lists of
Children
An important and useful way of representing trees is to form for each node a list of
its children. The lists can be represented by any of the methods suggested in Chapter
2, but because the number of children each node may have can be variable, the
linked-list representations are often more appropriate.
Figure 3.12 suggests how the tree of Fig. 3.10(a) might be represented. There is
an array of header cells, indexed by nodes, which we assume to be numbered 1, 2, . .
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (12 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
. , 10. Each header points to a linked list of "elements," which are nodes. The
elements on the list headed by header[i] are the children of node i; for example, 9
and 10 are the children of 3.
Fig. 3.12. A linked-list representation of a tree.
Let us first develop the data structures we need in terms of an abstract data type
LIST (of nodes), and then give a particular implementation of lists and see how the
abstractions fit together. Later, we shall see some of the simplifications we can
make. We begin with the following type declarations:
type
node = integer;
LIST = { appropriate definition for list of nodes };
position = { appropriate definition for positions in lists };
TREE = record
header: array [1..maxnodes] of LIST;
labels: array [1..maxnodes] of labeltype;
root: node
end;
We assume that the root of each tree is stored explicitly in the root field. Also, 0 is
used to represent the null node.
Figure 3.13 shows the code for the LEFTMOST_CHILD operation. The reader
should write the code for the other operations as exercises.
function LEFTMOST_CHILD ( n: node; T: TREE ): node;
{ returns the leftmost child of node n of tree T }
var
L: LIST; { shorthand for the list of n's children }
begin
L := T.header[n];
if EMPTY(L) then { n is a leaf }
return (0)
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (13 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
else
return (RETRIEVE(FIRST(L), L))
end; { LEFTMOST_CHILD }
Fig. 3.13. Function to find leftmost child.
Now let us choose a particular implementation of lists, in which both LIST and
position are integers, used as cursors into an array cellspace of records:
var
cellspace : array [1..maxnodes] of record
node: integer;
next: integer
end;
To simplify, we shall not insist that lists of children have header cells. Rather, we
shall let T.header [n] point directly to the first cell of the list, as is suggested by Fig.
3.12. Figure 3.14(a) shows the function LEFTMOST_CHILD of Fig. 3.13 rewritten
for this specific implementation. Figure 3.14(b) shows the operator PARENT, which
is more difficult to write using this representation of lists, since a search of all lists is
required to determine on which list a given node appears.
The Leftmost-Child, Right-Sibling
Representation
The data structure described above has, among other shortcomings, the inability to
create large trees from smaller ones, using the CREATEi operators. The reason is
that, while all trees share cellspace for linked lists of children, each has its own array
of headers for its nodes. For example, to implement CREATE2(v, T1, T2) we would
have to copy T1 and T2 into a third tree and add a new node with label v and two
children -- the roots of T1 and T2.
If we wish to build trees from smaller ones, it is best that the representation of
nodes from all trees share one area. The logical extension of Fig. 3.12 is to replace
the header array by an array nodespace consisting of records with
function LEFTMOST_CHILD ( n: node; T: TREE ): node;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (14 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
{ returns the leftmost child of node n on tree T }
var
L: integer; { a cursor to the beginning of the list of n's children }
begin
L := T.header[n];
if L = 0 then { n is a leaf }
return (0)
else
return (cellspace[L].node)
end; { LEFTMOST_CHILD }
(a) The function LEFTMOST_CHILD.
function PARENT ( n: node; T: TREE ): node;
{ returns the parent of node n in tree T }
var
p: node; { runs through possible parents of n }
i: position; { runs down list of p's children }
begin
for p := 1 to maxnodes do begin
i := T.header[p];
while i <> 0 do { see if n is among children of
p }
if cellspace[i].node = n then
return (p)
else
i := cellspace[i].next
end;
return (0) { return null node if parent not found }
end; { PARENT }
(b) The function PARENT.
Fig. 3.14. Two functions using linked-list representation of trees.
two fields label and header. This array will hold headers for all nodes of all trees.
Thus, we declare
var
nodespace : array [1..maxnodes] of record
label: labeltype;
header: integer; { cursor to cellspace }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (15 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
end;
Then, since nodes are no longer named 1, 2, . . . , n, but are represented by arbitrary
indices in nodespace, it is no longer feasible for the field node of cellspace to
represent the "number" of a node; rather, node is now a cursor into nodespace,
indicating the position of that node. The type TREE is simply a cursor into
nodespace, indicating the position of the root.
Example 3.7. Figure 3.15(a) shows a tree, and Fig. 3.15(b) shows the data structure
where we have placed the nodes labeled A, B, C, and D arbitrarily in positions 10, 5,
11, and 2 of nodespace. We have also made arbitrary choices for the cells of
cellspace used for lists of children.
Fig. 3.15. Another linked-list structure for trees.
The structure of Fig. 3.15(b) is adequate to merge trees by the CREATEi
operations. This data structure can be significantly simplified, however, First,
observe that the chains of next pointers in cellspace are really right-sibling pointers.
Using these pointers, we can obtain leftmost children as follows. Suppose
cellspace[i].node = n. (Recall that the "name" of a node, as opposed to its label, is in
effect its index in nodespace, which is what cellspace[i].node gives us.) Then
nodespace[n].header indicates the cell for the leftmost child of n in cellspace, in the
sense that the node field of that cell is the name of that node in nodespace.
We can simplify matters if we identify a node not with its index in nodespace, but
with the index of the cell in cellspace that represents it as a child. Then, the next
pointers of cellspace truly point to right siblings, and the information contained in
the nodespace array can be held by introducing a field leftmost_child in cellspace.
The datatype TREE becomes an integer used as a cursor to cellspace indicating the
root of the tree. We declare cellspace to have the following structure.
var
cellspace : array [1..maxnodes] of record
label: labeltype;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (16 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
leftmost_child: integer;
right_sibling: integer;
end;
Example 3.8. The tree of Fig. 3.15(a) is represented in our new data structure in Fig.
3.16. The same arbitrary indices as in Fig. 3.15(b) have been used for the nodes.
Fig. 3.16. Leftmost-child, right-sibling representation of a tree.
All operations but PARENT are straightforward to implement in the leftmostchild,
right-sibling representation. PARENT requires searching the entire cellspace.
If we need to perform the PARENT operation efficiently, we can add a fourth field to
cellspace to indicate the parent of a node directly.
As an example of a tree operation written to use the leftmost- child, right-sibling
structure as in Fig. 3.16, we give the function CREATE2 in Fig. 3.17. We assume
that unused cells are linked in an available space list, headed by avail, and that
available cells are linked by their right-sibling fields. Figure 3.18 shows the old
(solid) and the new (dashed) pointers.
function CREATE2 ( v: labeltype; T1, T2: integer ): integer;
{ returns new tree with root v, having T1 and T2 as subtrees }
var
temp: integer; { holds index of first available cell
for root of new tree }
begin
temp := avail;
avail := cellspace [avail].right_sibling;
cellspace[temp].leftmost_child := T1;
cellspace[temp].label := v;
cellspace[temp].right_sibling := 0;
cellspace[T1].right_sibling := T2;
cellspace[T2].right_sibling := 0; { not necessary;
that field should be 0 as the cell was formerly a root }
return (temp)
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (17 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
end; { CREATE2 }
Fig. 3.17. The function CREATE2.
Fig. 3.18. Pointer changes produced by CREATE2.
Alternatively, we can use less space but more time if we put in the right-sibling
field of the rightmost child a pointer to the parent, in place of the null pointer that
would otherwise be there. To avoid confusion, we need a bit in every cell indicating
whether the right-sibling field holds a pointer to the right sibling or to the parent.
Given a node, we find its parent by following right-sibling pointers until we find
one that is a parent pointer. Since all siblings have the same parent, we thereby find
our way to the parent of the node we started from. The time required to find a node's
parent in this representation depends on the number of siblings a node has.
3.4 Binary Trees
The tree we defined in Section 3.1 is sometimes called an ordered, oriented tree
because the children of each node are ordered from left-to-right, and because there is
an oriented path (path in a particular direction) from every node to its descendants.
Another useful, and quite different, notion of "tree" is the binary tree, which is either
an empty tree, or a tree in which every node has either no children, a left child, a
right child, or both a left and a right child. The fact that each child in a binary tree is
designated as a left child or as a right child makes a binary tree different from the
ordered, oriented tree of Section 3.1.
Example 3.9. If we adopt the convention that left children are drawn extending to
the left, and right children to the right, then Fig. 3.19 (a) and (b) represent two
different binary trees, even though both "look like" the ordinary (ordered, oriented)
tree of Fig. 3.20. However, let us emphasize that Fig. 3.19(a) and (b) are not the
same binary tree, nor are either in any sense equal to Fig. 3.20, for the simple reason
that binary trees are not directly comparable with ordinary trees. For example, in Fig.
3.19(a), 2 is the left child of 1, and 1 has no right child, while in Fig. 3.19(b), 1 has
no left child but has 2 as a right child. In either binary tree, 3 is the left child of 2,
and 4 is 2's right child.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (18 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
The preorder and postorder listings of a binary tree are similar to those of an
ordinary tree given on p. 78. The inorder listing of the nodes of a binary tree with
root n, left subtree T1 and right subtree T2 is the inorder listing of T1 followed by n
followed by the inorder listing of T2. For example, 35241 is the inorder listing of the
nodes of Fig. 3.19(a).
Representing Binary Trees
A convenient data structure for representing a binary tree is to name the nodes 1, 2, .
. . , n, and to use an array of records declared
var
cellspace : array [1..maxnodes] of record
leftchild: integer;
rightchild: integer;
end;
Fig. 3.19. Two binary trees.
Fig. 3.20. An "ordinary" tree.
The intention is that cellspace[i].leftchild is the left child of node i, and rightchild is
analogous. A value of 0 in either field indicates the absence of a child.
Example 3.10. The binary tree of Fig. 3.19(a) can be represented as shown in Fig.
3.21.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (19 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
An Example: Huffman Codes
Let us give an example of how binary trees can be used as a data structure. The
particular problem we shall consider is the construction of "Huffman codes."
Suppose we have messages consisting of sequences of characters. In each message,
the characters are independent and appear with a known
Fig. 3.21. Representation of a binary tree.
probability in any given position; the probabilities are the same for all positions. As
an example, suppose we have a message made from the five characters a, b, c, d, e,
which appear with probabilities .12, .4, .15, .08, .25, respectively.
We wish to encode each character into a sequence of 0's and 1's so that no code
for a character is the prefix of the code for any other character. This prefix property
allows us to decode a string of 0's and 1's by repeatedly deleting prefixes of the string
that are codes for characters.
Example 3.11. Figure 3.22 shows two possible codes for our five symbol alphabet.
Clearly Code 1 has the prefix property, since no sequence of three bits can be the
prefix of another sequence of three bits. The decoding algorithm for Code 1 is
simple. Just "grab" three bits at a time and translate each group of three into a
character. Of course, sequences 101, 110, and 111 are impossible, if the string of bits
really codes characters according to Code 1. For example, if we receive 001010011
we know the original message was bcd.
Fig. 3.22. Two binary codes.
It is easy to check that Code 2 also has the prefix property. We can decode a
string of bits by repeatedly "grabbing" prefixes that are codes for characters and
removing them, just as we did for Code 1. The only difference is that here, we cannot
slice up the entire sequence of bits at once, because whether we take two or three bits
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (20 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
for a character depends on the bits. For example, if a string begins 1101001, we can
again be sure that the characters coded were bcd. The first two bits, 11, must have
come from b, so we can remove them and worry about 01001. We then deduce that
the bits 01 came from c, and so on.
The problem we face is: given a set of characters and their probabilities, find a
code with the prefix property such that the average length of a code for a character is
a minimum. The reason we want to minimize the average code length is to compress
the length of an average message. The shorter the average code for a character is, the
shorter the length of the encoded message. For example, Code 1 has an average code
length of 3. This is obtained by multiplying the length of the code for each symbol
by the probability of occurrence of that symbol. Code 2 has an average length of 2.2,
since symbols a and d, which together appear 20% of the time, have codes of length
three, and the other symbols have codes of length two.
Can we do better than Code 2? A complete answer to this question is to exhibit a
code with the prefix property having an average length of 2.15. This is the best
possible code for these probabilities of symbol occurrences. One technique for
finding optimal prefix codes is called Huffman's algorithm. It works by selecting two
characters a and b having the lowest probabilities and replacing them with a single
(imaginary) character, say x, whose probability of occurrence is the sum of the
probabilities for a and b. We then find an optimal prefix code for this smaller set of
characters, using this procedure recursively. The code for the original character set is
obtained by using the code for x with a 0 appended as the code for a and with a 1
appended as a code for b.
We can think of prefix codes as paths in binary trees. Think of following a path
from a node to its left child as appending a 0 to a code, and proceeding from a node
to its right child as appending a 1. If we label the leaves of a binary tree by the
characters represented, we can represent any prefix code as a binary tree. The prefix
property guarantees no character can have a code that is an interior node, and
conversely, labeling the leaves of any binary tree with characters gives us a code
with the prefix property for these characters.
Example 3.12. The binary trees for Code 1 and Code 2 of Fig. 3.22 are shown in
Fig. 3.23(a) and (b), respectively.
We shall implement Huffman's algorithm using a forest (collection of trees), each
of which has its leaves labeled by characters whose codes we desire to select and
whose roots are labeled by the sum of the probabilities of all the leaf labels. We call
this sum the weight of the tree. Initially, each character is in a one-node tree by itself,
and when the algorithm ends, there will be only one tree, with all the characters at its
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (21 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
leaves. In this tree, the path from the root to any leaf represents the code for the label
of that leaf, according to the left = 0, right = 1 scheme of Fig. 3.23.
The essential step of the algorithm is to select the two trees in the forest that have
the smallest weights (break ties arbitrarily). Combine these two trees into one, whose
weight is the sum of the weights of the two trees. To combine the trees we create a
new node, which becomes the root and has the
Fig. 3.23. Binary trees representing codes with the prefix property.
roots of the two given trees as left and right children (which is which doesn't matter).
This process continues until only one tree remains. That tree represents a code that,
for the probabilities given, has the minimum possible average code length.
Example 3.13. The sequence of steps taken for the characters and probabilities in
our running example is shown in Fig. 3.24. From Fig. 3.24(e) we see the code words
for a, b, c, d, and e are 1111, 0, 110, 1110, and 10. In this example, there is only one
nontrivial tree, but in general, there can be many. For example, if the probabilities of
b and e were .33 and .32, then after Fig. 3.24(c) we would combine b and e, rather
than attaching e to the large tree as we did in Fig. 3.24(d).
Let us now describe the needed data structures. First, we shall use an array TREE
of records of the type
record
leftchild: integer;
rightchild: integer;
parent: integer;
end
to represent binary trees. Parent pointers facilitate finding paths from leaves to roots,
so we can discover the code for a character. Second, we use an array ALPHABET of
records of type
record
symbol: char;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (22 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
probability: real;
leaf: integer { cursor into tree }
end
(e) Final tree
Fig. 3.24. Steps in the construction of a Huffman tree.
to associate, with each symbol of the alphabet being encoded, its corresponding leaf.
This array also records the probability of each character. Third, we need an array
FOREST of records that represent the trees themselves. The type of these records is
record
weight: real;
root: integer { cursor into tree }
end
The initial values of all these arrays, assuming the data of Fig. 3.24(a), are shown in
Fig. 3.25. A sketch of the program to build the Huffman tree is shown in Fig. 3.26.
Fig. 3.25. Initial contents of arrays.
(1) while there is more then one tree in the forest do
begin
(2) i := index of the tree in FOREST with smallest weight;
(3) j := index of the tree in FOREST with second smallest weight;
(4) create a new node with left child FOREST[i].root and
right child FOREST[j].root;
(5) replace tree i in FOREST by a tree whose root
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (23 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
is the new node and whose weight is
FOREST[i].weight +
FOREST[j].weight;
(6) delete tree j from FOREST
end;
Fig. 3.26. Sketch of Huffman tree construction.
To implement line (4) of Fig. 3.26, which increases the number of cells of the
TREE array used, and lines (5) and (6), which decrease the number of utilized cells
of FOREST, we shall introduce cursors lasttree and lastnode, pointing to FOREST
and TREE, respectively. We assume that cells 1 to lasttree of FOREST and 1 to
lastnode of TREE are occupied.† We assume that arrays of Fig. 3.25 have some
declared lengths, but in what follows we omit comparisons between these limits and
cursor values.
procedure lightones ( var least, second: integer );
{ sets least and second to the indices in FOREST of
the trees of smallest weight. We assume lasttree ³2. }
var
i: integer;
begin { initialize least and second, considering first two trees }
if FOREST[1].weight < = FOREST[2].weight
then
begin least := 1; second := 2 end
else
begin least := 2; second := 1 end;
{ Now let i run from 3 to lasttree. At each iteration
least is the tree of smallest weight among the first i trees
in FOREST, and second is the next smallest of these }
for i := 3 to lasttree do
if FOREST[i].weight <
FOREST[least].weight then
begin second := least; least := i
end
else if FOREST[i].weight <
FOREST[second].weight then
second: = i
end; { lightones }
function create ( lefttree, righttree: integer ): integer;
{ returns new node whose left and right children are
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (24 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
FOREST[lefttree].root and
FOREST[righttree].root }
begin
lastnode := lastnode + 1;
{ cell for new node is TREE[lastnode] }
TREE[lastnode].leftchild : =
FOREST[lefttree].root;
TREE[lastnode].rightchild : =
FOREST[righttree].root;
{ now enter parent pointers for new node and its children }
TREE[lastnode].parent := 0;
TREE[FOREST[lefttree].root].parent :=
lastnode;
TREE[FOREST[righttree].root].parent :=
lastnode;
return(lastnode)
end; { create }
Fig. 3.27. Two procedures.
Figure 3.27 shows two useful procedures. The first implements lines (2) and (3)
of Fig. 3.26 to select indices of the two trees of smallest weight. The second is the
command create(n1, n2) that creates a new node and makes n1 and n2 its left and
right children.
Now the steps of Fig. 3.26 can be described in greater detail. A procedure
Huffman, which has no input or output, but works on the global structures of Fig.
3.25, is shown in Fig. 3.28.
procedure Huffman;
var
i, j: integer; { the two trees of least weight in FOREST }
newroot: integer;
begin
while lasttree > 1 do begin
lightones(i, j);
newroot := create(i, j);
{ Now replace tree i by the tree whose root is newroot }
FOREST[i].weight := FOREST[i].weight +
FOREST[j].weight;
FOREST[i].root := newroot;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (25 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
{ next, replace tree j, which is no longer needed, by lasttree,
and shrink FOREST by one }
FOREST[j] := FOREST[lasttree];
lasttree := lasttree - 1
end
end; { Huffman }
Fig. 3.28. Huffman's algorithm.
Figure 3.29 shows the data structure of Fig. 3.25 after lasttree has been reduced to
3, that is, when the forest looks like Fig. 3.24(c).
Fig. 3.29. Tree data structure after two iterations.
After completing execution of the algorithm, the code for each symbol can be
determined as follows. Find the symbol in the symbol field of the ALPHABET array.
Follow the leaf field of the same record to get to a record of the TREE array; this
record corresponds to the leaf for that symbol. Repeatedly follow the parent pointer
from the "current" record, say for node n, to the record of the TREE array for its
parent p. Remember node n, so it is possible to examine the leftchild and rightchild
pointers for node p and see which is n. In the former case, print 0, in the latter print
1. The sequence of bits printed is the code for the symbol, in reverse. If we wish the
bits printed in the correct order, we could push each onto a stack as we go up the
tree, and then repeatedly pop the stack, printing symbols as we pop them.
Pointer-Based Implementations of Binary
Trees
Instead of using cursors to point to left and right children (and parents if we wish),
we can use true Pascal pointers. For example, we might declare
type
node = record
leftchild: ­ node;
rightchild: ­ node;
parent: ­ node;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (26 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
end
For example, if we used this type for nodes of a binary tree, the function create of
Fig. 3.27 could be written as in Fig. 3.30.
function create ( lefttree, righttree: ­ node ): ­ node;
var
root: ­ node;
begin
new(root);
root ­.leftchild := lefttree;
root ­.rightchild :=
righttree;
root ­.parent := 0;
lefttree ­.parent := root;
righttree ­.parent := root;
return (root)
end; { create }
Fig. 3.30. Pointer-based implementation of binary trees.
Exercises
3.1
Answer the following questions about the tree of Fig. 3.31.
a. Which nodes are leaves?
b. Which node is the root?
c. What is the parent of node C?
d. Which nodes are children of C?
e. Which nodes are ancestors of E?
f. Which nodes are descendants of E?
g. What are the right siblings of D and E?
h. Which nodes are to the left and to the right of G?
i. What is the depth of node C?
j. What is the height of node C?
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (27 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
Fig. 3.31. A tree.
3.2
In the tree of Fig. 3.31 how many different paths of length three are
there?
3.3
Write programs to compute the height of a tree using each of the three
tree representations of Section 3.3.
3.4
List the nodes of Fig. 3.31 in
a. preorder,
b. postorder, and
c. inorder.
3.5
If m and n are two different nodes in the same tree, show that exactly one
of the following statements is true:
a. m is to the left of n
b. m is to the right of n
c. m is a proper ancestor of n
d. m is a proper descendant of n.
3.6
Place a check in row i and column j if the two conditions represented by
row i and column j can occur simultaneously.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (28 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
For example, put a check in row 3 and column 2 if you believe that n can
be a proper ancestor of m and at the same time n can precede m in
inorder.
3.7
Suppose we have arrays PREORDER[n], INORDER[n], and
POSTORDER[n] that give the preorder, inorder, and postorder positions,
respectively, of each node n of a tree. Describe an algorithm that tells
whether node i is an ancestor of node j, for any pair of nodes i and j.
Explain why your algorithm works.
*3.8
We can test whether a node m is a proper ancestor of a node n by testing
whether m precedes n in X-order but follows n in Y-order, where X and
Y are chosen from {pre, post, in}. Determine all those pairs X and Y for
which this statement holds.
3.9
Write programs to traverse a binary tree in
a. preorder,
b. postorder,
c. inorder.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (29 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
3.10
The level-order listing of the nodes of a tree first lists the root, then all
nodes of depth 1, then all nodes of depth 2, and so on. Nodes at the same
depth are listed in left-to-right order. Write a program to list the nodes of
a tree in level-order.
3.11
Convert the expression ((a + b) + c * (d + e) + f) * (g + h) to a
a. prefix expression
b. postfix expression.
3.12
Draw tree representations for the prefix expressions
a. *a + b*c + de
b. *a + *b + cde
3.13
Let T be a tree in which every nonleaf node has two children. Write a
program to convert
a. a preorder listing of T into a postorder listing,
b. a postorder listing of T into a preorder listing,
c. a preorder listing of T into an inorder listing.
3.14
Write a program to evaluate
a. preorder
b. postorder
arithmetic expressions.
3.15
We can define a binary tree as an ADT with the binary tree structure as a
mathematical model and with operations such as LEFTCHILD(n),
RIGHTCHILD(n), PARENT(n), and NULL(n). The first three operations
return the left child, the right child, and the parent of node n (L if there is
none) and the last returns true if and only if n is L. Implement these
procedures using the binary tree representation of Fig. 3.21.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (30 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
3.16
Implement the seven tree operations of Section 3.2 using the following
tree implementations:
a. parent pointers
b. lists of children
c. leftmost-child, right-sibling pointers.
3.17
The degree of a node is the number of children it has. Show that in any
binary tree the number of leaves is one more than the number of nodes of
degree two.
3.18
Show that the maximum number of nodes in a binary tree of height h is
2h+1 - 1. A binary tree of height h with the maximum number of nodes is
called a full binary tree.
*3.19
Suppose trees are implemented by leftmost-child, right-sibling and parent
pointers. Give nonrecursive preorder, postorder, and inorder traversal
algorithms that do not use "states" or a stack, as Fig. 3.9 does.
3.20
Suppose characters a, b, c, d, e, f have probabilities .07, .09, .12, .22, .23,
.27, respectively. Find an optimal Huffman code and draw the Huffman
tree. What is the average code length?
*3.21
Suppose T is a Huffman tree, and that the leaf for symbol a has greater
depth than the leaf for symbol b. Prove that the probability of symbol b is
no less than that of a.
*3.22
Prove that Huffman's algorithm works, i.e., it produces an optimal code
for the given probabilities. Hint: Use Exercise 3.21.
Bibliographic Notes
Berge [1958] and Harary [1969] discuss the mathematical properties of trees. Knuth
[1973] and Nievergelt [1974] contain additional information on binary search trees.
Many of the works on graphs and applications referenced in Chapter 6 also cover
material on trees.
The algorithm given in Section 3.4 for finding a tree with a minimal weighted
path length is from Huffman [1952]. Parker [1980] gives some more recent
explorations into that algorithm.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (31 of 32) [1.7.2001 19:01:17]
Data Structures and Algorithms: CHAPTER 3: Trees
† Recall our discussion of recursion in Section 2.6 in which we illustrated how the
implementation of a recursive procedure involves a stack of activation records. If we
examine Fig. 3.8, we can observe that when PREORDER(n) is called, the active
procedure calls, and therefore the stack of activation records, correspond to the calls
of PREORDER for all the ancestors of n. Thus our nonrecursive preorder procedure,
like the example in Section 2.6, models closely the way the recursive procedure is
implemented.
† For the data reading phase, which we omit, we also need a cursor for the array
ALPHABET as it fills with symbols and their probabilities.
Table of Contents Go to Chapter 4
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1203.htm (32 of 32) [1.7.2001 19:01:17]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_2.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_2.gif [1.7.2001 19:01:28]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_4.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_4.gif [1.7.2001 19:01:33]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_7.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_7.gif [1.7.2001 19:01:41]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_8.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_8.gif [1.7.2001 19:01:45]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_9.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_9.gif [1.7.2001 19:02:03]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_10.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_10.gif [1.7.2001 19:02:10]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_13.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_13.gif [1.7.2001 19:02:14]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_15.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_15.gif [1.7.2001 19:02:20]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_17.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_17.gif [1.7.2001 19:02:30]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_20.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_20.gif [1.7.2001 19:02:46]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_21.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_21.gif [1.7.2001 19:02:55]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_27.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig2_27.gif [1.7.2001 19:03:22]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
Basic Operations on Sets
The set is the basic structure underlying all of mathematics. In algorithm design, sets
are used as the basis of many important abstract data types, and many techniques have
been developed for implementing set-based abstract data types. In this chapter we
review the basic operations on sets and introduce some simple implementations for
sets. We present the "dictionary" and "priority queue," two abstract data types based
on the set model. Implementations for these abstract data types are covered in this and
the next chapter.
4.1 Introduction to Sets
A set is a collection of members (or elements); each member of a set either is itself a
set or is a primitive element called an atom. All members of a set are different, which
means no set can contain two copies of the same element.
When used as tools in algorithm and data structure design, atoms usually are
integers, characters, or strings, and all elements in any one set are usually of the same
type. We shall often assume that atoms are linearly ordered by a relation, usually
denoted "<" and read "less than" or "precedes." A linear order < on a set S satisfies
two properties:
1. For any a and b in S, exactly one of a < b, a = b, or b < a is true.
2. For all a, b, and c in S, if a < b and b < c, then a < c (transitivity).
Integers, reals, characters, and character strings have a natural linear ordering for
which < is used in Pascal. A linear ordering can be defined on objects that consist of
sets of ordered objects. We leave as an exercise how one develops such an ordering.
For example, one question to be answered in constructing a linear order for a set of
integers is whether the set consisting of integers 1 and 4 should be regarded as being
less than or greater than the set consisting of 2 and 3.
Set Notation
A set of atoms is generally exhibited by putting curly brackets around its members, so
{1, 4} denotes the set whose only members are 1 and 4. We should bear in mind that
a set is not a list, even though we represent sets in this manner as if they were lists.
The order in which the elements of a set are listed is irrelevant, and we could just as
well have written {4, 1} in place of {1, 4}. Note also that in a set each element
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (1 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
appears exactly once, so {1, 4, 1} is not a set.†
Sometimes we represent sets by set formers, which are expressions of the form
{ x | statement about x }
where the statement about x is a predicate that tells us exactly what is needed for an
arbitrary object x to be in the set. For example, {x | x is a positive integer and x £
1000} is another way of representing {1, 2, . . . , 1000}, and {x | for some integer y, x
= y2} denotes the set of perfect squares. Note that the set of perfect squares is infinite
and cannot be represented by listing its members.
The fundamental relationship of set theory is membership, which is denoted by the
symbol Î. That is, x Î A means that element x is a member of set A; the element x
could be an atom or another set, but A cannot be an atom. We use x Ï A for "x is not a
member of A." There is a special set, denoted Ø and called the null set or empty set,
that has no members. Note that Ø is a set, not an atom, even though the set Ø does not
have any members. The distinction is that x Î Ø is false for every x, whereas if y is an
atom, then x Î y doesn't even make sense; it is syntactically meaningless rather than
false.
We say set A is included (or contained) in set B, written A Í B, or B Ê A, if every
member of A is also a member of B. We also say A is a subset of B and B is a superset
of A, if A Í B. For example, {1, 2} Í {1, 2, 3}, but {1, 2, 3} is not a subset of {1, 2}
since 3 is a member of the former but not the latter. Every set is included in itself, and
the empty set is included in every set. Two sets are equal if each is included in the
other, that is, if their members are the same. Set A is a proper subset or proper
superset of set B if A ¹ B, and A Í B or A Ê B, respectively.
The most basic operations on sets are union, intersection, and difference. If A and
B are sets, then A È B, the union of A and B, is the set of elements that are members
of A or B or both. The intersection of A and B, written A Ç B, is the set of elements in
both A and B, and the difference, A - B, is the set of elements in A that are not in B.
For example, if A = {a, b, c} and B = {b, d}, then A È B = {a, b, c, d}, A Ç B = {b},
and A - B = {a, c}.
Abstract Data Types Based on Sets
We shall consider ADT's that incorporate a variety of set operations. Some
collections of these operations have been given special names and have special
implementations of high efficiency. Some of the more common set operations are the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (2 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
following.
1. -3. The three procedures UNION(A, B, C), INTERSECTION(A, B, C), and
DIFFERENCE(A, B, C) take set-valued arguments A and B, and assign the
result, A È B, A Ç B, or A - B, respectively, to the set variable C.
4. We shall sometimes use an operation called merge, or disjoint set union, that is
no different from union, but that assumes its operands are disjoint (have no
members in common). The procedure MERGE(A, B, C) assigns to the set
variable C the value A È B, but is not defined if A Ç B ¹ Ø, i.e., if A and B are
not disjoint.
5. The function MEMBER(x, A) takes set A and object x, whose type is the type
of elements of A, and returns a boolean value -- true if x Î A and false if x Ï A.
6. The procedure MAKENULL(A) makes the null set be the value for set
variable A.
7. The procedure INSERT(x, A), where A is a set-valued variable, and x is an
element of the type of A's members, makes x a member of A. That is, the new
value of A is A È {x}. Note that if x is already a member of A, then INSERT(x,
A) does not change A.
8. DELETE(x, A) removes x from A, i.e., A is replaced by A - {x}. If x is not in A
originally, DELETE(x, A) does not change A.
9. ASSIGN(A, B) sets the value of set variable A to be equal to the value of set
variable B.
10. The function MIN(A) returns the least element in set A. This operation may be
applied only when the members of the parameter set are linearly ordered. For
example, MIN({2, 3, 1}) = 1 and MIN({'a','b','c'}) = 'a'. We also use a function
MAX with the obvious meaning.
11. EQUAL(A, B) is a function whose value is true if and only if sets A and B
consist of the same elements.
12. The function FIND(x) operates in an environment where there is a collection
of disjoint sets. FIND(x) returns the name of the (unique) set of which x is a
member.
4.2 An ADT with Union, Intersection, and
Difference
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (3 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
We begin by defining an ADT for the mathematical model "set" with the three basic
set-theoretic operations, union, intersection, and difference. First we give an example
where such an ADT is useful and then we discuss several simple implementations of
this ADT.
Example 4.1. Let us write a program to do a simple form of "data-flow analysis" on
flowcharts that represent procedures. The program will use variables of an abstract
data type SET, whose operations are UNION, INTERSECTION, DIFFERENCE,
EQUAL, ASSIGN, and MAKENULL, as defined in the previous section.
In Fig. 4.1 we see a flowchart whose boxes have been named B1, . . . , B8, and for
which the data definitions (read and assignment statements) have been numbered 1, 2,
. . . , 9. This flowchart happens to implement the Euclidean algorithm, to compute the
greatest common divisor of inputs p and q, but the details of the algorithm are not
relevant to the example.
In general, data-flow analysis refers to that part of a compiler that examines a
flowchart-like representation of a source program, such as Fig. 4.1, and collects
information about what can be true as control reaches each box of the flowchart. The
boxes are often called blocks or basic blocks, and they represent collections of
statements through which the flow-of-control proceeds sequentially. The information
collected during data-flow analysis is used to help improve the code generated by the
compiler. For example, if data-flow analysis told us that each time control reached
block B, variable x had the value 27, then we could substitute 27 for all uses of x in
block B, unless x were assigned a new value within block B. If constants can be
accessed more quickly than variables, this change could speed up the code produced
by the compiler.
In our example, we want to determine where a variable could last have been given
a new value. That is, we want to compute for each block Bi the set DEFIN[i] of data
definitions d such that there is a path from B1 to Bi in which d appears, but is not
followed by any other definition of the same variable as d defines. DEFIN[i] is called
the set of reaching definitions for Bi.
To see how such information could be useful, consider Fig. 4.1. The first block B1
is a "dummy" block of three data definitions, making the three variables t, p, and q
have "undefined" values. If we discover, for example, that DEFIN[7] includes
definition 3, which gives q an undefined value, then the program might contain a bug,
as apparently it could print q without first assigning a valid value to q. Fortunately,
we shall discover that it is impossible to reach block B7 without assigning to q; that is,
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (4 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
3 is not in DEFIN[7].
The computation of the DEFIN[i]'s is aided by several rules. First, we precompute
for each block i two sets GEN[i] and KILL[i]. GEN[i] is the set of data definitions in
block i, with the exception that if Bi contains two or more definitions of variable x,
then only the last is in GEN[i]. Thus, GEN[i] is the set of definitions in Bi that are
"generated" by Bi; they reach the end of Bi without having their variables redefined.
The set KILL[i] is the set of definitions d not in Bi such that Bi has a definition of
the same variable as d. For example, in Fig. 4.1, GEN[4] = {6}, since definition 6 (of
variable t) is in B4 and there are no subsequent definitions of t in B4. KILL[4] = {l, 9},
since these are the definitions of variable t that are not in B4.
Fig. 4.1. A flowchart of the Euclidean algorithm.
In addition to the DEFIN[i]'s, we compute the set DEFOUT[i] for each block Bi.
Just as DEFIN[i] is the set of definitions that reach the beginning of Bi, DEFOUT[i] is
the set of definitions reaching the end of Bi. There is a simple formula relating DEFIN
and DEFOUT, namely
That is, definition d reaches the end of Bi if and only if it either reaches the beginning
of Bi and is not killed by Bi, or it is generated in Bi. The second rule relating DEFIN
and DEFOUT is that DEFIN[i] is the union, over all predecessors p of Bi, of
DEFOUT[p], that is:
Rule (4.2) says that a data definition enters Bi if and only if it reaches the end of one
of Bi's predecessors. As a special case, if Bi has no predecessors, as B1 in Fig. 4.1,
then DEFIN[i] = Æ.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (5 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
Because we have introduced a variety of new concepts in this example, we shall
not try to complicate matters by writing a general algorithm for computing the
reaching definitions of an arbitrary flowgraph. Rather, we shall write a part of a
program that assumes GEN[i] and KILL[i] are available for i = 1, . . . , 8 and
computes DEFIN[i] and DEFOUT[i] for 1, . . . , 8, assuming the particular flowgraph
of Fig. 4.1. This program fragment assumes the existence of an ADT SET with
operations UNION, INTERSECTION, DIFFERENCE, EQUAL, ASSIGN, and
MAKENULL; we shall give alternative implementations of this ADT later.
The procedure propagate( GEN, KILL, DEFIN, DEFOUT) applies rule (4.1) to
compute DEFOUT for a block, given DEFIN. If a program were loop-free, then the
calculation of DEFOUT would be straightforward. The presence of a loop in the
program fragment of Fig. 4.2 necessitates an iterative procedure. We approximate
DEFIN[i] by starting with DEFIN[i] = Ø and DEFOUT[i] = GEN[i] for all i, and then
repeatedly apply (4.1) and (4.2) until no more changes to DEFIN's and DEFOUT's
occur. Since each new value assigned to DEFIN[i] or DEFOUT[i] can be shown to be
a superset (not necessarily proper) of its former value, and there are only a finite
number of data definitions in any program, the process must converge eventually to a
solution to (4.1) and (4.2).
The successive values of DEFIN[i] after each iteration of the repeat-loop are
shown in Fig. 4.3. Note that none of the dummy assignments 1, 2, and 3 reaches a
block where their variable is used, so there are no undefined variable uses in the
program of Fig. 4.1. Also note that by deferring the application of (4.2) for Bi until
just before we apply (4.1) for Bi would make the process of Fig. 4.2 converge in
fewer iterations in general.
4.3 A Bit-Vector Implementation of Sets
The best implementation of a SET ADT depends on the operations to be performed
and on the size of the set. When all sets in our domain of discourse are subsets of a
small "universal set" whose elements are the integers 1, . . . , N for some fixed N, then
we can use a bit-vector (boolean array) implementation. A set is represented by a bit
vector in which the ith bit is true if i is an element of the set. The major advantage of
this representation
var
GEN, KILL, DEFIN, DEFOUT: array[l..8] of SET;
{ we assume GEN and KILL are computed externally }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (6 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
i: integer;
changed: boolean;
procedure propagate ( G, K, I: SET; var
O: SET );
{ apply (4.1) and set changed to true if
a change in DEFOUT is detected }
var
TEMP: SET;
begin
DIFFERENCE(I, K, TEMP);
UNION(TEMP, G, TEMP);
if not EQUAL(TEMP, O) do begin
ASSIGN(O, TEMP);
changed := true
end
end; { propagate }
begin
for i:= 1 to 8 do
ASSIGN(DEFOUT[i], GEN[i]);
repeat
changed := false;
{the next 8 statements apply (4.2) for the
graph of Fig. 4.1 only}
MAKENULL(DEFIN[1]);
ASSIGN(DEFIN[2], DEFOUT[1]);
ASSIGN(DEFIN[3], DEFOUT[2]);
ASSIGN(DEFIN[4], DEFOUT[3]);
UNION(DEFOUT[4], DEFOUT[8], DEFIN[5]);
UNION(DEFOUT[3], DEFOUT[5], DEFIN[6]);
ASSIGN(DEFIN[7], DEFOUT[6]);
ASSIGN(DEFIN[8], DEFOUT[6]);
for i:= 1 to 8 do
propagate(GEN[i], KILL[i],
DEFIN[i], DEFOUT[i]);
until
not changed
end.
Fig. 4.2. Program to compute reaching definitions.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (7 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
Fig. 4.3. DEFIN[i] after each iteration.
is that MEMBER, INSERT, and DELETE operations can be performed in constant
time by directly addressing the appropriate bit. UNION, INTERSECTION, and
DIFFERENCE can be performed in time proportional to the size of the universal set.
If the universal set is sufficiently small so that a bit vector fits in one computer
word, then UNION, INTERSECTION, and DIFFERENCE can be performed by
single logical operations in the language of the underlying machine. Certain small sets
can be represented directly in Pascal using the set construct. The maximum size of
such sets depends on the particular compiler used and, unfortunately, it is often too
small to be used in typical set problems. However, in writing our own programs, we
need not be constrained by any limit on our set size, as long as we can treat our sets as
subsets of some universal set {1, . . . , N}. We intend that if A is a set represented as a
boolean array, then A[i] is true if and only if element i is a member of A. Thus, we can
define an ADT SET by the Pascal declaration
const
N = { whatever value is appropriate };
type
SET = packed array[1..N] of boolean;
We can then implement the procedure UNION as shown in Fig. 4.4. To implement
INTERSECTION and DIFFERENCE, we replace "or" in Fig. 4.4 by "and" and "and
not," respectively. The reader can implement the other operations mentioned in
Section 4.1 (except MERGE and FIND, which make little sense in this context) as
easy exercises.
It is possible to use the bit-vector implementation of sets when the universal set is
a finite set other than a set of consecutive integers. Normally, we would then need a
way to translate between members of the universal set and the integers 1, . . . , N.
Thus, in Example 4.1 we assumed that the data definitions were assigned numbers
from 1 to 9. In general, the translations in
procedure UNION ( A, B: SET; var C: SET );
var
i: integer;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (8 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
begin
for i := 1 to N do
C[i] := A[i] or B[i]
end
Fig. 4.4. Implementation of UNION.
both directions could be performed by the MAPPING ADT described in Chapter 2.
However, the inverse translation from integers to elements of the universal set can be
accomplished better using an array A, where A[i] is the element corresponding to
integer i.
4.4 A Linked-List Implementation of Sets
It should also be evident that sets can be represented by linked lists, where the items
in the list are the elements of the set. Unlike the bit-vector representation, the list
representation uses space proportional to the size of the set represented, not the size of
the universal set. Moreover, the list representation is somewhat more general since it
can handle sets that need not be subsets of some finite universal set.
When we have operations like INTERSECTION on sets represented by linked
lists, we have several options. If the universal set is linearly ordered, then we can
represent a set by a sorted list. That is, we assume all set members are comparable by
a relation "<" and the members of a set appear on a list in the order e1, e2, . . . , en,
where e1 < e2 < e3 < . . . < en. The advantage of a sorted list is that we do not need to
search the entire list to determine whether an element is on the list.
An element is in the intersection of lists L1 and L2 if and only if it is on both lists.
With unsorted lists we must match each element on L1 with each element on L2, a
process that takes O(n2) steps on lists of length n. The reason that sorting the lists
makes intersection and some other operations easy is that if we wish to match an
element e on one list L1 with the elements of another list L2, we have only to look
down L2 until we either find e, or find an element greater than e; in the first case we
have found the match, while in the second case, we know none exists. More
importantly, if d is the element on L1 that immediately precedes e, and we have found
onL2 the first element, say f, such that d £ f, then to search L2 for an occurrence of e
we can begin with f. The conclusion from this reasoning is that we can find matches
for all the elements of L1, if they exist, by scanning L1 and L2 once, provided we
advance the position markers for the two lists in the proper order, always advancing
the one with the smaller element. The routine to implement INTERSECTION is
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (9 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
shown in Fig. 4.5. There, sets are represented by linked lists of "cells" whose type is
defined
type
celltype = record
element: elementtype;
next: ­ celltype
end
Figure 4.5 assumes elementtype is a type, such as integer, that can be compared by <.
If not, we have to write a function that determines which of two elements precedes
the other.
The linked lists in Fig. 4.5 are headed by empty cells that serve as entry points to
the lists. The reader may, as an exercise, write this program in a more general abstract
form using list primitives. The program in Fig. 4.5, however, may be more efficient
than the more abstract program. For example, Fig. 4.5 uses pointers to particular cells
rather than "position" variables that point to previous cells. We can do so because we
only append to list C, and A and B are only scanned, with no insertions or deletions
done on those lists.
The operations of UNION and DIFFERENCE can be written to look surprisingly
like the INTERSECTION procedure of Fig. 4.5. For UNION, we must attach all
elements from either the A or B list to the C list, in their proper, sorted order, so when
the elements are unequal (lines 12-14), we add the smaller to the C list just as we do
when the elements are equal. We also append to list C all elements on the list not
exhausted when the test of line (5) fails. For DIFFERENCE we do not add an element
to the C list when equal elements are found. We only add the current A list element to
the C list when it is smaller than the current B list element; for then we know the
former cannot be found on the B list. Also, we add to C those elements on A when
and if the test of line (5) fails because B is exhausted.
The operator ASSIGN(A, B) copies list A into list B. Note that, this operator
cannot be implemented simply by making the header cell of A point to the same place
as the header cell of B, because in that case, subsequent changes to B would cause
unexpected changes to A. The MIN operator is especially easy; just return the first
element on the list. DELETE and FIND can be implemented by finding the target
item as discussed for general lists and in the case of a DELETE, disposing of its cell.
Lastly, insertion is not difficult to implement, but we must arrange to insert the
new element into the proper position. Figure 4.6 shows a procedure INSERT that
takes as parameters an element and a pointer to the header cell of a list, and inserts the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (10 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
element into the list. Figure 4.7 shows the crucial cells and pointers just before (solid)
and after (dashed) insertion.
procedure INTERSECTION ( ahead, bhead: ­ celltype;
var pc: ­ celltype );
{ computes the intersection of sorted lists A and B with
header cells ahead and bhead, leaving the result
as a sorted list whose header is pointed to by pc }
var
acurrent, bcurrent, ccurrent: ­ celltype;
{the current cells of lists A and B, and the last cell added list C }
begin
(1) new(pc); { create header for list C }
(2) acurrent := ahead­.next;
(3) bcurrent := bhead ­.next;
(4) ccurrent := pc;
(5) while (acurrent <> nil) and (bcurrent <> nil)
do begin
{ compare current elements on lists A and B }
(6) if acurrent ­.element =
bcurrent ­.element then begin
{ add to intersection }
(7) new( ccurrent ­.next );
(8) ccurrent := ccurrent ­.next;
(9) ccurrent ­.element := acurrent
­.element;
(10 acurrent := acurrent­.next;
(11 bcurrent := bcurrent­.next
end
else { elements unequal }
(12) if acurrent­.element <
bcurrent­.element then
(13) acurrent := acurrent­.next
else
(14) bcurrent := bcurrent­.next
end;
(15) ccurrent­.next := nil
end; { INTERSECTION }
Fig. 4.5. Intersection procedure using sorted lists.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (11 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
4.5 The Dictionary
When we use a set in the design of an algorithm, we may not need powerful
operations like union and intersection. Often, we only need to keep a set of "current"
objects, with periodic insertions and deletions from the set. Also, from time to time
we may need to know whether a particular element is in the set. A set ADT with the
operations INSERT, DELETE, and MEMBER has been given the name dictionary.
We shall also include MAKENULL as a dictionary operation to initialize whatever
data structure is used in the implementation. Let us consider an example application
of the dictionary, and then discuss implementations that are well suited for
representing dictionaries.
procedure INSERT ( x: elementtype; p: ­ celltype );
{ inserts x onto list whose header is pointed to by p }
var
current, newcell: ­ celltype;
begin
current := p;
while current­.next <>
nil do begin
if current­.next­.element = x then
return; { if x is already on the list, return }
if current­.next­.element > x then
goto add; { break }
current: = current­.next
end;
add: { current is now the cell after which x is to be inserted }
new(newcell);
newcell­.element := x;
newcell­.next :=
current­.next;
current­.next := newcell
end; { INSERT }
Fig. 4.6. Insertion procedure.
Example 4.2. The Society for the Prevention of Injustice to Tuna (SPIT) keeps a
database recording the most recent votes of legislators on issues of importance to tuna
lovers. This database is, conceptually, two sets of legislators' names, called goodguys
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (12 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
and badguys. The society is very forgiving of past mistakes, but also tends to forget
former friends easily. For example, if a vote to declare Lake Erie off limits to tuna
fishermen is taken, all legislators voting in favor will be inserted into goodguys and
deleted from badguys, while the opposite will happen to those voting against.
Legislators not voting remain in whatever set they were in, if any.
In operation, the database system accepts three commands, each represented by a
single character, followed by a ten character string denoting the name of a legislator.
Each command is on a separate line. The commands are:
1. F (a legislator voting favorably follows)
2. U (a legislator voting unfavorably follows)
3. ? (determine the status of the legislator that follows).
We also allow the character 'E' on the input line to signal the end of processing.
Figure 4.8 shows the sketch of the program, written in terms of the as-yet-undefined
ADT DICTIONARY, which in this case is intended to be a set of strings of length 10.
Fig. 4.7. The insertion picture.
4.6 Simple Dictionary Implementations
A dictionary can be implemented by a sorted or unsorted linked list. Another possible
implementation of a dictionary is by a bit vector, provided the elements of the
underlying set are restricted to the integers 1, . . . , N for some N, or are restricted to a
set that can be put in correspondence with such a set of integers.
A third possible implementation of a dictionary is to use a fixed- length array with
a pointer to the last entry of the array in use. This implementation is only feasible if
we can assume our sets never get larger than the length of the array. It has the
advantage of simplicity over the linked-list representation, while its disadvantages are
that (1) sets cannot grow arbitrarily, (2 deletion is slower, and (3) space is not utilized
efficiently if sets are of varying sizes.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (13 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
It is for the last of these reasons that we did not discuss the array implementation
in connection with sets whose unions and intersections were taken frequently. Since
arrays as well as lists can be sorted, however, the reader could consider the array
implementation we now describe for dictionaries as a possible implementation for
sets in general. Figure 4.9 shows the declarations and procedures necessary to
supplement Fig. 4.8 to make it a working program.
program tuna ( input, output );
{ legislative database }
type
nametype = array[l..10] of char;
var
command: char;
legislator: nametype;
goodguys, badguys: DICTIONARY;
procedure favor (friend: nametype );
begin
INSERT(friend, goodguys) ;
DELETE(friend, badguys)
end; { favor }
procedure unfavor ( foe: nametype );
begin
INSERT(foe, badguys ) ;
DELETE(foe, goodguys )
end; { unfavor }
procedure report ( subject: nametype );
begin
if MEMBER(subject, goodguys) then
writeln(subject, 'is a friend')
else if MEMBER(subject, badguys) then
writeln(subject, 'is a foe')
else
writeln('we have no information about ', subject)
end; { report }
begin { main program }
MAKENULL(goodguys);
MAKENULL(badguys);
read(command);
while command < > 'E' do begin
readln (legislator);
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (14 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
if command = 'F' then
favor(legislator)
else if command = 'U' then
unfavor(legislator)
else if command = '?' then
report(legislator)
else
error('unknown command');
read(command)
end
end. { tuna }
Fig. 4.8. Outline of the SPIT database program.
const
maxsize = { some suitable number };
type
DICTIONARY = record
last: integer;
data: array[l..maxsize] of nametype
end;
procedure MAKENULL ( var A: DICTIONARY );
begin
A.last := 0
end; { MAKENULL }
function MEMBER ( x: nametype; var A: DICTIONARY ):
boolean;
var
i: integer;
begin
for i := 1 to A.last do
if A.data [i] = x then return (true);
return (false) { if x is not found }
end; { MEMBER }
procedure INSERT ( x: nametype; var A: DICTIONARY );
begin
if not MEMBER(x, A ) then
If A.last < maxsize then begin
A.last := A.last + 1;
A.data[A.last ] := x
end
else error ('database is full')
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (15 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
end; { INSERT }
procedure DELETE ( x: nametype; var A: DICTIONARY );
var
i: integer;
begin
if A.last > 0 then begin
i := 1;
while (A.data[i] <> x) and (i <
A.last) do
i := i + 1;
{ when we reach here, either x has been found, or
we are at the last element in set A, or both }
if A.data[i] = x then begin
A.data[i] := A.data[A.last];
{ move the last element into the place of x;
Note that if i = A.last, this step does
nothing, but the next step will delete x }
A.last := A.last - 1
end
end
end; { DELETE }
Fig. 4.9. Type and procedure declarations for array dictionary.
4.7 The Hash Table Data Structure
The array implementation of dictionaries requires, on the average, O(N) steps to
execute a single INSERT, DELETE, or MEMBER instruction on a dictionary of N
elements; we get a similar speed if a list implementation is used. The bit-vector
implementation takes constant time to do any of these three operations, but we are
limited to sets of integers in some small range for that implementation to be feasible.
There is another important and widely useful technique for implementing
dictionaries called "hashing." Hashing requires constant time per operation, on the
average, and there is no requirement that sets be subsets of any particular finite
universal set. In the worst case, this method requires time proportional to the size of
the set for each operation, just as the array and list implementations do. By careful
design, however, we can make the probability of hashing requiring more than a
constant time per operation be arbitrarily small.
We shall consider two somewhat different forms of hashing. One, called open or
external hashing, allows the set to be stored in a potentially unlimited space, and
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (16 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
therefore places no limit on the size of the set. The second, called closed or internal
hashing, uses a fixed space for storage and thus limits the size of sets.†
Open Hashing
In Fig. 4.10 we see the basic data structure for open hashing. The essential idea is that
the (possibly infinite) set of potential set members is partitioned into a finite number
of classes. If we wish to have B classes, numbered 0, 1, . . . , B-1, then we use a hash
function h such that for each object x of the data type for members of the set being
represented, h(x) is one of the integers 0 through B-1. The value of h(x), naturally, is
the class to which x belongs. We often call x the key and h(x) the hash value of x. The
"classes" we shall refer to as buckets, and we say that x belongs to bucket h(x).
In an array called the bucket table, indexed by the bucket numbers 0, 1, . . . , B-1,
we have headers for B lists. The elements on the ith list are the members of the set
being represented that belong to class i, that is, the elements x in the set such that h(x)
= i.
It is our hope that the buckets will be roughly equal in size, so the list for each
bucket will be short. If there are N elements in the set, then the average bucket will
have N/B members. If we can estimate N and choose B to be roughly as large, then the
average bucket will have only one or two members, and the dictionary operations
take, on the average, some small constant number of steps, independent of what N (or
equivalently B) is.
Fig. 4.10. The open hashing data organization.
It is not always clear that we can select h so that a typical set will have its
members distributed fairly evenly among the buckets. We shall later have more to say
about selecting h so that it truly "hashes" its argument, that is, so that h(x) is a
"random" value that is not dependent on x in any trivial way.
function h ( x: nametype ) : O..B - 1;
var
i, sum: integer
begin
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (17 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
sum: = 0;
for i: 1 to 10 do
sum: = sum + ord(x[i]);
h : = sum + ord(x[i]);
end { h }
Fig. 4.11. A simple hash function h.
Let us here, for specificity, introduce one hash function on character strings that is
fairly good, although not perfect. The idea is to treat the characters as integers, using
the character code of the machine to define the correspondence. Pascal provides the
ord built-in function, where ord(c) is the integer code for character c. Thus, if x is a
key and the type of keys is array[l..10] of char (what we called nametype in Example
4.2), we might declare the hash function h as in Fig. 4.11. In that function we sum the
integers for each character and divide the result by B, taking the remainder, which is
an integer from 0 to B - 1.
In Fig. 4.12 we see the declarations of the data structures for an open hash table
and the procedures implementing the operations for a dictionary. The member type
for the dictionary is assumed to be nametype (a character array), so these declarations
can be used in Example 4.2 directly. We should note that in Fig. 4.12 we have made
the headers of the bucket lists be pointers to cells, rather than complete cells. We did
so because the bucket table, where the headers are, could take as much space as the
lists themselves if we made the bucket be an array of cells rather than pointers.
Notice, however, that we pay a price for saving this space. The code for the DELETE
procedure now must distinguish the first cell from the remaining ones.
const
B = { suitable constant };
type
celltype = record
element: nametype;
next: ­ celltype
end;
DICTIONARY = array[0..B - l] of ­ celltype;
procedure MAKENULL ( var A: DICTIONARY );
var
i: integer;
begin
for i:= 0 to B-1 do
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (18 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
A [i]:= nil
end; { MAKENULL }
function MEMBER ( x: nametype; var A: DICTIONARY ) : boolean;
var
current: ­ celltype;
begin
current := A [h (x)];
{ initially current is the header of x's bucket }
while current < > nil do
if current­.element =
x then
return (true)
else
current := current­.next;
return (false) { if x is not found }
end; { MEMBER }
procedure INSERT ( x: nametype; var A: DICTIONARY );
var
bucket: integer;
oldheader: ­ celltype;
begin
if not MEMBER(x, A) then begin
bucket := h(x);
oldheader := A [bucket ];
new (A [bucket]);
A [bucket ]­.element :=
x;
A [bucket ]­.next: =
oldheader
end
end; { INSERT }
procedure DELETE ( x: nametype; var A: DICTIONARY );
var
current: ­ celltype;
bucket: integer;
begin
bucket := h(x);
if A [bucket ] < > nil then begin
if A [bucket]­.element
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (19 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
= x then { x is in first cell }
A [bucket ] := A [bucket ]­.next { remove x from list }
else begin { x, if present at all, is not in first cell of bucket }
current := A [bucket ]; { current points to the previous
cell }
while current­.next <> nil
do
if current­.next­.element = x then begin
current­.next :=
current­.next­.next;
{ remove x from list }
return { break }
end
else {x not yet found}
current := current­.next
end
end
end; { DELETE }
Fig. 4.12. Dictionary implementation by an open hash table.
Closed Hashing
A closed hash table keeps the members of the dictionary in the bucket table itself,
rather than using that table to store list headers. As a consequence, it appears that we
can put only one element in any bucket. However, associated with closed hashing is a
rehash strategy. If we try to place x in bucket h(x) and find it already holds an
element, a situation called a collision occurs, and the rehash strategy chooses a
sequence of alternative locations, h1(x), h2(x), . . . , within the bucket table in which
we could place x. We try each of these locations, in order, until we find an empty one.
If none is empty, then the table is full, and we cannot insert x.
Example 4.3. Suppose B = 8 and keys a, b, c, and d have hash values h(a) = 3, h(b) =
0, h(c) = 4, h(d) = 3. We shall use the simplest rehash strategy, called linear hashing,
where hi(x) = (h(x) + i) mod B. Thus, for example, if we wished to insert a and found
bucket 3 already filled, we would try buckets 4, 5, 6, 7, 0, 1, and 2, in that order.
Initially, we assume the table is empty, that is, each bucket holds a special value
empty, which is not equal to any value we might try to insert.† If we insert a, b, c, and
d, in that order, into an initially empty table, we find a goes into bucket 3, b into 0
and c into 4. When we try to insert d, we first try h(d) = 3 and find it filled. Then we
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (20 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
try h1(d) = 4 and find it filled as well. Finally, when we try h2(d) = 5, we find an
empty space and put d there. The resulting positions are shown in Fig. 4.13.
Fig. 4.13. Partially filled hash table.
The membership test for an element x requires that we examine h(x), hi(x), h2(x), .
. . , until we either find x or find an empty bucket. To see why we can stop looking if
we reach an empty bucket, suppose first that there are no deletions permitted. If, say,
h3(x) is the first empty bucket found in the series, it is not possible that x is in bucket
h4(x), h5(x), or further along the sequence, because x could not have been placed there
unless h3(x) were filled at the time we inserted x.
Note, however, that if we allow deletions, then we can never be sure, if we reach
an empty bucket without finding x, that x is not somewhere else, and the now empty
bucket was occupied when x was inserted. When we must do deletions, the most
effective approach is to place a constant called deleted into a bucket that holds an
element that we wish to delete. It is important that we can distinguish between deleted
and empty, the constant found in all buckets that have never been filled. In this way it
is possible to permit deletions without having to search the entire table during a
MEMBER test. When inserting we can treat deleted as an available space, so with
luck the space of a deleted element will eventually be reused. However, as the space
of a deleted element is not immediately reclaimable as it is with open hashing, we
may prefer the open to the closed scheme.
Example 4.4. Suppose we wish to test if e is in the set represented by Fig. 4.13. If
h(e) = 4, we try buckets 4, 5, and then 6. Bucket 6 is empty and since we have not
found e, we conclude e is not in the set.
If we delete c, we must put the constant deleted in bucket 4. In that way, when we
look for d, and we begin at h(d) = 3, we shall scan 4 and 5 to find d, and not stop at 4
as we would if we put empty in that bucket.
In Fig 4.14 we see the type declarations and operations for the DICTIONARY
ADT with set members of type nametype and the closed hash table as the underlying
implementation. We use an arbitrary hash function h, of which Fig. 4.11 is one
possibility, and we use the linear hashing strategy to rehash in case of collisions. For
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (21 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
convenience, we have identified empty with a string of ten blanks and deleted with a
string of 10 *'s, assuming that neither of these strings could be real keys. (In the SPIT
database these strings are unlikely to be names of legislators.) The procedure
INSERT(x, A ) first uses locate to determine if x is already in A, and, if not, it uses a
special routine locate 1 to find a location in which to insert x. Note that locate 1
searches for deleted as well as empty locations.
const
empty = ' '; { 10 blanks }
deleted = '**********'{ 10 *'s }
type
DICTIONARY = array[0..B -1] of nametype;
procedure MAKENULL ( var A: DICTIONARY );
var
i: integer;
begin
for i := 0 to B - 1 do
A [i] := empty
end; {MAKENULL}
function locate ( x: nametype; A: DICTIONARY ) : integer;
{ locate scans DICTIONARY from the bucket for h(x) until
either x is found, or an empty bucket is found, or it has
scanned completely around the table, thereby determining
that the table does not contain x. locate returns the index
of the bucket at which it stops for any of these reasons }
var
initial, i: integer;
{ initial holds h(x); i counts the number of
buckets thus far scanned when looking for x }
begin
initial := h(x);
i := 0;
while (i < B) and (A [(initial +
i) mod B ] <> x) and
(A [(initial + i) mod B] <>
empty) do
i := i + 1;
return ((initial + i) mod B)
end; { locate }
function locate 1 ( x: nametype; A: DICTIONARY ) : integer;
{ like locate, but it will also stop at and return a deleted entry }
function MEMBER ( x: nametype; var A: DICTIONARY ) :
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (22 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
boolean;
begin
if A [locate(x)] = x then
return (true)
else
return (false)
end; { MEMBER }
procedure INSERT (x: nametype; var A: DICTIONARY );
var
bucket: integer;
begin
if A[locate (x)] = x then return; { x
is already in A }
bucket := locate 1(x);
if (A [bucket] = empty) or (A
[bucket] = deleted) then
A [bucket] := x
else
error('INSERT failed: table is full')
end; { INSERT }
procedure DELETE ( x: nametype; var A: DICTIONARY );
var
bucket: integer;
begin
bucket := locate(x);
if A [bucket] = x then
A [bucket] := deleted
end; { DELETE }
Fig. 4.14. Dictionary implementation by a closed hash table.
4.8 Estimating the Efficiency of Hash
Functions
As we have mentioned, hashing is an efficient way to represent dictionaries and some
other abstract data types based on sets. In this section we shall examine the average
time per dictionary operation in an open hash table. If there are B buckets and N
elements stored in the hash table, then the average bucket has N/B members, and we
expect that an average INSERT, DELETE, or MEMBER operation will take O(1 +
N/B) time. The constant 1 represents the time to find the bucket, and N/B the time to
search the bucket. If we can choose B to be about N, this time becomes a constant per
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (23 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
operation. Thus the average time to insert, delete, or test an element for membership,
assuming the element is equally likely to be hashed to any bucket, is a constant
independent of N.
Suppose we are given a program written in some language like Pascal, and we
wish to insert all identifiers appearing in the program into a hash table. Whenever a
declaration of a new identifier is encountered, the identifier is inserted into the hash
table after checking that it is not already there. During this phase it is reasonable to
assume that an identifier is equally likely to be hashed into any given bucket. Thus we
can construct a hash table with N elements in time O(N(1 + N/B)). By choosing B
equal to N this becomes O(N).
In the next phase identifiers are encountered in the body of the program. We must
locate the identifiers in the hash table to retrieve information associated with them.
But what is the expected time to locate an identifier? If the element searched for is
equally likely to be any element in the table, then the expected time to search for it is
just the average time spent inserting an element. To see this, observe that in searching
once for each element in the table, the time spent is exactly the same as the time spent
inserting each element, assuming that elements are always appended to the end of the
list for the appropriate bucket. Thus the expected time for a search is also 0(1 + N/B).
The above analysis assumes that a hash function distributes elements uniformly
over the buckets. Do such functions exist? We can regard a function such as that of
Fig. 4.11 (convert characters to integers, sum and take the remainder modulo B) as a
typical hash function. The following example examines the performance of this hash
function.
Example 4.5. Suppose we use the hash function of Fig. 4.11 to hash the 100 keys
consisting of the character strings A0, A1, . . . , A99 into a table of 100 buckets. On
the assumption that ord(O), ord(1), . . . , ord(9) form an arithmetic progression, as
they do in the common character codes ASCII and EBCDIC, it is easy to check that
the keys hash into at most 29 of the 100 buckets,† and the largest bucket contains
A18, A27, A36, . . . , A90, or 9 of the 100 elements. If we compute the average
number of steps for insertion, using the fact that the insertion of the ith element into a
bucket takes i + 1 steps, we get 395 steps for these 100 keys. In comparison, our
estimate N(1 + N/B) would suggest 200 steps.
The simple hashing function of Fig. 4.11 may treat certain sets of inputs, such as
the consecutive strings in Example 4.5, in a nonrandom manner. "More random" hash
functions are available. As an example, we can use the idea of squaring and taking
middle digits. Thus, if we have a 5-digit number n as a key and square it, we get a 9
or 10 digit number. The "middle digits," such as the 4th through 7th places from the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (24 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
right, have values that depend on almost all the digits of n. For example, digit 4
depends on all but the leftmost digit of n, and digit 5 depends on all digits of n. Thus,
if B = 100, we might choose to take digits 6 and 5 to form the bucket number.
This idea can be generalized to situations where B is not a power of 10. Suppose
keys are integers in the range 0, 1, . . . , K. If we pick an integer C such that BC2 is
about equal to K2, then the function
h(n) = [n2/C] mod B
effectively extracts a base-B digit from the middle of n2.
Example 4.6. If K = 1000 and B = 8, we might choose C = 354. Then
To use the "square and take the middle" strategy when keys are character strings,
first group the characters in the string from the right into blocks of a fixed number of
characters, say 4, padding the left with blanks, if necessary. Treat each block as a
single integer, formed by concatenating the binary codes for the characters. For
example, ASCII uses a 7-bit character code, so characters may be viewed as "digits"
in base 27, or 128. Thus we can regard the character string abcd as the integer (128)3a
+ (128)2b + (128)c + d. After converting all blocks to integers, add them† and
proceed as we have suggested previously for integers.
Analysis of Closed Hashing
In a closed hashing scheme the speed of insertion and other operations depends not
only on how randomly the hash function distributes the elements into buckets, but
also on how well the rehashing strategy avoids additional collisions when a bucket is
already filled. For example, the linear strategy for resolving collisions is not as good
as possible. While the analysis is beyond the scope of this book, we can observe the
following. As soon as a few consecutive buckets are filled, any key that hashes to one
of them will be sent, by the rehash strategy, to the end of the group, thereby
increasing the length of that consecutive group. We are thus likely to find more long
runs of consecutive buckets that are filled than if elements filled buckets at random.
Moreover, runs of filled blocks cause some long sequences of tries before a newly
inserted element finds an empty bucket, so having unusually large runs of filled
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (25 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
buckets slows down insertion and other operations.
We might wish to know how many tries (or probes) are necessary on the average
to insert an element when N out of B buckets are filled, assuming all combinations of
N out of B buckets are equally likely to be filled. It is generally assumed, although not
proved, that no closed hashing strategy can give a better average time performance
than this for the dictionary operations. We shall then derive a formula for the cost of
insertion if the alternative locations used by the rehashing strategy are chosen at
random. Finally, we shall consider some strategies for rehashing that approximate
random behavior.
The probability of a collision on our initial probe is N/B. Assuming a collision, our
first rehash will try one of B - 1 buckets, of which N - 1 are filled, so the probability
of at least two collisions is Similarly, the probability of at least i collisions
is
If B and N are large, this probability approximates (N/B)i. The average number of
probes is one (for the successful insertion) plus the sum over all i ³ 1 of the
probability of at least i collisions, that is, approximately . It can
be shown that the exact value of the summation when formula (4.3) is substituted for
(N/B)i is , so our approximation is a good one except when N is very close to
B.
Observe that grows very slowly as N begins to grow from 0 to B - 1, the
largest value of N for which another insertion is possible. For example, if N is half B,
then about two probes are needed for the next insertion, on the average. The average
insertion cost per bucket to fill M of the B buckets is , which is
approximately , or Thus, to fill the table completely
(M = B) requires an average of logeB per bucket, or B logeB probes in total. However,
to fill the table to 90% of capacity (M = .9B) only requires B((10/9) logel0) or
approximately 2.56B probes.
The average cost of the membership test for a nonexistent element is the same as
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (26 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
the cost of inserting the next element, but the cost of the membership test for an
element in the set is the average cost of all insertions made so far, which is
substantially less if the table is fairly full. Deletions have the same average cost as
membership testing. But unlike open hashing, deletions from a closed hash table do
not help speed up subsequent insertions or membership tests. It should be emphasized
that if we never fill closed hash tables to more than any fixed fraction less than one,
the average cost of operations is a constant; the constant grows as the permitted
fraction of capacity grows. Figure 4.15 graphs the cost of insertions, deletions and
membership tests, as a function of the percentage of the table that is full at the time
the operation is performed.
Fig. 4.15. Average operation cost.
"Random" Strategies for Collision
Resolution
We have observed that the linear rehashing strategy tends to group full buckets into
large consecutive blocks. Perhaps we could get more "random" behavior if we probed
at a constant interval greater than one. That is, let hi(x) = (h(x)+ci) mod B for some c
> 1. For example, if B = 8, c = 3, and h(x) = 4, we would probe buckets 4, 7, 2, 5, 0, 3,
6, and 1, in that order. Of course, if c and B have a common factor greater than one,
this strategy doesn't even allow us to search all buckets; try B = 8 and c = 2, for
example. But more significantly, even if c and B are relatively prime (have no
common factors), we have the same "bunching up" problem as with linear hashing,
although here it is sequences of full buckets separated by difference c that tend to
occur. This phenomenon slows down operations as for linear hashing, since an
attempted insertion into a full bucket will tend to travel down a chain of full buckets
separated by distance c, and the length of this chain will increase by one.
In fact, any rehashing strategy where the target of a probe depends only on the
target of the previous probe (as opposed to depending on the number of unsuccessful
probes so far, the original bucket h(x), or the value of the key x itself) will exhibit the
bunching property of linear hashing. Perhaps the simplest strategy in which the
problem does not occur is to let hi(x) = (h(x)+di) mod B where d1, d2, . . . , dB-1 is a
"random" permutation of the integers 1, 2, . . . , B-1. Of course, the same sequence d1,
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (27 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
. . . , dB-1 is used for all insertions, deletions and membership tests; the "random"
shuffle of the integers is decided upon once, when we design the rehash algorithm.
The generation of "random" numbers is a complicated subject, but fortunately,
many common methods do produce a sequence of "random" numbers that is actually
a permutation of integers from 1 up to some limit. These random number generators,
if reset to their initial values for each operation on the hash table, serve to generate the
desired sequence d1, . . . , dB-1.
One effective approach uses "shift register sequences." Let B be a power of 2 and k
a constant between 1 and B-1. Start with some number d1 in the range 1 to B - 1, and
generate successive numbers in the sequence by taking the previous value, doubling
it, and if the result exceeds B, subtracting B and taking the bitwise modulo 2 sum of
the result and the selected constant k. The bitwise modulo 2 sum of x and y, written x
Å y, is computed by writing x and y in binary, with leading 0's if necessary so both
are of the same length, and forming the numbers whose binary representation has a 1
in those positions where one, but not both, of x and y have a 1.
Example 4.7. 25 Å 13 is computed by taking
25 = 11001
13 = 01101
______
25 Å 13 = 10100
Note that this "addition" can be thought of as ordinary binary addition with carries
from place to place ignored.
Not every value of k will produce a permutation of 1, 2, . . . , B-1; sometimes a
number repeats before all are generated. However, for given B, there is a small but
finite chance that any particular k will work, and we need only find one k for each B.
Example 4.8. Let B = 8. If we pick k = 3, we succeed in generating all of 1, 2, . . . , 7.
For example, if we start with d1 = 5, then we compute d2 by first doubling d1 to get
10. Since 10 > 8, we subtract 8 to get 2, and then compute d2 = 2 Å 3 = 1. Note that x
Å 3 can be computed by complementing the last two bits of x.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (28 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
It is instructive to see the 3-bit binary representations of d1, d2, . . . , d7. These are
shown in Fig. 4.16, along with the method of their calculation. Note that
multiplication by 2 corresponds to a shift left in binary. Thus we have a hint of the
origin of the term "shift register sequence."
Fig. 4.16. Calculating a shift register sequence.
The reader should check that we also generate a permutation of 1, 2, . . . ,7 if we
choose k = 5, but we fail for other values of k.
Restructuring Hash Tables
If we use an open hash table, the average time for operations increases as N/B, a
quantity that grows rapidly as the number of elements exceeds the number of buckets.
Similarly, for a closed hash table, we saw from Fig. 4.15 that efficiency goes down as
N approaches B, and it is not possible that N exceeds B.
To retain the constant time per operation that is theoretically possible with hash
tables, we suggest that if N gets too large, for example N ³ .9B for a closed table or N
³ 2B for an open one, we simply create a new hash table with twice as many buckets.
The insertion of the current members of the set into the new table will, on the
average, take less time than it took to insert them into the smaller table, and we more
than make up this cost when doing subsequent dictionary operations.
4.9 Implementation of the Mapping ADT
Recall our discussion of the MAPPING ADT from Chapter 2 in which we defined a
mapping as a function from domain elements to range elements. The operations for
this ADT are:
1. MAKENULL(A) initializes the mapping A by making each domain element
have no assigned range value.
2. ASSIGN(A, d, r) defines A(d) to be r.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (29 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
3. COMPUTE(A, d, r) returns true and sets r to A(d) if A(d) is defined; false is
returned otherwise.
The hash table is an effective way to implement a mapping. The operations
ASSIGN and COMPUTE are implemented much like INSERT and MEMBER
operations for a dictionary. Let us consider an open hash table first. We assume the
hash function h(d) hashes domain elements to bucket numbers. While for the
dictionary, buckets consisted of a linked list of elements, for the mapping we need a
list of domain elements paired with their range values. That is, we replace the cell
definition in Fig. 4.12 by
type
celltype = record
domainelement: domaintype;
range: rangetype;
next: ­ celltype
end
where domaintype and rangetype are whatever types domain and range
elements have in this mapping. The declaration of a MAPPING is
type
MAPPING = array[0..B-1] of ­ celltype
This array is the bucket array for a hash table. The procedure ASSIGN is written in
Fig. 4.17. The code for MAKENULL and COMPUTE are left as an exercise.
Similarly, we can use a closed hash table as a mapping. Define cells to consist of
domain element and range fields and declare a MAPPING to be an array of cells. As
for open hash tables, let the hash function apply to domain elements, not range
elements. We leave the implementation of the mapping operations for a closed hash
table as an exercise.
4.10 Priority Queues
The priority queue is an ADT based on the set model with the operations INSERT
and DELETEMIN, as well as the usual MAKENULL for initialization of the data
structure. To define the new operation, DELETEMIN, we first assume that elements
of the set have a "priority" function defined on
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (30 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
procedure ASSIGN ( var A: MAPPING; d: domaintype;
r: rangetype );
var
bucket: integer;
current: ­ celltype;
begin
bucket := h(d);
current := A[bucket];
while current <> nil do
if current­.domainelement = d then begin
current­.range := r;
{ replace old value for d }
return
end
else
current := current­.next;
{ at this point, d was not found on the list }
current := A[bucket]; { use current to remember first cell
}
new(A[bucket]);
A [bucket]­.domainelement
:= d;
A [bucket]­.range: =
r;
A [bucket]­.next :=
current
end; { ASSIGN }
Fig. 4.17. The procedure ASSIGN for an open hash table.
them; for each element a, p(a), the priority of a, is a real number or, more generally, a
member of some linearly ordered set. The operation INSERT has the usual meaning,
while DELETEMIN is a function that returns some element of smallest priority and,
as a side effect, deletes it from the set. Thus, as its name implies, DELETEMIN is a
combination of the operations DELETE and MIN discussed earlier in the chapter.
Example 4.9. The term "priority queue" comes from the following sort of use for this
ADT. The word "queue" suggests that people or entities are waiting for some service,
and the word "priority" suggests that the service is given not on a "first-come-firstserved"
basis as for the QUEUE ADT, but rather that each person has a priority based
on the urgency of need. An example is a hospital waiting room, where patients having
potentially fatal problems will be taken before any others, no matter how long the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (31 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
respective waits are.
As a more mundane example of the use of priority queues, a time- shared
computing system needs to maintain a set of processes waiting for service. Usually,
the system designers want to make short processes appear to be instantaneous (in
practice, response within a second or two appears instantaneous), so these are given
priority over processes that have already consumed substantial time. A process that
requires several seconds of computing time cannot be made to appear instantaneous,
so it is sensible strategy to defer these until all processes that have a chance to appear
instantaneous have been done. However, if we are not careful, processes that have
taken substantially more time than average may never get another time slice and will
wait forever.
One possible way to favor short processes, yet not lock out long ones is to give
process P a priority 100tused(P) - tinit(P). The parameter tused gives the amount of
time consumed by the process so far, and tinit gives the time at which the process
initiated, measured from some "time zero." Note that priorities will generally be large
negative integers, unless we choose to measure tinit from a time in the future. Also
note that 100 in the above formula is a "magic number"; it is selected to be somewhat
larger than the largest number of processes we expect to be active at once. The reader
may observe that if we always pick the process with the smallest priority number, and
there are not too many short processes in the mix, then in the long run, a process that
does not finish quickly will receive 1% of the processor's time. If that is too much or
too little, another constant can replace 100 in the priority formula.
We shall represent processes by records consisting of a process identifier and a
priority number. That is, we define
type
processtype = record
id: integer;
priority: integer
end;
The priority of a process is the value of the priority field, which here we have defined
to be an integer. We can define the priority function as follows.
function p ( a: processtype ): integer;
begin
return (a.priority)
end;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (32 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
In selecting processes to receive a time slice, the system maintains a priority queue
WAITING of processtype elements and uses two procedures, initial and select, to
manipulate the priority queue by the operations INSERT and DELETEMIN.
Whenever a process is initiated, procedure initial is called to place a record for that
process in WAITING. Procedure select is called when the system has a time slice to
award to some process. The record for the selected process is deleted from
WAITING, but retained by select for reentry into the queue with a new priority; the
priority is increased by 100 times the amount of time used.
We make use of function currenttime, which returns the current time, in whatever
time units are used by the system, say microseconds, and we use procedure
execute(P) to cause the process with identifier P to execute for one time slice. Figure
4.18 shows the procedures initial and select.
procedure initial ( P: integer );
{ initial places process with id P on the queue }
var
process: processtype;
begin
process.id := P;
process.priority := - currenttime;
INSERT (process, WAITING)
end; { initial }
procedure select;
{ select allocates a time slice to process with highest priority }
var
begintime, endtime: integer;
process: processtype;
begin
process := ­ DELETEMIN(WAITING);
{ DELETEMIN returns a pointer to the deleted element }
begintime := currenttime;
execute (process.id);
endtime := currenttime;
process.priority := process.priority + 100*(endtime -
begintime);
{ adjust priority to incorporate amount of time used }
INSERT (process, WAITING)
{ put selected process back on queue with new priority }
end; { select }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (33 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
Fig. 4.18. Allocating time to processes.
4.11 Implementations of Priority Queues
With the exception of the hash table, the set implementations we have studied so far
are also appropriate for priority queues. The reason the hash table is inappropriate is
that there is no convenient way to find the minimum element, so hashing merely adds
complications, and does not improve performance over, say, a linked list.
If we use a linked list, we have a choice of sorting it or leaving it unsorted. If we
sort the list, finding a minimum is easy -- just take the first element on the list.
However, insertion requires scanning half the list on the average to maintain the
sorted list. On the other hand, we could leave the list unsorted, which makes insertion
easy and selection of a minimum more difficult.
Example 4.10. We shall implement DELETEMIN for an unsorted list of elements of
type processtype, as defined in Example 4.9. The list is headed by an empty cell. The
implementations of INSERT and MAKENULL are straightforward, and we leave the
implementation using sorted lists as an exercise. Figure 4.19 gives the declaration for
cells, for the type PRIORITYQUEUE, and for the procedure DELETEMIN.
Partially Ordered Tree Implementation of
Priority Queues
Whether we choose sorted or unsorted lists to represent priority queues, we must
spend time proportional to n to implement one or the other of INSERT or
DELETEMIN on sets of size n. There is another implementation in which
DELETEMIN and INSERT each require O(logn) steps, a substantial improvement for
large n (say n ³ 100). The basic idea is to organize the elements of the priority queue
in a binary tree that is as balanced as possible; an example is in Fig. 4.20. At the
lowest level, where some leaves may be missing, we require that all missing leaves
are to the right of all leaves that are present on the lowest level.
Most importantly, the tree is partially ordered, meaning that the priority of node v
is no greater than the priority of the children of v, where the priority of a node is the
priority number of the element stored at the node. Note from Fig. 4.20 that small
priority numbers need not appear at higher levels than larger priority numbers. For
example, level three has 6 and 8, which are less than the priority number 9 appearing
on level 2. However, the parent of 6 and 8 has priority 5, which is, and must be, at
least as small as the priorities of its children.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (34 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
To execute DELETEMIN, we return the minimum-priority element, which, it is
easily seen, must be at the root. However, if we simply remove the root, we no longer
have a tree. To maintain the property of being a partially ordered tree, as balanced as
possible, with leaves at the lowest level as far left as can be, we take the rightmost
leaf at the lowest level and temporarily put it at the root. Figure 4.21(a) shows this
change from Fig. 4.20. Then we push this element as far down the tree as it will go,
by exchanging it with the one of its children having smaller priority, until the element
is either at a leaf or at a position where it has priority no larger than either of its
children.
In Fig. 4.21(a) we must exchange the root with its smaller-priority child, which has
priority 5. The result is shown in Fig. 4.21(b). The element being pushed down is still
larger than its children, with priorities 6 and 8. We exchange it with the smaller of
these, 6, to reach the tree of Fig. 4.21(c). This tree has the partially ordered property.
In this percolation, if a node v has an element with priority a, if its children have
elements with priorities b and c, and if at least one of b and c is smaller than a, then
exchanging a with the smaller of b and c results in v having an element whose priority
is smaller than either of its children. To prove this, suppose b £ c. After the exchange,
node v acquires b, and its
type
celltype = record
element: processtype;
next: ­ celltype
end;
PRIORITYQUEUE = ­ celltype;
{ cell pointed to is a list header }
function DELETEMIN ( var A: PRIORITYQUEUE ): ­ celltype;
var
current: ­ celltype; { cell before one being
"scanned" }
lowpriority: integer; { smallest priority found so far }
prewinner: ­ celltype; { cell before one with
element
of smallest priority }
begin
if A ­.next = nil
then
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (35 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
error('cannot find minimum of empty list')
else begin
lowpriority := p(A­.next­.element);
{ p returns priority of the first element. Note A points
to a header cell that does not contain an element }
prewinner := A;
current := A­.next;
while current­.next <>
nil do begin
{ compare priorities of current winner
and next element }
if p( current­.next­.element) <
lowpriority then begin
prewinner := current;
lowpriority : = p (current­.next­. element)
end;
current := current­.next
end;
DELETEMIN := prewinner­.next;
{ return pointer to the winner }
prewinner­.next :=
prewinner­.next­.next
{ remove winner from list }
end
end; { DELETEMIN }
Fig. 4.19. Linked list priority queue implementation.
children hold a and c. We assumed b £ c, and we were told that a is larger than at
least one of b and c. Therefore b £ a surely holds. Thus the insertion procedure
outlined above percolates an element down the tree until there
Fig. 4.20. A partially ordered tree.
are no more violations of the partially ordered property.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (36 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
Let us also observe that DELETEMIN applied to a set of n elements takes O(logn)
time. This follows since no path in the tree has more than 1 + logn nodes, and the
process of forcing an element down the tree takes a constant time per node. Observe
that for any constant c, the quantity c(1+logn) is at most 2clogn for n ³ 2. Thus
c(1+logn) is O(logn).
Now let us consider how INSERT should work. First, we place the new element as
far left as possible on the lowest level, starting a new level if the current lowest level
is all filled. Fig. 4.22(a) shows the result of placing an element with priority 4 into
Fig. 4.20. If the new element has priority lower than its parent, exchange it with its
parent. The new element is now at a position where it is of lower priority than either
of its children, but it may also be of lower priority than its parent, in which case we
must exchange it with its parent, and repeat the process, until the new element is
either at the root or has larger priority than its parent. Figures 4.22(b) and (c) show
how 4 moves up by this process.
We could prove that the above steps result in a partially ordered tree. While we
shall not attempt a rigorous proof, let us observe that an element with priority a can
become the parent of an element with priority b in three ways. (In what follows, we
identify an element with its priority.)
1. a is the new element and moves up the tree replacing the old parent of b. Let
the old parent of b have priority c. Then a < c, else the exchange would not
take place. But c £ b, since the original tree was partially ordered. Thus a < b.
For example in Fig. 4.22(c) 4 became the parent of 6, replacing a parent of
larger priority, 5.
2. a was pushed down the tree due to an exchange with the new element. In this
case a must have been an ancestor of b in the original partially ordered tree.
Thus a £ b. For example, in Fig. 4.22(c), 5 becomes the
Fig. 4.21. Pushing an element down the tree.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (37 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
Fig. 4.22. Inserting an element.
parent of elements with priority 8 and 9. Originally, 5 was the parent of the
former and the "grandparent" of the latter.
3. b might be the new element, and it moves up to become a child of a. If a > b,
then a and b will be exchanged at the next step, so this violation of the
partially ordered property will be removed.
The time to perform an insertion is proportional to the distance up the tree that the
new element travels. As for DELETEMIN, we observe that this distance can be no
greater than 1 + logn, so both INSERT and DELETEMIN take O(log n) steps.
An Array Implementation of Partially
Ordered Trees
The fact that the trees we have been considering are binary, balanced as much as
possible, and have leaves at the lowest level pushed to the left means that we can use
a rather unusual representation for these trees, called a heap. If there are n nodes, we
use the first n positions of an array A. A [1] holds the root. The left child of the node
in A[i], if it exists, is at A [2i], and the right child, if it exists, is at A [2i + 1]. An
equivalent viewpoint is that the parent of A [i] is A [i div 2], for i > 1. Still another
equivalent observation is that the nodes of a tree fill up A[1], A[2], . . . , A [n] level-bylevel,
from the top, and within a level, from the left. For example, Fig. 4.20
corresponds to an array containing the elements 3, 5, 9, 6, 8, 9, 10, 10, 18, 9.
We can declare a priority queue of elements of some type, say processtype as in
Example 4.9, to consist of an array of processtype and an integer last, indicating the
currently last element of the array that is in use. If we assume maxsize is the desired
size of arrays for a priority queue, we can declare:
type
PRIORITYQUEUE = record
contents: array[1..maxsize] of processtype;
last: integer
end;
The priority queue operations are implemented below in Fig. 4.23.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (38 of 52) [1.7.2001 19:04:14]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
procedure MAKENULL ( var A: PRIORITYQUEUE );
begin
A .last := 0
end; { MAKENULL }
procedure INSERT ( x: processtype; var A:
PRIORITYQUEUE );
var
i: integer;
temp : processtype;
begin
if A .last >= maxsize then
error ('priority queue is full')
else begin
A .last := A .last + 1;
A .contents [A .last] := x;
i := A .last; { i is index of current position of x }
while (i > 1) and†
(p(A .contents[i]) < p(A .contents[i
div 2])) do
begin { push x up the tree by exchanging it with its parent
of larger priority. Recall p computes the priority of
a processtype element }
temp := A .contents [i];
A .contents [i] := A .contents [i
div 2];
A .contents [i div 2] := temp;
i := i div 2
end
end
end; { INSERT }
function DELETEMIN ( var A: PRIORITYQUEUE ): ­ processtype;
var
i, j: integer;
temp : processtype;
minimum: ­ processtype;
begin
if A .last = 0 then
error('priority queue is empty')
else begin
new (minimum);
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (39 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
minimum ­ := A .contents
[1];
{ we shall return a pointer to a copy of the root of A }
A .contents [1] := A .contents [A .last];
A .last := A .last - 1;
{ move the last element to the beginning }
i := 1; { i is the current position of the old last element }
while i < = A .last div 2 do begin
{ push old last element down tree }
if (p (A .contents [2*i]) < p
(A .contents [2*i + 1]))
or (2*i = A.last) then
j := 2 * i
else
j := 2 * i + 1;
{ j will be the child of i having the smaller priority
or if 2*i = A.last, then j is the only child of i }
if p (A .contents [i]) > p (A
.contents [j]) then begin
{ exchange old last element with smaller priority child }
temp := A.contents[i];
A.contents[i] := A.contents[j];
A.contents[j] := temp;
i := j
end
else
return (minimum) { cannot push further }
end;
return (minimum) { pushed all the way to a leaf }
end
end; { DELETEMIN }
Fig. 4.23. Array implementation of priority queue.
4.12 Some Complex Set Structures
In this section we consider two more complex uses of sets to represent data. The first
problem is that of representing many-many relationships, as might occur in a database
system. A second case study exhibits how a pair of data structures representing the
same object (a mapping in our example) can be a more efficient representation than
either acting alone.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (40 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
Many-Many Relationships and the
Multilist Structure
An example of a many-many relationship between students and courses is suggested
in Fig. 4.24. It is called a "many-many" relationship because there can be many
students taking each course and many courses taken by each student.
Fig. 4.24. An example relationship between students and courses.
From time to time the registrar may wish to insert or delete students from courses,
to determine which students are taking a given course, or to know which courses a
given student is taking. The simplest data structure with which these questions can be
answered is obvious; just use the 2-dimensional array suggested by Fig. 4.24, where
the value 1 (or true) replaces the X's and 0 (or false) replaces the blanks.
For example, to insert a student into a course, we need a mapping, MS, perhaps
implemented as a hash table, that translates student names into array indices, and
another, MC, that translates course names into array indices. Then, to insert student s
into course c, we simply set
Enrollment[MS(s ), MC(c)] := 1.
Deletions are performed by setting this element to 0. To find the courses taken by the
student with name s, run across the row MS(s), and similarly run down the column
MC(c) find the students in course c.
Why might we wish to look further for a more appropriate data structure?
Consider a large university with perhaps 1000 courses and 20,000 students, taking an
average of three courses each. The array suggested by Fig. 4.24 would have
20,000,000 elements, of which 60,000, or 0.3%, would be 1. † Such an array, which
is called sparse to indicate that almost all its elements are zero, can be represented in
far less space if we simply list the nonzero entries. Moreover, we can spend a great
deal of time scanning a column of 20,000 entries searching for, on the average, 60
that are nonzero; row scans take considerable time as well.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (41 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
One way to do better is to begin by phrasing the problem as one of maintaining a
collection of sets. Two of these sets are S and C, the sets of all students and all
courses. Each element of S is actually a record of a type such as
type
studenttype = record
id: integer;
name: array[l..30] of char;
end
and we would invent a similar record type for courses. To implement the structure we
have in mind, we need a third set of elements, E, standing for enrollments. The
elements of E each represent one of the boxes in the array of Fig. 4.24 in which there
is an X. The E elements are records of a fixed type. As of now, we don't know any
fields that belong in these records‡, although we shall soon learn of them. For the
moment, let us simply postulate that there is an enrollment record for each X-entry in
the array and that enrollment records are distinguishable from one another somehow.
We also need sets that represent the answers to the crucial questions: given a
student or course, what are the related courses or students, respectively. It would be
nice if for each student s there were a set Cs of all the courses s was taking and
conversely, a set Sc of the students taking course c.
Such sets would be hard to implement because there would be no limit on the number
of sets any element could be in, forcing us to complicate student and course records.
We could instead let Sc and Cs be sets of pointers to course and student records, rather
than the records themselves, but there is a method that saves a significant fraction of
the space and allows equally fast answers to questions about students and courses.
Let us make each set Cs be the set of enrollment records corresponding to student s
and some course c. That is, if we think of an enrollment as a pair (s, c), then
Cs = {(s, c) | s is taking course c}
We can similarly define
Sc = {(s, c) | s is taking course c}
Note the only difference in the meaning of the two set formers above is that in the
first case s is constant and in the second c is. For example, based on Fig. 4.24, CAlex =
{(Alex, CS101), (Alex, CS202)} and SCS101 = {(Alex, CS101), (Amy, CS101), (Ann,
CS101)}.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (42 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
Multilist Structures
In general, a multilist structure is any collection of cells some of which have more
than one pointer and can therefore be on more than one list simultaneously. For each
type of cell in a multilist structure, it is important to distinguish among pointer fields
so we can follow one particular list and not get confused about which of several
pointers in a particular cell we are following.
As a case in point, we can put one pointer field in each student and course record,
pointing to the first enrollment record in the set Cs or Sc, respectively. Each
enrollment record needs two pointer fields, one, which we shall call cnext, for the
next enrollment on the list for the Cs set to which it belongs and the other, snext, for
the Sc set to which it belongs.
It turns out that an enrollment record indicates explicitly neither the student nor the
course that it represents. The information is implicit in the lists on which the
enrollment record appears. Call the student and course records heading these lists the
owners of the enrollment record. Thus, to tell what courses student s is taking, we
must scan the enrollment records in Cs and find for each one the owner course record.
We could do that by placing a pointer in each enrollment record to the owning course
record, and we would also need a pointer to the owning student record.
While we might wish to use these pointers and thereby answer questions in the
minimum possible time, we can save substantial space† at the cost of slowing down
some computations, if we eliminate these pointers and instead place at the end of each
Sc list a pointer to the owning course and at the end of each Cs list a pointer to the
owning student. Thus, each student and course record becomes part of a ring that
includes all the enrollment records it owns. These rings are depicted in Fig. 4.25 for
the data of Fig. 4.24. Note that each enrollment record has its cnext pointer first and
its snext pointer second.
Fig. 4.25. Multilist representation of Fig. 4.24.
Example 4.11. To answer a question like "which students are taking CS101," we find
the course record for CS101. How we find such a record depends on how the set of
courses is maintained. For example, there might be a hash table containing all such
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (43 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
records, and we obtain the desired record by applying some hash function to
"CS101".
We follow the pointer in the CS101 record to the first enrollment record in the ring
for CS101. This is the second enrollment record from the left. We must then find the
student owner of this enrollment record, which we do by following cnext pointers (the
first pointer in enrollment records) until we reach a student record.† In this case, we
reach the third enrollment record, then the student record for Alex; we now know that
Alex is taking CS101.
Now we must find the next student in CS101. We do so by following the snext
pointer (second pointer) in the second enrollment record, which leads to the fifth
enrollment record. The cnext pointer in that record leads us directly to the owner,
Amy, so Amy is in CS101. Finally, we follow the snext pointer in the fifth enrollment
record to the eighth enrollment record. The ring of cnext pointers from that record
leads to the ninth enrollment record, then to the student record for Ann, so Ann is in
CS101. The snext pointer in the eighth enrollment record leads back to CS101, so we
are done.
Abstractly, we can express the operation in Example 4.11 above as
for each enrollment record in the set for CS101 do begin
s := the student owner of the enrollment record;
print(s)
end
The above assignment to s can be elaborated as
f := e;
repeat
f := f­.cnext
until
f is a pointer to a student record;
s := studentname field of record pointed to by f;
where e is a pointer to the first enrollment record in the set for CS101.
In order to implement a structure like Fig. 4.25 in Pascal, we need to have only
one record type, with variants to cover the cases of student, course, and enrollment
records. Pascal forces this arrangement on us since the fields cnext and snext each
have the ability to point to different record types. This arrangement, however, does
solve one of our other problems. It is now easy to tell what type of record we have
reached as we travel around a ring. Figure 4.26 shows a possible declaration of the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (44 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
records and a procedure that prints the students taking a particular class.
Dual Data Structures for Efficiency
Often, a seemingly simple representation problem for a set or mapping presents a
difficult problem of data structure choice. Picking one data structure for the set makes
certain operations easy, but others take too much time, and it seems that there is no
one data structure that makes all the operations easy. In that case, the solution often
turns out to be the use of two or more different structures for the same set or mapping.
Suppose we wish to maintain a "tennis ladder," in which each player is on a unique
"rung." New players are added to the bottom, that is, the highest- numbered rung. A
player can challenge the player on the rung above, and if the player below wins the
match, they trade rungs. We can represent this situation as an abstract data type,
where the underlying model is a mapping from names (character strings) to rungs
(integers 1, 2, . . . ). The three operations we perform are
1. ADD(name) adds the named person to the ladder at the highest-numbered
rung.
2. CHALLENGE(name) is a function that returns the name of the person on rung
i- 1 if the named person is on rung i, i > 1.
3. EXCHANGE(i) swaps the names of the players on rungs i and i-l, i > l.
type
stype = array[l..20] of char;
ctype = array[1..5] of char;
recordkinds = (student, course, enrollment);
recordtype = record
case kind: recordkinds of
student: (studentname: stype;
firstcourse: ­ recordtype);
course: (coursename: ctype;
firststudent: ­ recordtype);
enrollment: (cnext, snext: ­ recordtype)
end;
procedure printstudents ( cname: ctype );
var
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (45 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
c, e, f: ­ recordtype;
begin
c := pointer to course record with c­.coursename = cname;
{ above depends on how course set is implemented }
e := c ­.firststudent;
{ e runs around the ring of enrollments pointed to by c }
while e ­.kind =
enrollment do begin
f := e;
repeat
f := f­.cnext
until
f­.kind = student;
{ f now points to student owner of enrollment e ­ }
writeln (f­.studentname ) ;
e := e­.snext
end
end
Fig. 4.26. Implementation of search through a multilist.
Notice that we have chosen to pass only the higher rung number to EXCHANGE,
while the other two operations take a name as argument.
We might, for example, choose an array LADDER, where LADDER[i] is the name
of the person on rung i. If we also keep a count of the number of players, adding a
player to the first unoccupied rung can be done in some small constant number of
steps.
EXCHANGE is also easy, as we simply swap two elements of the array. However,
CHALLENGE(name) requires that we examine the entire array for the name, which
takes O(n) time, if n is the number of players on the ladder.
On the other hand, we might consider a hash table to represent the mapping from
names to rungs. On the assumption that we can keep the number of buckets roughly
proportional to the number of players, ADD takes O(1) time, on the average.
Challenging takes O(1) time on the average to look up the given name, but O(n) time
to find the name on the next lower-numbered rung, since the entire hash table may
have to be searched. Exchanging requires O(n) time to find the players on rungs i and
i - 1.
Suppose, however, that we combine the two structures. The cells of the hash table
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (46 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
will contain pairs consisting of a name and a rung, while the array will have in
LADDER[i] a pointer to the cell for the player on rung i as suggested in Fig. 4.27.
Fig. 4.27. Combined structure for high performance.
We can add a name by inserting into the hash table in O(1) time on the average,
and also placing a pointer to the newly created cell into the array LADDER at a
position marked by the cursor nextrung in Fig. 4.27. To challenge, we look up the
name in the hash table, taking O(1) time on the average, get the rung i for the given
player, and follow the pointer in LADDER[i-1] to the cell of the hash table for the
player to be challenged. Consulting LADDER[i - 1] takes constant time in the worst
case, and the lookup in the hash table takes O(1) time on the average, so
CHALLENGE is O(1) in the average case.
EXCHANGE(i) takes O(1) time to find the cells for the players on rungs i and i-l,
swap the rung numbers in those cells, and swap the pointers to the two cells in
LADDER. Thus EXCHANGE requires constant time in even the worst case.
Exercises
4.1
If A = {1, 2, 3} and B = {3, 4, 5}, what are the results of
a. UNION(A,B, C)
b. INTERSECTION(A, B, C)
c. DIFFERENCE(A, B, C)
d. MEMBER(1, A)
e. INSERT(1, A)
f. DELETE(1, A)
g. MIN(A)?
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (47 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
*4.2
Write a procedure in terms of the basic set operations to print all the
elements of a (finite) set. You may assume that a procedure to print an
object of the element type is available. You must not destroy the set you
are printing. What data structures would be most suitable for implementing
sets in this case?
4.3
The bit-vector implementation of sets can be used whenever the "universal
set" can be translated into the integers 1 through N. Describe how this
translation would be done if the universal set were
a. the integers 0, 1, . . . , 99
b. the integers n through m for any n £ m
c. the integers n, n +2, n +4, . . . , n + 2k for any n and k
d. the characters 'a', 'b', . . . , 'z'
e. arrays of two characters, each chosen from 'a' through 'z'.
4.4
Write MAKENULL, UNION, INTERSECTION, MEMBER, MIN,
INSERT, and DELETE procedures for sets represented by lists using the
abstract operations for the sorted list ADT. Note that Fig. 4.5 is a procedure
for INTERSECTION using a specific implementation of the list ADT.
4.5
Repeat Exercise 4.4 for the following set implementations:
a. open hash table (use abstract list operations within buckets).
b. closed hash table with linear resolution of collisions.
c. unsorted list (use abstract list operations).
d. fixed length array and pointer to the last position used.
4.6
For each of the operations and each of the implementations in Exercises 4.4
and 4.5, give the order of magnitude for the running time of the operations
on sets of size n.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (48 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
4.7
Suppose we are hashing integers with a 7-bucket hash table using the hash
function h(i) = i mod 7.
a. Show the resulting open hash table if the perfect cubes 1, 8, 27, 64,
125, 216, 343 are inserted.
b. Repeat part (a) using a closed hash table with linear resolution of
collisions.
4.8
Suppose we are using a closed hash table with 5 buckets and the hashing
function h(i) = i mod 5. Show the hash table that results with linear
resolution of collisions if the sequence 23, 48, 35, 4, 10 is inserted into an
initially empty table.
4.9 Implement the mapping ADT operations using open and closed hash tables.
4.10
To improve the speed of operations, we may wish to replace an open hash
table with B1 buckets holding many more than B1 elements by another hash
table with B2 buckets. Write a procedure to construct the new hash table
from the old, using the list ADT operations to process each bucket.
4.11
In Section 4.8 we discussed "random" hash functions, where hi(x), the
bucket to be tried after i collisions, is (h (x) + di) mod B for some sequence
d1, d2, . . . ,dB-1. We also suggested that one way to compute a suitable
sequence d 1, d2, . . . , dB-1 was to pick a constant k, an arbitrary d1 > 0, and
let
where i > 1, B is a power of 2, and Å stands for the bitwise modulo 2 sum.
If B = 16, find those values of k for which the sequence d1, d2, . . . , d15
includes all the integers 1, 2, . . . , 15.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (49 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
4.12
a. Show the partially ordered tree that results if the integers 5, 6, 4, 9,
3, 1, 7 are inserted into an empty tree.
b. What is the result of three successive DELETEMIN operations on
the tree from (a)?
4.13
Suppose we represent the set of courses by
a. a linked list
b. a hash table
c. a binary search tree
Modify the declarations in Fig. 4.26 for each of these structures.
4.14
Modify the data structure of Fig. 4.26 to give each enrollment record a
direct pointer to its student and course owners. Rewrite the procedure
printstudents of Fig. 4.26 to take advantage of this structure.
4.15
Assuming 20,000 students, 1,000 courses, and each student in an average
of three courses, compare the data structure of Fig. 4.26 with its
modification suggested in Exercise 4.14 as to
a. the amount of space required
b. the average time to execute printstudents
c. the average time to execute the analogous procedure that prints the
courses taken by a given student.
4.16
Assuming the data structure of Fig. 4.26, given course record c and student
record s, write procedures to insert and to delete the fact that s is taking c.
4.17
What, if anything, is the difference between the data structure of Exercise
4.14 and the structure in which sets Cs and Sc are represented by lists of
pointers to course and student records, respectively?
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (50 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
4.18
Employees of a certain company are represented in the company data base
by their name (assumed unique), employee number, and social security
number. Suggest a data structure that lets us, given one representation of an
employee, find the other two representations of the same individual. How
fast, on the average, can you make each such operation?
Bibliographic Notes
Knuth [1973] is a good source for additional information on hashing. Hashing was
developed in the mid-to-late 1950's, and Peterson [1957] is a fundamental early paper
on the subject. Morris [1968] and Maurer and Lewis [1975] are good surveys of
hashing.
The multilist is a central data structure of the network-based database systems
proposed in DBTG [1971]. Ullman [1982] provides additional information on
database applications of structures such as these.
The heap implementation of partially ordered trees is based on an idea in Williams
[1964]. Priority queues are discussed further in Knuth [1973].
Reingold [1972] discusses the computational complexity of basic set operations.
Set-based data flow analysis techniques are treated in detail in Cocke and Allen
[1976] and Aho and Ullman [1977].
†Sometimes the term multiset is used for a "set with repetitions." That is, the multiset
{1, 4, 1} has 1 (twice) and 4 (once) as members. A multiset is not a list any more than
a set is, and thus this multiset could also have been written {4, 1, 1} or {1, 1, 4}.
†Changes to the data structure can be made that will speed up open hashing and allow
closed hashing to handle larger sets. We shall describe these techniques after giving
the basic methods.
†If the type of members of the dictionary does not suggest an appropriate value for
empty, let each bucket have an extra one-bit field to tell whether or not it is empty.
†Note that A2 and A20 do not necessarily hash to the same bucket, but A23 and A41,
for example, must do so.
†If strings can be extremely long, this addition will have to be performed modulo
some constant c. For example, c could be one more than the largest integer obtainable
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (51 of 52) [1.7.2001 19:04:15]
Data Structures and Algorithms: CHAPTER 4: Basic Operations on Sets
from a single block.
†If this were a real database system, the array would be kept in secondary storage.
However, this data structure would waste a great deal of space.
†Note that there are probably many more enrollment records than course or student
records, so shrinking enrollment records shrinks the total space requirement almost as
much.
‡It would in practice be useful to place fields like grade or status (credit or audit) in
enrollment records, but our original problem statement does not require this.
†We must have some way of identifying the type of records; we shall discuss a way
to do this momentarily.
†We assume this and is a conditional and.
Table of Contents Go to Chapter 5
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1204.htm (52 of 52) [1.7.2001 19:04:15]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_1.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_1.gif [1.7.2001 19:04:20]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_2.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_2.gif [1.7.2001 19:04:49]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_3.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_3.gif [1.7.2001 19:04:55]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_4.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_4.gif [1.7.2001 19:05:23]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_6.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_6.gif [1.7.2001 19:05:37]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_7.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_7.gif [1.7.2001 19:05:46]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_10.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_10.gif [1.7.2001 19:05:53]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_12.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_12.gif [1.7.2001 19:06:04]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_15.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_15.gif [1.7.2001 19:06:21]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_16.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_16.gif [1.7.2001 19:06:37]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_18.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_18.gif [1.7.2001 19:06:46]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_19.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_19.gif [1.7.2001 19:06:51]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_20.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_20.gif [1.7.2001 19:06:54]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_21.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_21.gif [1.7.2001 19:07:11]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_22.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_22.gif [1.7.2001 19:07:31]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_23.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_23.gif [1.7.2001 19:07:36]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_24.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_24.gif [1.7.2001 19:07:44]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_25.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_25.gif [1.7.2001 19:07:59]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_29.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_29.gif [1.7.2001 19:08:17]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_31.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig3_31.gif [1.7.2001 19:08:26]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Advanced Set
Representation Methods
This chapter introduces data structures for sets that permit more efficient
implementation of common collections of set operations than those of the previous
chapter. These structures, however, are more complex and are often only appropriate
for large sets. All are based on various kinds of trees, such as binary search trees,
tries, and balanced trees.
5.1 Binary Search Trees
We shall begin with binary search trees, a basic data structure for representing sets
whose elements are ordered by some linear order. We shall, as usual, denote that
order by <. This structure is useful when we have a set of elements from a universe so
large that it is impractical to use the elements of the set themselves as indices into
arrays. An example of such a universe would be the set of possible identifiers in a
Pascal program. A binary search tree can support the set operations INSERT,
DELETE, MEMBER, and MIN, taking O(logn) steps per operation on the average
for a set of n elements.
A binary search tree is a binary tree in which the nodes are labeled with elements
of a set. The important property of a binary search tree is that all elements stored in
the left subtree of any node x are all less than the element stored at x, and all elements
stored in the right subtree of x are greater than the element stored at x. This condition,
called the binary search tree property, holds for every node of a binary search tree,
including the root.
Figure 5.1 shows two binary search trees representing the same set of integers.
Note the interesting property that if we list the nodes of a binary search tree in
inorder, then the elements stored at those nodes are listed in sorted order.
Suppose a binary search tree is used to represent a set. The binary search tree
property makes testing for membership in the set simple. To determine whether x is a
member of the set, first compare x with the element r at the root of the tree. If x = r
we are done and the answer to the membership query is "true." If x < r, then by the
binary search tree property, x can only
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (1 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Fig. 5.1. Two binary search trees.
be a descendant of the left child of the root, if x is present at all.† Similarly, if x > r,
then x could only be at a descendant of the right child of the root.
We shall write a simple recursive function MEMBER(x, A) to implement this
membership test. We assume the elements of the set are of an unspecified type that
will be called elementtype. For convenience, we assume elementtype is a type for
which < and = are defined. If not, we must define functions LT(a, b) and EQ(a, b),
where a and b are of type elementtype, such that LT(a, b) is true if and only if a is
"less than" b, and EQ(a, b) is true if and only if a and b are the same.
The type for nodes consists of an element and two pointers to other nodes:
type
nodetype = record
element: elementtype;
leftchild, rightchild: ­ nodetype
end;
Then we can define the type SET as a pointer to a node, which we take to be the root
of the binary search tree representing the set. That is:
type
SET = ­ nodetype;
Now we can specify fully the function MEMBER, in Fig. 5.2. Notice that since SET
and "pointer to nodetype" are synonymous, MEMBER can call itself on subtrees as if
those subtrees represented sets. In effect, the set can be subdivided into the subset of
members less than x and the subset of members greater than x.
function MEMBER ( x: elementtype; A: SET ): boolean,
{ returns true if x is in A, false otherwise }
begin
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (2 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
if A = nil then
return (false) { x is never in Ø }
else if x = A ­.element then
return (true)
else if x < A ­.element then
return (MEMBER(x, A ­.leftchild ) )
else { x > A ­.element }
return (MEMBER(x, A ­.rightchild ) )
end; { MEMBER }
Fig. 5.2. Testing membership in a binary search tree.
The procedure INSERT(x, A), which adds element x to set A, is also easy to write.
The first action INSERT must do is test whether A = nil, that is, whether the set is
empty. If so, we create a new node to hold x and make A point to it. If the set is not
empty, we search for x more or less as MEMBER does, but when we find a nil
pointer during our search, we replace it by a pointer to a new node holding x. Then x
will be in the right place, namely, the place where the function MEMBER will find it.
The code for INSERT is shown in Fig. 5.3.
Deletion presents some problems. First, we must locate the element x to be deleted
in the tree. If x is at a leaf, we can delete that leaf and be done. However, x may be at
an interior node inode, and if we simply deleted inode, we would disconnect the tree.
If inode has only one child, as the node numbered 14 in Fig. 5.1(b), we can replace
inode by that child, and we shall be left with the appropriate binary search tree. If
inode has two children, as the node numbered 10 in Fig. 5.1(a), then we must find the
lowest-valued element among the descendants of the right child.† For example, in the
case the element 10 is deleted from Fig. 5.1(a), we must replace it by 12, the
minimum-valued descendant of
procedure INSERT ( x: elementtype; var A: SET );
{ add x to set A }
begin
if A = nil then begin
new (A);
A ­ .element: = x;
A ­ .leftchild := nil;
A ­ .rightchild: = nil
end
else if x < A ­.element
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (3 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
then
INSERT(x, A ­.leftchild)
else if x > A ­.element
then
INSERT (x, A ­.rightchild)
{ if x = A ­.element, we do
nothing; x is already in the set }
end; { INSERT }
Fig. 5.3. Inserting an element into a binary search tree.
the right child of 10.
To write DELETE, it is useful to have a function DELETEMIN(A) that removes
the smallest element from a nonempty tree and returns the value of the element
removed. The code for DELETEMIN is shown in Fig. 5.4. The code for DELETE
uses DELETEMIN and is shown in Fig. 5.5.
function DELETEMIN ( var A: SET ): elementtype;
{ returns and removes the smallest element from set A }
begin
if A ­.leftchild = nil then
begin
{ A points to the smallest element }
DELETEMIN := A ­.element;
A := A ­.rightchild
{ replace the node pointed to by A by its right child }
end
else { the node pointed to by A has a left child }
DELETEMIN := DELETEMIN(A ­.leftchild)
end; { DELETEMIN }
Fig. 5.4. Deleting the smallest element.
Example 5.1. Suppose we try to delete 10 from Fig. 5.1(a). Then, in the last
statement of DELETE we call DELETEMIN with argument a pointer to node 14.
That pointer is the rightchild field in the root. That call results in another call to
DELETEMIN. The argument is then a pointer to node 12; this pointer is found in the
leftchild field of node 14. We find that 12 has no
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (4 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
procedure DELETE ( x: elementtype; var A: SET );
{ remove x from set A }
begin
if A < > nil then
if x < A ­.element
then
DELETE(x, A ­.leftchild)
else if x > A ­.element then
DELETE(x, A ­.rightchild)
{ if we reach here, x is at the node pointed to by A }
else if (A ­.leftchild =
nil) and (A ­.rightchild =
nil) then
A := nil { delete the leaf holding x }
else if A ­.leftchild =
nil then
A := A ­.rightchild
else if A ­.rightchild =
nil then
A := A ­.leftchild
else { both children are present }
A ­.element :=
DELETEMIN(A ­.rightchild)
end; { DELETE }
Fig. 5.5. Deletion from a binary search tree.
left child, so we return element 12 and make the left child for 14 be the right child of
12, which happens to be nil. Then DELETE takes the value 12 returned by
DELETEMIN and replaces 10 by it. The resulting tree is shown in Fig. 5.6.
Fig. 5.6. Tree of Fig.5.1 after deleting 1-0.
5.2 Time Analysis of Binary Search Tree
Operations
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (5 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
In this section we analyze the average behavior of various binary search tree
operations. We show that if we insert n random elements into an initially empty
binary search tree, then the average path length from the root to a leaf is O(logn).
Testing for membership, therefore, takes O(logn) time.
It is easy to see that if a binary tree of n nodes is complete (all nodes, except those
at the lowest level have two children), then no path has more than 1+logn nodes.†
Thus, each of the procedures MEMBER, INSERT, DELETE, and DELETEMIN
takes O(logn) time. To see this, observe that they all take a constant amount of time
at a node, then may call themselves recursively on at most one child. Therefore, the
sequence of nodes at which calls are made forms a path from the root. Since that path
is O(logn) in length, the total time spent following the path is O(logn).
However, when we insert n elements in a "random" order, they do not necessarily
arrange themselves into a complete binary tree. For example, if we happen to insert
smallest first, in sorted order, then the resulting tree is a chain of n nodes, in which
each node except the lowest has a right child but no left child. In this case, it is easy
to show that, as it takes O(i) steps to insert the ith element, and , the
whole process of n insertions takes O(n2) steps, or O(n) steps per operation.
We must determine whether the "average" binary search tree of n nodes is closer
to the complete tree in structure than to the chain, that is, whether the average time
per operation on a "random" tree takes O(logn) steps, O(n) steps, or something in
between. As we cannot know the true frequency of insertions and deletions, or
whether deleted elements have some special property (e.g., do we always delete the
minimum?), we can only analyze the average path length of "random" trees if we
make some assumptions. The particular assumptions we make are that trees are
formed by insertions only, and all orders of the n inserted elements are equally likely.
Under these fairly natural assumptions, we can calculate P(n), the average number
of nodes on the path from the root to some node (not necessarily a leaf). We assume
the tree was formed by inserting n random elements into an initially empty tree.
Clearly P(0) = 0 and P(1) = 1. Suppose we have a list of n³2 elements to insert into
an empty tree. The first element on the list, call it a, is equally likely to be first,
second, or nth in the sorted order. Suppose that i elements on the list are less than a,
so n-i-1 are greater than a. When we build the tree, a will appear at the root, the i
smaller elements will be descendants of the left child of the root, and the remaining ni-
1 will be descendants of the right child. This tree is sketched in Fig. 5.7.
As all orders for the i small elements and for the n-i-1 large elements are equally
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (6 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
likely, we expect that the left and right subtrees of the root will
Fig. 5.7. Binary search tree.
have average path lengths P(i) and P(n-i-1), respectively. Since these elements are
reached through the root of the complete tree, we must add 1 to the number of nodes
on every path. Thus P(n) can be calculated by averaging, for all i between 0 and n-1,
the sum
The first term is the average path length in the left subtree, weighted by its size. The
second term is the analogous quantity for the right subtree, and the 1/n term
represents the contribution of the root. By averaging the above sum for all i between
1 and n, we obtain the recurrence
The first part of the summation (5.1), , can be made identical to the
second part if we substitute i for n-i-1 in the second part.
Also, the term for i=0 in the summation is zero, so we can begin the
summation at 1. Thus (5.1) can be written
We shall show by induction on n, starting at n=1, that P(n) £ 1 + 4logn. Surely this
statement is true for n = 1, since P(1) = 1. Suppose it is true for all i < n. Then by
(5.2)
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (7 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
The last step is justified, since , and therefore, the last term of the
second line is at most 1. We shall divide the terms in the summation of (5.3) into two
parts, those for i £ [n/2]-1, which do not exceed ilog(n/2), and those for i> [n/2]-1,
which do not exceed ilogn. Thus (5.3) can be rewritten
Whether n is even or odd, one can show that the first sum of (5.4) does not exceed
(n2/8)log(n/2), which is (n2/8)logn - (n2/8), and the second sum does not exceed
(3n2/8)logn. Thus, we can rewrite (5.4) as
as we wished to prove. This step completes the induction and shows that the average
time to follow a path from the root to a random node of a binary search tree
constructed by random insertions is O(logn), that is to within a constant factor as
good as if the tree were complete. A more careful analysis shows that the constant 4
above is really about 1.4.
We can conclude from the above that the time of membership testing for a random
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (8 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
member of the set takes O(logn) time. A similar analysis shows that if we include in
our average path length only those nodes that are missing both children, or only those
missing left children, then the average path length still obeys an equation similar to
(5.1), and is therefore O(logn). We may then conclude that testing membership of a
random element not in the set, inserting a random new element, and deleting a
random element also all take O(logn) time on the average.
Evaluation of Binary Search Tree
Performance
Hash table implementations of dictionaries require constant time per operation on the
average. Although this performance is better than that for a binary search tree, a hash
table requires O(n) steps for the MIN operation, so if MIN is used frequently, the
binary search tree will be the better choice; if MIN is not needed, we would probably
prefer the hash table.
The binary search tree should also be compared with the partially ordered tree
used for priority queues in Chapter 4. A partially ordered tree with n elements
requires only O(logn) steps for each INSERT and DELETEMIN operation not only
on the average, but also in the worst case. Moreover, the actual constant of
proportionality in front of the logn factor will be smaller for a partially ordered tree
than for a binary search tree. However, the binary search tree permits general
DELETE and MIN operations, as well as the combination DELETEMIN, while the
partially ordered tree permits only the latter. Moreover, MEMBER requires O(n)
steps on a partially ordered tree but only O(logn) steps on a binary search tree. Thus,
while the partially ordered tree is well suited to implementing priority queues, it
cannot do as efficiently any of the additional operations that the binary search tree
can do.
5.3 Tries
In this section we shall present a special structure for representing sets of character
strings. The same method works for representing data types that are strings of objects
of any type, such as strings of integers. This structure is known as the trie, derived
from the middle letters of the word "retrieval." † By way of introduction, consider the
following use of a set of character strings.
Example 5.2. As indicated in Chapter 1, one way to implement a spelling checker is
to read a text file, break it into words (character strings separated by blanks and new
lines), and find those words not in a standard dictionary of words in common use.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (9 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Words in the text but not in the dictionary are printed out as possible misspellings. In
Fig. 5.8 we see a sketch of one possible program spell. It makes use of a procedure
getword(x, f) that sets x to be the next word in text file f; variable x is of a type called
wordtype, which we shall define later. The variable A is of type SET; the SET
operations we shall need are INSERT, DELETE, MAKENULL, and PRINT. The
PRINT operator prints the members of the set.
The trie structure supports these set operations when the elements of the set are
words, i.e., character strings. It is appropriate when many words begin with the same
sequences of letters, that is, when the number of distinct prefixes among all the words
in the set is much less than the total length of all the words.
In a trie, each path from the root to a leaf corresponds to one word in the
represented set. This way, the nodes of the trie correspond to the prefixes of words in
the set. To avoid confusion between words like THE and THEN, let us add a special
endmarker symbol, $, to the ends of all words, so no prefix of a word can be a word
itself.
Example 5.3. In Fig. 5.9 we see a trie representing the set of words {THE, THEN,
THIN, THIS, TIN, SIN, SING}. That is, the root corresponds to the empty string, and
its two children correspond to the prefixes T and S. The
program spell ( input, output, dictionary );
type
wordtype = { to be defined };
SET = { to be defined, using the trie structure };
var
A: SET; { holds input words not yet found in the dictionary }
nextword: wordtype;
dictionary: file of char;
procedure getword ( var x: wordtype; f: file
of char );
{ a procedure to be defined that sets x
to be the next word in file f}
procedure INSERT ( x: wordtype; var S: SET );
{ to be defined }
procedure DELETE (x: wordtype; var S: SET );
{ to be defined }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (10 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
procedure MAKENULL ( var S: SET );
{ to be defined }
procedure PRINT ( var S: SET );
{ to be defined }
begin
MAKENULL(A);
while not eof(input) do begin
getword ( nextword, input);
INSERT(nextword, A)
end;
while not eof (dictionary) do begin
getword ( nextword, dictionary);
DELETE(nextword, A)
end;
PRINT(A)
end; { spell }
Fig. 5.8. Sketch of spelling checker.
leftmost leaf represents the word THE, the next leaf the word THEN, and so on.
We can make the following observations about the trie in Fig. 5.9.
1. Each node has at most 27 children, one for each letter and $.
Fig. 5.9 A Trie.
2. Most nodes will have many fewer than 27 children.
3. A leaf reached by an edge labeled $ cannot have any children, and may as well
not be there.
Trie Nodes as ADT's
We can view a node of a trie as a mapping whose domain is {A, B, ... , Z, $} (or
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (11 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
whatever alphabet we choose) and whose value set is the type "pointer to trie node."
Moreover, the trie itself can be identified with its root, so the ADT's TRIE and
TRIENODE have the same data type, although their operations are substantially
different. On TRIENODE we need the following operations:
1. procedure ASSIGN(node, c, p) that assigns value p (a pointer to a node) to
character c in node node,
2. function VALUEOF(node, c) that produces the value associated with
character c in node,† and
3. procedure GETNEW(node, c) to make the value of node for character c be a
pointer to a new node.
Technically, we also need a procedure MAKENULL(node) to make node be the null
mapping. One simple implementation of trie nodes is an array node of pointers to
nodes, with the index set being {A, B, ..., Z, $}. That is, we define
type
chars = ('A', 'B', ... , 'Z', '$');
TRIENODE = array[chars] of ­;
TRIENODE;
If node is a trie node, node[c] is VALUEOF(node, c) for any c in the set chars. To
avoid creating many leaves that are children corresponding to '$', we shall adopt the
convention that node['$'] is either nil or a pointer to the node itself. In the former
case, node has no child corresponding to '$', and in the latter case it is deemed to have
such a child, although we never create it. Then we can write the procedures for trie
nodes as in Fig. 5.10.
procedure MAKENULL ( var node: TRIENODE );
{ makes node a leaf, i.e., a null mapping }
var
c: chars;
begin
for c: = 'A' to '$' do
node [c] := nil
end; { MAKENULL }
procedure ASSIGN ( var node: TRIENODE; c: chars;
p: ­ TRIENODE );
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (12 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
begin
node [c] := p
end; { ASSIGN }
function VALUEOF ( var node: TRIENODE; c: chars ):
­ TRIENODE;
begin
return (node [c])
end; { VALUEOF }
procedure GETNEW ( var node: TRIENODE; c: chars );
begin
new (node [c]);
MAKENULL(node [c])
end; { GETNEW }
Fig. 5.10 Operation in trie nodes.
Now let us define
type
TRIE = ­ TRIENODE;
We shall assume wordtype is an array of characters of some fixed length. The value
of such an array will always be assumed to have at least one '$'; we take the end of
the represented word to be the first '$', no matter what follows (presumably more '$'
's). On this assumption, we can write the procedure INSERT(x, words) to insert x into
set words represented by a trie, as shown in Fig. 5.11. We leave the writing of
MAKENULL, DELETE, and PRINT for tries represented as arrays for exercises.
procedure INSERT ( x: wordtype; var words: TRIE );
var
i: integer; { counts positions in word x }
t: TRIE; { used to point to trie nodes
corresponding to prefixes of x }
begin
i : = 1;
t := words;
while x[i] <> '$' do begin
if VALUEOF(t­,
x[i]) = nil then
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (13 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
{ if current node has no child for character x[i],
create one }
GETNEW(t­, x[i]);
t := VALUEOF(t­,
x[i]);
{ proceed to the child of t for character x[i],
whether or not that child was just created }
i := i+1 { move along the word x }
end;
{ now we have reached the first '$' in x }
ASSIGN(t­, '$', t)
{ make loop for '$' to represent a leaf }
end; {INSERT}
Fig. 5.11 The procedure INSERT.
A List Representation for Trie Nodes
The array representation of trie nodes takes a collection of words, having among them
p different prefixes, and represents them with 27p bytes of storage. That amount of
space could far exceed the total length of the words in the set. However, there is
another implementation of tries that may save space. Recall that each trie node is a
mapping, as discussed in Section 2.6. In principle, any implementation of mappings
would do. Yet in practice, we want a representation suitable for mappings with a
small domain and for mappings defined for comparatively few members of that
domain. The linked list representation for mappings satisfies these requirements
nicely. We may represent the mapping that is a trie node by a linked list of the
characters for which the associated value is not the nil pointer. That is, a trie node is a
linked list of cells of the type
type
celltype = record
domain: chars;
value: ­ celltype;
{ pointer to first cell on list for the child node }
next: ­ celltype
{ pointer to next cell on the list }
end;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (14 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
We shall leave the procedures ASSIGN, VALUEOF, MAKENULL, and
GETNEW for this implementation of trie nodes as exercises. After writing these
procedures, the INSERT operations on tries, in Fig. 5.11, and the other operations on
tries that we left as exercises, should work correctly.
Evaluation of the Trie Data Structure
Let us compare the time and space needed to represent n words with a total of p
different prefixes and a total length of l using a hash table and a trie. In what follows,
we shall assume that pointers require four bytes. Perhaps the most space-efficient
way to store words and still support INSERT and DELETE efficiently is in a hash
table. If the words are of varying length, the cells of the buckets should not contain
the words themselves; rather, the cells consist of two pointers, one to link the cells of
the bucket and the other pointing to the beginning of a word belonging in the bucket.
The words themselves are stored in a large character array, and the end of each
word is indicated by an endmarker character such as '$'. For example, the words THE,
THEN, and THIN could be stored as
THE$THEN$THIN$ . . .
The pointers for the three words are cursors to positions 1, 5, and 10 of the array. The
amount of space used in the buckets and character array is
1. 8n bytes for the cells of the buckets, there being one cell for each of the n
words, and a cell has two pointers or 8 bytes,
2. l + n bytes for the character array to store the n words of total length l and
their endmarkers.
The total space is thus 9n + l bytes plus whatever amount is used for the bucket
headers.
In comparison, a trie with nodes implemented by linked lists requires p + n cells,
one cell for each prefix and one cell for the end of each word. Each trie cell has a
character and two pointers, and needs nine bytes, for a total space of 9n + 9p. If l plus
the space for the bucket headers exceeds 9p, the trie uses less space. However, for
applications such as storing a dictionary where l/p is typically less than 3, the hash
table would use less space.
In favor of the trie, however, let us note that we can travel down a trie, and thus
perform operations INSERT, DELETE, and MEMBER in time proportional to the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (15 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
length of the word involved. A hash function to be truly "random" should involve
each character of the word being hashed. It is fair, therefore, to state that computing
the hash function takes roughly as much time as performing an operation like
MEMBER on the trie. Of course the time spent computing the hash function does not
include the time spent resolving collisions or performing the insertion, deletion, or
membership test on the hash table, so we can expect tries to be considerably faster
than hash tables for dictionaries whose elements are character strings.
Another advantage to the trie is that it supports the MIN operation efficiently,
while hash tables do not. Further, in the hash table organization described above, we
cannot easily reuse the space in the character array when a word is deleted (but see
Chapter 12 for methods of handling such a problem).
5.4 Balanced Tree Implementations of
Sets
In Sections 5.1 and 5.2 we saw how sets could be implemented by binary search
trees, and we saw that operations like INSERT could be performed in time
proportional to the average depth of nodes in that tree. Further, we discovered that
this average depth is O(logn) for a "random" tree of n nodes. However, some
sequences of insertions and deletions can produce binary search trees whose average
depth is proportional to n. This suggests that we might try to rearrange the tree after
each insertion and deletion so that it is always complete; then the time for INSERT
and similar operations would always be O(logn).
In Fig. 5.12(a) we see a tree of six nodes that becomes the complete tree of seven
nodes in Fig. 5.12(b), when element 1 is inserted. Every element in Fig. 5.12(a),
however, has a different parent in Fig. 5.12(b), so we must take n steps to insert 1 into
a tree like Fig. 5.12(a), if we wish to keep the tree as balanced as possible. It is thus
unlikely that simply insisting that the binary search tree be complete will lead to an
implementation of a dictionary, priority queue, or other ADT that includes INSERT
among its operations, in O(logn) time.
There are several other approaches that do yield worst case O(logn) time per
operation for dictionaries and priority queues, and we shall consider one of them,
called a "2-3 tree," in detail. A 2-3 tree is a tree with the following two properties.
1. Each interior node has two or three children.
2. Each path from the root to a leaf has the same length.
We shall also consider a tree with zero nodes or one node as special cases of a 2-3
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (16 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
tree.
We represent sets of elements that are ordered by some linear order <, as follows.
Elements are placed at the leaves; if element a is to the left of
Fig. 5.12. Complete trees.
element b, then a < b must hold. We shall assume that the "<" ordering of elements is
based on one field of a record that forms the element type; this field is called the key.
For example, elements might represent people and certain information about people,
and in that case, the key field might be "social security number."
At each interior node we record the key of the smallest element that is a
descendant of the second child and, if there is a third child, we record the key of the
smallest element descending from that child as well.† Figure 5.13 is an example of a
2-3 tree. In that and subsequent examples, we shall identify an element with its key
field, so the order of elements becomes obvious.
Fig. 5.13 A 2-3 tree.
Observe that a 2-3 tree of k levels has between 2k-1 and 3k-1 leaves. Put another
way, a 2-3 tree representing a set of n elements requires at least 1 + log3n levels and
no more than 1 + log2n levels. Thus, path lengths in the tree are O(logn).
We can test membership of a record with key x in a set represented by a 2-3 tree in
O(logn) time by simply moving down the tree, using the values of the elements
recorded at the interior nodes to guide our path. At a node node, compare x with the
value y that represents the smallest element descending from the second child of
node. (Recall we are treating elements as if they consisted solely of a key field.) If x <
y, move to the first child of node. If x³y, and node has only two children, move to the
second child of node. If node has three children and x³y, compare x with z, the
second value recorded at node, the value that indicates the smallest descendant of the
third child of node. If x<z, go to the second child, and if x³z, go to the third child. In
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (17 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
this manner, we find ourselves at a leaf eventually, and x is in the represented set if
and only if x is at the leaf. Evidently, if during this process we find x=y or x=z, we
can stop immediately. However, we stated the algorithm as we did because in some
cases we shall wish to find the leaf with x as well as to verify its existence.
Insertion into a 2-3 Tree
To insert a new element x into a 2-3 tree, we proceed at first as if we were testing
membership of x in the set. However, at the level just above the leaves, we shall be at
a node node whose children, we discover, do not include x. If node has only two
children, we simply make x the third child of node, placing the children in the proper
order. We then adjust the two numbers at node to reflect the new situation.
For example, if we insert 18 into Fig. 5.13, we wind up with node equal to the
rightmost node at the middle level. We place 18 among the children of node, whose
proper order is 16, 18, 19. The two values recorded at node become 18 and 19, the
elements at the second and third children. The result is shown in Fig. 5.14.
Suppose, however, that x is the fourth, rather than the third child of node. We
cannot have a node with four children in a 2-3 tree, so we split node into two nodes,
which we call node and node'. The two smallest elements among the four children of
node stay with node, while the two larger elements become children of node'. Now,
we must insert node' among the children of p, the parent of node. This part of the
insertion is analogous to the insertion of a leaf as a child of node. That is, if p had two
children, we make node' the third and place it immediately to the right of node. If p
had three children before node' was created, we split p into p and p', giving p the two
leftmost children and p' the remaining two, and then we insert p' among the children
of p's parent, recursively.
One special case occurs when we wind up splitting the root. In that case we create
a new root, whose two children are the two nodes into which the
Fig. 5.14 2-3 tree with 18 inserted.
old root was split. This is how the number of levels in a 2-3 tree increases.
Example 5.4. Suppose we insert 10 into the tree of Fig. 5.14. The intended parent of
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (18 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
10 already has children 7, 8, and 12, so we split it into two nodes. The first of these
has children 7 and 8; the second has 10 and 12. The result is shown in Fig. 5.15(a).
We must now insert the new node with children 10 and 12 in its proper place as a
child of the root of Fig. 5.15(a). Doing so gives the root four children, so we split it,
and create a new root, as shown in Fig. 5.15(b). The details of how information
regarding smallest elements of subtrees is carried up the tree will be given when we
develop the program for the command INSERT.
Deletion in a 2-3 tree
When we delete a leaf, we may leave its parent node with only one child. If node is
the root, delete node and let its lone child be the new root. Otherwise, let p be the
parent of node. If p has another child, adjacent to node on either the right or the left,
and that child of p has three children, we can transfer the proper one of those three to
node. Then node has two children, and we are done.
If the children of p adjacent to node have only two children, transfer the lone child
of node to an adjacent sibling of node, and delete node. Should p now have only one
child, repeat all the above, recursively, with p in place of node.
Example 5.5. Let us begin with the tree of Fig. 5.15(b). If 10 is deleted, its parent has
only one child. But the grandparent has another child that has three children, 16, 18,
and 19. This node is to the right of the deficient node, so we pass the deficient node
the smallest element, 16, leaving the 2-3 tree in Fig. 5.16(a).
Fig. 5.15 Insertion of 10 into the tree of Fig.5.14.
Next suppose we delete 7 from the tree of Fig. 5.16(a). Its parent now has only one
child, 8, and the grandparent has no child with three children. We therefore make 8 be
a sibling of 2 and 5, leaving the tree of Fig. 5.16(b). Now the node starred in Fig.
5.16(b) has only one child, and its parent has no other child with three children. Thus
we delete the starred node, making its child be a child of the sibling of the starred
node. Now the root has only one child, and we delete it, leaving the tree of Fig.
5.16(c).
Observe in the above examples, the frequent manipulation of the values at interior
nodes. While we can always calculate these values by walking the tree, it can be done
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (19 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
as we manipulate the tree itself, provided we remember the smallest value among the
descendants of each node along the path from the root to the deleted leaf. This
information can be computed by a recursive deletion algorithm, with the call at each
node being passed, from above, the correct quantity (or the value "minus infinity" if
we are along the leftmost path). The details require careful case analysis and will be
sketched later when we consider the program for the DELETE operation.
Data Types for 2-3 Trees
Let us restrict ourselves to representing by 2-3 trees sets of elements whose keys are
real numbers. The nature of other fields that go with the field key, to make up a
record of type elementtype, we shall leave unspecified, as it has no bearing on what
follows.
Fig. 5.16 Deletion in a 2-3 tree.
In Pascal, the parents of leaves must be records of consisting of two reals (the key
of the smallest elements in the second and third subtrees) and of three pointers to
elements. The parents of these nodes are records consisting of two reals and of three
pointers to parents of leaves. This progression continues indefinitely; each level in a 2-
3 tree is of a different type from all other levels. Such a situation would make
programming 2-3 tree operations in Pascal impossible, but fortunately, Pascal
provides a mechanism, the variant record structure, that enables us to regard all 2-3
tree nodes as having the same type, even though some are elements, and some are
records with pointers and reals.† We can define nodes as in Fig. 5.17. Then we
declare a set, represented by a 2-3 tree, to be a pointer to the root as in Fig. 5.17.
Implementation of INSERT
The details of operations on 2-3 trees are quite involved, although the principles are
simple. We shall therefore describe only one operation, insertion, in detail; the others,
deletion and membership testing, are similar in spirit, and finding the minimum is a
trivial search down the leftmost path. We shall write the insertion routine as main
procedure, INSERT, which we call at the root, and a procedure insert 1, which gets
called recursively down the tree.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (20 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
type
elementtype = record
key: real;
{other fields as warranted}
end;
nodetypes = (leaf, interior);
twothreenode = record
case kind: nodetypes of
leaf: (element: elementtype);
interior: (firstchild, secondchild, thirdchild: ­ twothreenode;
lowofsecond, lowofthird: real)
end;
SET = ­ twothreenode;
Fig. 5.17. Definition of a node in a 2-3 tree.
For convenience, we assume that a 2-3 tree is not a single node or empty. These two
cases require a straightforward sequence of steps which the reader is invited to
provide as an exercise.
procedure insert1 ( node: ­ twothreenode;
x: elementtype; { x is to be inserted into the subtree of node }
var pnew: ­ twothreenode; { pointer to new node
created to right of node }
var low: real ); { smallest element in the subtree pointed to by pnew}
begin
pnew := nil;
if node is a leaf then begin
if x is not the element at node then begin
create new node pointed to by pnew;
put x at the new node;
low := x.key
end
end
else begin { node is an interior node }
let w be the child of node to whose subtree x belongs;
insert1(w, x, pback, lowback);
if pback < > nil then begin
insert pointer pback among the children of node just
to the right of w;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (21 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
if node has four children then begin
create new node pointed to by pnew;
give the new node the third and fourth children of node;
adjust lowofsecond and lowofthird in node and the new node;
set low to be the lowest key among the
children of the new node
end
end
end
end; { insert 1 }
Fig. 5.18. Sketch of 2-3 tree insertion program.
We would like insert 1 to return both a pointer to a new node, if it must create one,
and the key of the smallest element descended from that new node. As the
mechanism in Pascal for creating such a function is awkward, we shall instead
declare insert 1 to be a procedure that assigns values to parameters pnew and low in
the case that it must "return" a new node. We sketch insert 1 in Fig. 5.18. The
complete procedure is shown in Fig. 5.19; some comments in Fig. 5.18 have been
omitted from Fig. 5.19 to save space.
procedure insert 1 ( node: ­ twothreenode;
x: elementtype;
var pnew: ­ twothreenode; var
low: real );
var
pback: ­ twothreenode;
lowback : real;
child: 1..3; { indicates which child of node is followed
in recursive call (cf. w in Fig. 5.18) }
w: ­ twothreenode; { pointer to the child }
begin
pnew := nil;
if node ­.kind = leaf
then begin
if node ­.element.key < >
x .key then begin
{ create new leaf holding x and "return" this node }
new (pnew, leaf);
if (node ­.element.key < x.key ) then
{ place x in new node to right of current node }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (22 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
begin pnew ­.element
:= x; low := x .key end
else begin { x belongs to left of element at current node }
pnew ­.element := node
­.element;
node ­.element : = x;
low: =pnew ­.element.key
end
end
end
else begin { node is an interior node }
{ select the child of node that we must follow }
if x.key < node ­.lowofsecond then
begin child := 1; w := node ­.firstchild end
else if (node ­.thirdchild
= nil) or (x.key < node ­.lowofthird) then begin
{ x is in second subtree }
child := 2;
w := node ­.secondchild
end
else begin { x is in third subtree }
child := 3;
w := node ­.thirdchild
end;
insert 1 ( w , x, pback, lowback ) ;
if pback < > nil then
{ a new child of node must be inserted }
if node ­.thirdchild =
nil then
{ node had only two children, so insert new node in proper place }
if child = 2 then begin
node ­.thirdchild :=
pback ;
node ­.lowofthird :=
lowback
end
else begin { child = 1 }
node ­.thirdchild :=
node ­.secondchild ;
node ­.lowofthird : =
node ­.lowofsecond ;
node ­.secondchild :=
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (23 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
pback ;
node ­.lowofsecond :=
lowback
end
else begin { node already had three children }
new (pnew , interior );
if child = 3 then begin
{ pback and third child become children of new node }
pnew ­.firstchild :=
node ­.thirdchild ;
pnew ­.secondchild : =
pback ;
pnew ­.thirdchild :=
nil;
pnew ­.lowofsecond :=
lowback;
{ lowofthird is undefined for pnew}
low := node ­.lowofthird ;
node ­.thirdchild := nil
end
else begin { child ­ 2; move third
child of node to pnew}
pnew ­.secondchild :=
node ­.thirdchild ;
pnew ­.lowofsecond :=
node ­.lowofthird ;
pnew ­.thirdchild : =
nil;
node ­.thirdchild := nil
end;
if child = 2 then begin
{ pback becomes first child of pnew}
pnew ­.firstchild :=
pback ;
low := lowback
end;
if child = 1 then begin
{ second child of node is moved to pnew;
pback becomes second child of node }
pnew ­.firstchild :=
node ­.secondchild ;
low := node ­.lowofsecond ;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (24 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
node ­.secondchild : =
pback ;
node ­.lowofsecond :=
lowback
end
end
end
end; { insert 1 }
Fig. 5.19. The procedure insert 1.
Now we can write the procedure INSERT, which calls insert 1. If insert 1
"returns" a new node, then INSERT must create a new root. The code is shown in
Fig. 5.20 on the assumption that the type SET is ­ twothreenode, i.e., a pointer to the
root of a 2-3 tree whose leaves contain the members of the set.
procedure INSERT ( x: elementtype; var S: SET );
var
pback: ­ twothreenode; { pointer to new node
returned by insert 1 }
lowback: real; { low value in subtree of pback }
saveS: SET; { place to store a temporary copy of the pointer S }
begin
{ checks for S being empty or a single node should occur here,
and an appropriate insertion procedure should be included }
insert 1(S, x, pback , lowback ) ;
if pback < > nil then begin
{ create new root; its children are now pointed to by S and pback }
saveS := S;
new(S);
S ­.firstchild := saveS;
S ­.secondchild := pback;
S ­.lowofsecond := lowback;
S ­.thirdchild := nil;
end
end; { INSERT }
Fig. 5.20. INSERT for sets represented by 2-3 trees.
Implementation of DELETE
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (25 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
We shall sketch a function delete 1 that takes a pointer to a node node and an element
x, and deletes a leaf descended from node having value x, if there is one.† Function
delete 1 returns true if after deletion node has only one child, and it returns false if
node still has two or three children. A sketch of the code for delete 1 is shown in Fig.
5.21.
We leave the detailed code for function delete 1 for the reader. Another exercise is
to write a procedure DELETE(S, x) that checks for the special cases that the set S
consists only of a single leaf or is empty, and otherwise calls delete 1(S, x); if delete 1
returns true, the procedure removes the root (the node pointed to by S) and makes S
point to its lone child.
function delete 1 ( node : ­ twothreenode;
x: elementtype ) : boolean;
var
onlyone: boolean; { to hold the value returned by a call to delete 1 }
begin
delete 1 := false;
if the children of node are leaves then begin
if x is among those leaves then begin
remove x;
shift children of node to the right of x one position left;
if node now has one child then
delete 1 := true
end
end
else begin { node is at level two or higher }
determine which child of node could have x as a descendant;
onlyone := delete 1(w, x); ( w stands for node
­.firstchild,
node ­.secondchild, or node
­.thirdchild, as appropriate }
if onlyone then begin ( fix children of node )
if w is the first child of node then
if y, the second child of node, has three children then
make the first child of y be the second child of w
else begin { y has two children }
make the child of w be the first child of y;
remove w from among the children of node;
if node now has one child then
delete 1 := true
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (26 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
end;
if w is the second child of node then
if y, the first child of node, has three children then
make the third child of y be the first child of w
else { y has two children }
if z, the third child of node, exists
and has three children then
make first child of z be the second child of w
else begin { no other child of node has three children }
make the child of w be the third child of y;
remove w from among the children of node;
if node now has one child then
delete 1 := true
end;
if w is the third child of node then
if y, the second child of node, has three children then
make the third child of y be the first child of w
else begin { y has two children }
make the child of w be the third child of y;
remove w from among the children of node
end { note node surely has two children left in this case }
end
end
end; { delete 1 }
Fig. 5.21. Recursive deletion procedure.
5.5 Sets with the MERGE and FIND
Operations
In certain problems we start with a collection of objects, each in a set by itself; we
then combine sets in some order, and from time to time ask which set a particular
object is in. These problems can be solved using the operations MERGE and FIND.
The operation MERGE(A, B, C) makes C equal to the union of sets A and B, provided
A and B are disjoint (have no member in common); MERGE is undefined if A and B
are not disjoint. FIND(x) is a function that returns the set of which x is a member; in
case x is in two or more sets, or in no set, FIND is not defined.
Example 5.6. An equiva
