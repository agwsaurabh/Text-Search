lowback
end
end
end
end; { insert 1 }
Fig. 5.19. The procedure insert 1.
Now we can write the procedure INSERT, which calls insert 1. If insert 1
"returns" a new node, then INSERT must create a new root. The code is shown in
Fig. 5.20 on the assumption that the type SET is ­ twothreenode, i.e., a pointer to the
root of a 2-3 tree whose leaves contain the members of the set.
procedure INSERT ( x: elementtype; var S: SET );
var
pback: ­ twothreenode; { pointer to new node
returned by insert 1 }
lowback: real; { low value in subtree of pback }
saveS: SET; { place to store a temporary copy of the pointer S }
begin
{ checks for S being empty or a single node should occur here,
and an appropriate insertion procedure should be included }
insert 1(S, x, pback , lowback ) ;
if pback < > nil then begin
{ create new root; its children are now pointed to by S and pback }
saveS := S;
new(S);
S ­.firstchild := saveS;
S ­.secondchild := pback;
S ­.lowofsecond := lowback;
S ­.thirdchild := nil;
end
end; { INSERT }
Fig. 5.20. INSERT for sets represented by 2-3 trees.
Implementation of DELETE
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (25 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
We shall sketch a function delete 1 that takes a pointer to a node node and an element
x, and deletes a leaf descended from node having value x, if there is one.† Function
delete 1 returns true if after deletion node has only one child, and it returns false if
node still has two or three children. A sketch of the code for delete 1 is shown in Fig.
5.21.
We leave the detailed code for function delete 1 for the reader. Another exercise is
to write a procedure DELETE(S, x) that checks for the special cases that the set S
consists only of a single leaf or is empty, and otherwise calls delete 1(S, x); if delete 1
returns true, the procedure removes the root (the node pointed to by S) and makes S
point to its lone child.
function delete 1 ( node : ­ twothreenode;
x: elementtype ) : boolean;
var
onlyone: boolean; { to hold the value returned by a call to delete 1 }
begin
delete 1 := false;
if the children of node are leaves then begin
if x is among those leaves then begin
remove x;
shift children of node to the right of x one position left;
if node now has one child then
delete 1 := true
end
end
else begin { node is at level two or higher }
determine which child of node could have x as a descendant;
onlyone := delete 1(w, x); ( w stands for node
­.firstchild,
node ­.secondchild, or node
­.thirdchild, as appropriate }
if onlyone then begin ( fix children of node )
if w is the first child of node then
if y, the second child of node, has three children then
make the first child of y be the second child of w
else begin { y has two children }
make the child of w be the first child of y;
remove w from among the children of node;
if node now has one child then
delete 1 := true
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (26 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
end;
if w is the second child of node then
if y, the first child of node, has three children then
make the third child of y be the first child of w
else { y has two children }
if z, the third child of node, exists
and has three children then
make first child of z be the second child of w
else begin { no other child of node has three children }
make the child of w be the third child of y;
remove w from among the children of node;
if node now has one child then
delete 1 := true
end;
if w is the third child of node then
if y, the second child of node, has three children then
make the third child of y be the first child of w
else begin { y has two children }
make the child of w be the third child of y;
remove w from among the children of node
end { note node surely has two children left in this case }
end
end
end; { delete 1 }
Fig. 5.21. Recursive deletion procedure.
5.5 Sets with the MERGE and FIND
Operations
In certain problems we start with a collection of objects, each in a set by itself; we
then combine sets in some order, and from time to time ask which set a particular
object is in. These problems can be solved using the operations MERGE and FIND.
The operation MERGE(A, B, C) makes C equal to the union of sets A and B, provided
A and B are disjoint (have no member in common); MERGE is undefined if A and B
are not disjoint. FIND(x) is a function that returns the set of which x is a member; in
case x is in two or more sets, or in no set, FIND is not defined.
Example 5.6. An equivalence relation is a reflexive, symmetric, and transitive
relation. That is, if º is an equivalence relation on set S, then for any (not necessarily
distinct) members a, b, and c in S, the following properties hold:
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (27 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
1. a º a (reflexivity)
2. If a º b, then b º a (symmetry).
3. If a º b and b º c, then a º c (transitivity).
The relation "is equal to" (=) is the paradigm equivalence relation on any set S.
For a, b, and c in S, we have (1) a = a, (2) if a = b, then b = a, and (3) if a = b and b =
c, then a = c. There are many other equivalence relations, however, and we shall
shortly see several additional examples.
In general, whenever we partition a collection of objects into disjoint groups, the
relation a º b if and only if a and b are in the same group is an equivalence relation.
"Is equal to" is the special case where every element is in a group by itself.
More formally, if a set S has an equivalence relation defined on it, then the set S
can be partitioned into disjoint subsets S1, S2, ..., called equivalence classes, whose
union is S. Each subset Si consists of equivalent members of S. That is, a º b for all a
and b in Si, and a ?? b if a and b are in different subsets. For example, the relation
congruence modulo n† is an equivalence relation on the set of integers. To check that
this is so, note that a-a = 0, which is a multiple of n (reflexivity); if a-b = dn, then b-a
= (-d)n (symmetry); and if a-b = dn and b-c = en, then a-c = (d+e)n (transitivity). In
the case of congruence modulo n there are n equivalence classes, which are the set of
integers congruent to 0, the set of integers congruent to 1, ... , the set of integers
congruent to n - 1.
The equivalence problem can be formulated in the following manner. We are
given a set S and a sequence of statements of the form "a is equivalent to b." We are
to process the statements in order in such a way that at any time we are able to
determine in which equivalence class a given element belongs. For example, suppose
S = {1, 2, ... , 7} and we are given the sequence of statements
1º2 5º6 3º4 1º4
to process. The following sequence of equivalence classes needs to be constructed,
assuming that initially each element of S is in an equivalence class by itself.
1º2 {1,2} {3} {4} {5} {6} {7}
5º6 {1,2} {3} {4} {5,6} {7}
3º4 {1,2} {3,4} {5,6} {7}
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (28 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
1º4 {1,2,3,4} {5,6} {7}
We can "solve" the equivalence problem by starting with each element in a named
set. When we process statement aºb, we FIND the equivalence classes of a and b and
then MERGE them. We can at any time use FIND to tell us the current equivalence
class of any element.
The equivalence problem arises in several areas of computer science. For example,
one form occurs when a Fortran compiler has to process "equivalence declarations"
such as
EQUIVALENCE (A(1),B(1,2),C(3)), (A(2),D,E), (F,G)
Another example, presented in Chapter 7, uses solutions to the equivalence
problem to help find minimum-cost spanning trees.
A Simple Implementation of MFSET
Let us begin with a simplified version of the MERGE-FIND ADT. We shall define an
ADT, called MFSET, consisting of a set of subsets, which we shall call components,
together with the following operations:
1. MERGE(A, B) takes the union of the components A and B and calls the result
either A or B, arbitrarily.
2. FIND(x) is a function that returns the name of the component of which x is a
member.
3. INITIAL(A, x) creates a component named A that contains only the element x.
To make a reasonable implementation of MFSET, we must restrict our underlying
types, or alternatively, we should recognize that MFSET really has two other types as
"parameters" - the type of set names and the type of members of these sets. In many
applications we can use integers as set names. If we take n to be the number of
elements, we may also use integers in the range [1..n] for the members of
components. For the implementation we have in mind, it is important that the type of
set members be a subrange type, because we want to index into an array defined over
that subrange. The type of set names is not important, as this type is the type of array
elements, not their indices. Observe, however, that if we wanted the member type to
be other than a subrange type, we could create a mapping, with a hash table, for
example, that assigned these to unique integers in a subrange. We only need to know
the total number of elements in advance.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (29 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
The implementation we have in mind is to declare
const
n = { number of elements };
type
MFSET = array[1..n] of integer;
as a special case of the more general type
array[subrange of members] of (type of set names);
Suppose we declare components to be of type MFSET with the intention that
components[x] holds the name of the set currently containing x. Then the three
MFSET operations are easy to write. For example, the operation MERGE is shown in
Fig. 5.22. INITIAL(A, x) simply sets components[x] to A, and FIND(x) returns
components [x].
The time performance of this implementation of MFSET is easy to
procedure MERGE ( A, B: integer; var C: MFSET );
var
x: 1..n;
begin
for x:= 1 to n do
if C[x] = B then
C[x] := A
end; { MERGE }
Fig. 5.22. The procedure MERGE.
analyze. Each execution of the procedure MERGE takes O(n) time. On the other
hand, the obvious implementations of INITIAL(A, x) and FIND(x) have constant
running times.
A Faster Implementation of MFSET
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (30 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Using the algorithm in Fig. 5.22, a sequence of n-1 MERGE instructions will take
O(n2) time.† One way to speed up the MERGE operation is to link together all
members of a component in a list. Then, instead of scanning all members when we
merge component B into A, we need only run down the list of members of B. This
arrangement saves time on the average. However, it could happen that the ith merge is
of the form MERGE(A, B) where A is a component of size 1 and B is a component of
size i, and that the result is named A. This merge operation would require O(i) steps,
and a sequence of n-1 such merge instructions would take on the order of
time.
One way to avoid this worst case situation is to keep track of the size of each
component and always merge the smaller into the larger.‡ Thus, every time a member
is merged into a bigger component, it finds itself in a component at least twice as big.
Thus, if there are initially n components, each with one member, none of the n
members can have its component changed more than 1+log n times. As the time spent
by this new version of MERGE is proportional to the number of members whose
component names are changed, and the total number of such changes is at most
n(1+log n), we see that O(n log n) work suffices for all merges.
Now let us consider the data structure needed for this implementation. First, we
need a mapping from set names to records consisting of
1. a count giving the number of members in the set and
2. the index in the array of the first element of that set.
We also need another array of records, indexed by members, to indicate
1. the set of which each element is a member and
2. the next array element on the list for that set.
We use 0 to serve as NIL, the end-of-list marker. In a language that lent itself to such
constructs, we would prefer to use pointers in this array, but Pascal does not permit
pointers into arrays.
In the special case where set names, as well as members, are chosen from the
subrange 1..n, we can use an array for the mapping described above. That is, we
define
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (31 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
type
nametype: 1..n;
elementtype = 1..n;
MFSET = record
setheaders: array[ 1..n] of record
{ headers for set lists }
count: 0..n;
firstelement: 0..n
end;
names; array[1..n] of record
{ table giving set containing each member }
setname: nametype;
nextelement: 0..n
end
end;
The procedures INITIAL, MERGE, and FIND are shown in Fig. 5.23.
Figure 5.24 shows an example of the data structure used in Fig. 5.23, where set 1
is {1, 3, 4}, set 2 is {2}, and set 5 is {5, 6}.
A Tree Implementation of MFSET's
Another, completely different, approach to the implementation of MFSET's uses trees
with pointers to parents. We shall describe this approach informally. The basic idea is
that nodes of trees correspond to set members, with an array or other implementation
of a mapping leading from set members to their nodes. Each node, except the root of
each tree, has a pointer to its parent. The roots hold the name of the set, as well as an
element. A mapping from set names to roots allows access to any given set, when
merges are done.
Figure 5.25 shows the sets A = {1, 2, 3, 4}, B = {5, 6}, and C = {7} represented in
this form. The rectangles are assumed to be part of the root node, not separate nodes.
procedure INITIAL ( A: nametype; x: elementtype; var C:
MFSET );
{ initialize A to a set containing x only }
begin
C.names [x].setname : = A;
C.names [x].nextelement := 0;
{ null pointer at end of list of members of A }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (32 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
C.setheaders [A].count := 1;
C.setheaders [A].firstelement := x
end; { INITIAL }
procedure MERGE ( A, B: nametype; var C: MFSET );
{ merge A and B, calling the result A or B, arbitrarily }
var
i: 0..n; { used to find end of smaller list }
begin
if C.setheaders [A ].count > C.setheaders
[B].count then begin
{ A is the larger set; merge B into A }
{ find end of B, changing set names to A as we go }
i := C.setheaders[B].firstelement;
while C.names [i].nextelement <> 0 do
begin
C.names [i].setname := A;
i := C.names [i].nextelement
end;
{ append list A to the end of B and call the result A }
{ now i is the index of the last member of B }
C.names [i].setname := A;
C.names [i].nextelement := C.setheaders [A
].firstelement;
C.setheaders [A ].firstelement := C.setheaders [B
].firstelement;
C.setheaders [A ].count := C.setheaders [A
].count +
C.setheaders [B ].count;
C.setheaders[B ].count := 0;
C.setheaders[B ].firstelement := 0
{ above two steps not really necessary, as set B no longer exists }
end
else { B is at least as large as A }
{ code similar to case above, but with A and B interchanged }
end; { MERGE }
function FIND ( x: 1..n; var C: MFSET );
{ return the name of the set of which x is a member }
begin
return ( C. names [x ].setname)
end; { FIND }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (33 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Fig. 5.23. The operations of an MFSET.
To find the set containing an element x, we first consult a mapping (e.g., an array)
not shown in Fig. 5.25, to obtain a pointer to the node for x. We then follow the path
from that node to the root of its tree and read the name of the set there.
The basic merge operation is to make the root of one tree be a child of the root of
the other. For example, we could merge A and B of Fig. 5.25 and call the result A, by
making node 5 a child of node 1. The result is shown in Fig. 5.26. However,
indiscriminate merging could result in a tree of n nodes that is a single chain. Then
doing a FIND operation on each of those nodes would take O(n2) time. Observe that
although a merge can be done in O(1) steps, the cost of a reasonable number of
FIND's will dominate the total cost, and this approach is not necessarily better than
the simplest one for executing n merges and n finds.
However, a simple improvement guarantees that if n is the number of elements,
then no FIND will take more than O(log n) steps. We simply keep at each root a
count of the number of elements in the set, and when called upon to merge two sets,
we make the root of the smaller tree be a child of the root of the larger. Thus, every
time a node is moved to a new tree, two things happen: the distance from the node to
its root increases by one, and the node will be in a set with at least twice as many
elements as before. Thus, if n is the total number of elements, no node can be moved
more than logn times; hence, the distance to its root can never exceed logn. We
conclude that each FIND requires at most O(log n) time.
Path Compression
Another idea that may speed up this implementation of MFSET's is path
compression. During a FIND, when following a path from some node to the root,
make each node encountered along the path be a child of the root. The easiest way to
do this is in two passes. First, find the root, and then retraverse the same path, making
each node a child of the root.
Example 5.7. Figure 5.27(a) shows a tree before executing a FIND operation on the
node for element 7 and Fig. 5.27(b) shows the result after 5 and 7 are made children
of the root. Nodes 1 and 2 on the path are not moved because 1 is the root, and 2 is
already a child of the root.
Path compression does not affect the cost of MERGE's; each MERGE still takes a
constant amount of time. There is, however, a subtle speedup in FIND's since path
compression tends to shorten a large number of paths from various nodes to the root
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (34 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
with relatively little effort.
Unfortunately, it is very difficult to analyze the average cost of FIND's when path
compression is used. It turns out that if we do not require that smaller trees be merged
into larger ones, we require no more than O(n log n) time to do n FIND's. Of course,
the first FIND may take O(n) time by itself for a tree consisting of one chain. But
path compression can change a tree very rapidly and no matter in what order we
apply FIND to elements of any tree no more than O(n) time is spent on n FIND's.
However, there are
Fig. 5.24. Example of the MFSET data structure.
Fig. 5.25. MFSET represented by a collection of trees.
sequences of MERGE and FIND instructions that require W(nlog n) time.
The algorithm that both uses path compression and merges the smaller tree into
the larger is asymptotically the most efficient method known for implementing
MFSET's. In particular, n FIND's require no more than O(na(n)) time, where a(n) is
a function that is not constant, yet grows much more slowly than logn. We shall
define a(n) below, but the analysis that leads to this bound is beyond the scope of this
book.
Fig. 5.26. Merging B into A.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (35 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Fig. 5.27. An example of path compression.
The Function a(n)
The function a(n) is closely related to a very rapidly growing function A(x, y), known
as Ackermann's function. A(x, y) is defined recursively by:
A(0,y) = 1 for y ³ 0
A(1, 0) = 2
A(x, 0) = x+2 for x ³ 2
A(x,y) = A(A(x-1, y),y-1) for x, y ³ 1
Each value of y defines a function of one variable. For example, the third line above
tells us that for y=0, this function is "add 2." For y = 1, we have A(x, 1) = A(A(x-1,
1),0) = A(x-1, 1) + 2, for x > 1, with A(1, 1) = A(A(0, 1),0) = A(1, 0) = 2. Thus A(x, 1)
= 2x for all x ³ 1. In other words, A(x, 1) is "multiply by 2." Then, A(x, 2) = A(A(x-1,
2), 1) = 2A(x-1, 2) for x > 1. Also, A(1, 2) = A(A(0,2), 1) = A(1, 1) = 2. Thus A(x, 2) =
2x. Similarly, we can show that A(x, 3) = 22...2 (stack of x 2's), while A(x, 4) is so
rapidly growing there is no accepted mathematical notation for such a function.
A single-variable Ackermann's function can be defined by letting A(x) = A(x, x).
The function a(n) is a pseudo-inverse of this single variable function. That is, a(n) is
the least x such that n £ A(x). For example, A(1) = 2, so a(1) = a(2) = 1. A(2) = 4, so
a(3) = a(4) = 2. A(3) = 8, so a(5) = . . . = a(8) = 3. So far, a(n) seems to be growing
rather steadily.
However, A(4) is a stack of 65536 2's. Since log(A(4)) is a stack of 65535 2's, we
cannot hope even to write A(4) explicitly, as it would take log(A(4)) bits to do so.
Thus a(n) £ 4 for all integers n one is ever likely to encounter. Nevertheless, a(n)
eventually reaches 5, 6, 7, . . . on its unimaginably slow course toward infinity.
5.6 An ADT with MERGE and SPLIT
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (36 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Let S be a set whose members are ordered by the relation <. The operation SPLIT(S,
S1, S2, x) partitions S into two sets: S1={ a ½ a is in S and a < x} and S2 = {a ½ a is in
S and a ³ x}. The value of S after the split is undefined, unless it is one of S1 or S2.
There are several situations where the operation of splitting sets by comparing each
member with a fixed value x is essential. We shall consider one such problem here.
The Longest Common Subsequence
Problem
A subsequence of a sequence x is obtained by removing zero or more (not necessarily
contiguous) elements from x. Given two sequences x and y, a longest common
subsequence (LCS) is a longest sequence that is a subsequence of both x and y.
For example, an LCS of 1, 2, 3, 2, 4, 1, 2 and 2, 4, 3, 1, 2, 1 is the subsequence 2,
3, 2, 1, formed as shown in Fig. 5.28. There are other LCS's as well, such as 2, 4, 1, 2,
but there are no common subsequences of length 5.
Fig. 5.28. A longest common subsequence.
There is a UNIX command called diff that compares files line-by-line, finding a
longest common subsequence, where a line of a file is considered an element of the
subsequence. That is, whole lines are analogous to the integers 1, 2, 3, and 4 in Fig.
5.28. The assumption behind the command diff is that the lines of each file that are
not in this LCS are lines inserted, deleted or modified in going from one file to the
other. For example, if the two files are versions of the same program made several
days apart, diff will, with high probability, find the changes.
There are several general solutions to the LCS problem that work in O(n2) steps
on sequences of length n. The command diff uses a different strategy that works well
when the files do not have too many repetitions of any line. For example, programs
will tend to have lines "begin" and "end" repeated many times, but other lines are not
likely to repeat.
The algorithm used by diff for finding an LCS makes use of an efficient
implementation of sets with operations MERGE and SPLIT, to work in time
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (37 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
O(plogn), where n is the maximum number of lines in a file and p is the number of
pairs of positions, one from each file, that have the same line. For example, p for the
strings in Fig. 5.28 is 12. The two 1's in each string contribute four pairs, the 2's
contribute six pairs, and 3 and 4 contribute one pair each. In the worst case, p,. could
be n2, and this algorithm would take O(n2logn) time. However, in practice, p is
usually closer to n, so we can expect an O(nlogn) time complexity.
To begin the description of the algorithm let A = a1a2 × × × an and B = b1b2 × × × bm
be the two strings whose LCS we desire. The first step is to tabulate for each value a,
the positions of the string A at which a appears. That is, we define PLACES(a)= { i ½
a = ai }. We can compute the sets PLACES(a) by constructing a mapping from
symbols to headers of lists of positions. By using a hash table, we can create the sets
PLACES(a) in O(n) "steps" on the average, where a "step" is the time it takes to
operate on a symbol, say to hash it or compare it with another. This time could be a
constant if symbols are characters or integers, say. However, if the symbols of A and
B are really lines of text, then steps take an amount of time that depends on the
average length of a line of text.
Having computed PLACES(a) for each symbol a that occurs in string A, we are
ready to find an LCS. To simplify matters, we shall only show how to find the length
of the LCS, leaving the actual construction of the LCS as an exercise. The algorithm
considers each bj, for j = 1, 2, ... , m, in turn. After considering bj, we need to know,
for each i between 0 and n, the length of the LCS of strings a1 × × × ai and b1 × × × bj.
We shall group values of i into sets Sk, for k = 0, 1, . . . , n, where Sk consists of all
those integers i such that the LCS of a1 × × × ai and b1 × × × bj has length k. Note that Sk
will always be a set of consecutive integers, and the integers in Sk+1 are larger than
those in Sk, for all k.
Example 5.8. Consider Fig. 5.28, with j = 5. If we try to match zero symbols from
the first string with the first five symbols of the second (24312), we naturally have an
LCS of length 0, so 0 is in S0. If we use the first symbol from the first string, we can
obtain an LCS of length 1, and if we use the first two symbols, 12, we can obtain an
LCS of length 2. However, using 123, the first three symbols, still gives us an LCS of
length 2 when matched against 24312. Proceeding in this manner, we discover S0 =
{0}, S1 = {1}, S2 = {2, 3}, S3 = {4, 5, 6}, and S4 = {7}.
Suppose that we have computed the Sk's for position j-1 of the second string and
we wish to modify them to apply to position j. We consider the set PLACES(bj). For
each r in PLACES(bj), we consider whether we can improve some of the LCS's by
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (38 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
adding the match between ar and bj to the LCS of a1 × × × ar- 1 and b1 × × × bj. That is, if
both r-1 and r are in Sk, then all s ³ r in Sk really belong in Sk+1 when bj is
considered. To see this we observe that we can obtain k matches between a1 × × × ar-1
and bl × × × bj-1, to which we add a match between ar and bj. We can modify Sk and
Sk+1 by the following steps.
1. FIND(r) to get Sk.
2. If FIND(r-1) is not Sk, then no benefit can be had by matching bj with ar. Skip
the remaining steps and do not modify Sk or Sk+1.
3. If FIND(r-1) = Sk, apply SPLIT(Sk, Sk, S'k, r) to separate from Sk those
members greater than or equal to r.
4. MERGE(S'k, Sk+1, Sk+1) to move these elements into Sk+1.
It is important to consider the members of PLACES(bj) largest first. To see why,
suppose for example that 7 and 9 are in PLACES(bj), and before bj is considered,
S3={6, 7, 8, 9} and S4={10, 11}.
If we consider 7 before 9, we split S3 into S3 = {6} and S'3 = {7, 8, 9}, then make
S4 = {7, 8, 9, 10, 11}. If we then consider 9, we split S4 into S4={7, 8} and S'4 = {9,
10, 11}, then merge 9, 10 and 11 into S5. We have thus moved 9 from S3 to S5 by
considering only one more position in the second string, representing an
impossibility. Intuitively, what has happened is that we have erroneously matched bj
against both a7 and a9 in creating an imaginary LCS of length 5.
In Fig. 5.29, we see a sketch of the algorithm that maintains the sets Sk as we scan
the second string. To determine the length of an LCS, we need only execute FIND(n)
at the end.
procedure LCS;
begin
(1) initialize S0 = {0, 1, ... , n} and Si = ­ for i = 1, 2, ... , n;
(2) for j := 1 to n do { compute Sk's for position
j }
(3) for r in PLACES(bj), largest first do begin
(4) k := FIND(r);
(5) if k = FIND(r-1) then begin { r is not
smallest in Sk }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (39 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
(6) SPLIT(Sk, Sk, S'k,
r);
(7) MERGE(S'k, Sk+1,
Sk+1)
end
end
end; { LCS }
Fig. 5.29. Sketch of longest common subsequence program.
Time Analysis of the LCS Algorithm
As we mentioned earlier, the algorithm of Fig. 5.29 is a useful approach only if there
are not too many matches between symbols of the two strings. The measure of the
number of matches is
where |PLACES(bj)| denotes the number of elements in set PLACES(bj). In other
words, p is the sum over all bj of the number of positions in the first string that match
bj. Recall that in our discussion of file comparison, we expect p to be of the same
order as m and n, the lengths of the two strings (files).
It turns out that the 2-3 tree is a good structure for the sets Sk. We can initialize
these sets, as in line (1) of Fig. 5.29, in O(n) steps. The FIND operation requires an
array to serve as a mapping from positions r to the leaf for r and also requires
pointers to parents in the 2-3 tree. The name of the set, i.e., k for Sk, can be kept at the
root, so we can execute FIND in O(logn) steps by following parent pointers until we
reach the root. Thus all executions of lines (4) and (5) together take O(plogn) time,
since those lines are each executed exactly once for each match found.
The MERGE operation of line (5) has the special property that every member of
S'k is lower than every member of Sk+1, and we can take advantage of this fact when
using 2-3 trees for an implementation.† To begin the MERGE, place the 2-3 tree for
S'k to the left of that for Sk+1. If both are of the same height, create a new root with
the roots of the two trees as children. If S'k is shorter, insert the root of that tree as the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (40 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
leftmost child of the leftmost node of Sk+1 at the appropriate level. If this node now
has four children, we modify the tree exactly as in the INSERT procedure of Fig.
5.20. An example is shown in Fig. 5.30. Similarly, if Sk+1 is shorter, make its root the
rightmost child of the rightmost node of S'k at the appropriate level.
Fig. 5.30. Example of MERGE.
The SPLIT operation at r requires that we travel up the tree from leaf r,
duplicating every interior node along the path and giving one copy to each of the two
resulting trees. Nodes with no children are eliminated, and nodes with one child are
removed and have that child inserted into the proper tree at the proper level.
Example 5.9. Suppose we split the tree of Fig. 5.30(b) at node 9. The two trees, with
duplicated nodes, are shown in Fig. 5.31(a). On the left, the parent of 8 has only one
child, so 8 becomes a child of the parent of 6 and 7. This parent now has three
children, so all is as it should be; if it had four children, a new node would have been
created and inserted into the tree. We need only eliminate nodes with zero children
(the old parent of 8) and the chain of nodes with one child leading to the root. The
parent of 6, 7, and 8 becomes the new root, as shown in Fig. 5.31(b). Similarly, in the
right-hand tree, 9 becomes a sibling of 10 and 11, and unnecessary nodes are
eliminated, as is also shown in Fig. 5.31(b).
Fig. 5.31. An example of SPLIT.
If we do the splitting and reorganization of the 2-3 tree bottom up, it can be shown
by consideration of a large number of cases that O(logn) steps suffices. Thus, the total
time spent in lines (6) and (7) of Fig. 5.29 is O(plogn), and hence the entire algorithm
takes O(plogn) steps. We must add in the preprocessing time needed to compute and
sort PLACES(a) for symbols a. As we mentioned, if the symbols a are "large"
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (41 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
objects, this time can be much greater than any other part of the algorithm. As we
shall see in Chapter 8, if the symbols can be manipulated and compared in single
"steps," then O(nlogn) time suffices to sort the first string a1a2 × × × an (actually, to
sort objects (i, ai) on the second field), whereupon PLACES(a) can be read off from
this list in O(n) time. Thus, the length of the LCS can be computed in O(max(n, p)
logn) time which, since p ³ n is normal, can be taken as O(plogn).
Exercises
5.1
Draw all possible binary search trees containing the four elements 1, 2,
3, 4.
5.2
Insert the integers 7, 2, 9, 0, 5, 6, 8, 1 into a binary search tree by
repeated application of the procedure INSERT of Fig. 5.3.
5.3 Show the result of deleting 7, then 2 from the final tree of Exercise 5.2.
*5.4
When deleting two elements from a binary search tree using the
procedure of Fig. 5.5, does the final tree ever depend on the order in
which you delete them?
5.5
We wish to keep track of all 5-character substrings that occur in a given
string, using a trie. Show the trie that results when we insert the 14
substrings of length five of the string ABCDABACDEBACADEBA.
*5.6
To implement Exercise 5.5, we could keep a pointer at each leaf, which,
say, represents string abcde, to the interior node representing the suffix
bcde. That way, if the next symbol, say f, is received, we don't have to
insert all of bcdef, starting at the root. Furthermore, having seen abcde,
we may as well create nodes for bcde, cde, de, and e, since we shall,
unless the sequence ends abruptly, need those nodes eventually. Modify
the trie data structure to maintain such pointers, and modify the trie
insertion algorithm to take advantage of this data structure.
5.7
Show the 2-3 tree that results if we insert into an empty set, represented
as a 2-3 tree, the elements 5, 2, 7, 0, 3, 4, 6, 1, 8, 9.
5.8
Show the result of deleting 3 from the 2-3 tree that results from Exercise
5.7.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (42 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
5.9
Show the successive values of the various Si's when implementing the
LCS algorithm of Fig. 5.29 with first string abacabada, and second
string bdbacbad.
5.10
Suppose we use 2-3 trees to implement the MERGE and SPLIT
operations as in Section 5.6.
a. Show the result of splitting the tree of Exercise 5.7 at 6.
b. Merge the tree of Exercise 5.7 with the tree consisting of leaves
for elements 10 and 11.
5.11
Some of the structures discussed in this chapter can be modified easily
to support the MAPPING ADT. Write procedures MAKENULL,
ASSIGN, and COMPUTE to operate on the following data structures.
a. Binary search trees. The "<" ordering applies to domain elements.
b. 2-3 trees. At interior nodes, place only the key field of domain
elements.
5.12
Show that in any subtree of a binary search tree, the minimum element is
at a node without a left child.
5.13 Use Exercise 5.12 to produce a nonrecursive version of DELETE-MIN.
5.14
Write procedures ASSIGN, VALUEOF, MAKENULL and GETNEW
for trie nodes represented as lists of cells.
*5.15
How do the trie (list of cells implementation), the open hash table, and
the binary search tree compare for speed and for space utilization when
elements are strings of up to ten characters?
*5.16
If elements of a set are ordered by a "<" relation, then we can keep one
or two elements (not just their keys) at interior nodes of a 2-3 tree, and
we then do not have to keep these elements at the leaves. Write INSERT
and DELETE procedures for 2-3 trees of this type.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (43 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
5.17
Another modification we could make to 2-3 trees is to keep only keys at
interior nodes, but do not require that the keys k1 and k2 at a node truly
be the minimum keys of the second and third subtrees, just that all keys
k of the third subtree satisfy k ³ k2, all keys k of the second satisfy k1 £ k
< k2, and all keys k of the first satisfy k < k1.
a. How does this convention simplify the DELETE operation?
b. Which of the dictionary and mapping operations are made more
complicated or less efficient?
*5.18
Another data structure that supports dictionaries with the MIN operation
is the AVL tree (named for the inventors' initials) or height-balanced
tree. These trees are binary search trees in which the heights of two
siblings are not permitted to differ by more than one. Write procedures
to implement INSERT and DELETE, while maintaining the AVL-tree
property.
5.19 Write the Pascal program for procedure delete1 of Fig. 5.21.
*5.20
A finite automaton consists of a set of states, which we shall take to be
the integers 1..n and a table transitions[state, input] giving a next state
for each state and each input character. For our purposes, we shall
assume that the input is always either 0 or 1. Further, certain of the states
are designated accepting states. For our purposes, we shall assume that
all and only the even numbered states are accepting. Two states p and q
are equivalent if either they are the same state, or (i) they are both
accepting or both nonaccepting, (ii) on input 0 they transfer to
equivalent states, and (iii) on input 1 they transfer to equivalent states.
Intuitively, equivalent states behave the same on all sequences of inputs;
either both or neither lead to accepting states. Write a program using the
MFSET operations that computes the sets of equivalent states of a given
finite automaton.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (44 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
**5.21
In the tree implementation of MFSET:
a. Show that W(n log n) time is needed for certain lists of n
operations if path compression is used but larger trees are
permitted to be merged into smaller ones.
b. Show that O(na(n)) is the worst case running time for n
operations if path compression is used, and the smaller tree is
always merged into the larger.
5.22
Select a data structure and write a program to compute PLACES
(defined in Section 5.6) in average time O(n) for strings of length n.
*5.23
Modify the LCS procedure of Fig. 5.29 to compute the LCS, not just its
length.
*5.24 Write a detailed SPLIT procedure to work on 2-3 trees.
*5.25
If elements of a set represented by a 2-3 tree consist only of a key field,
an element whose key appears at an interior node need not appear at a
leaf. Rewrite the dictionary operations to take advantage of this fact and
avoid storing any element at two different nodes.
Bibliographic Notes
Tries were first proposed by Fredkin [1960]. Bayer and McCreight [1972] introduced
B-trees, which, as we shall see in Chapter 11, are a generalization of 2-3 trees. The
first uses of 2-3 trees were by J. E. Hopcroft in 1970 (unpublished) for insertion,
deletion, concatenation, and splitting, and by Ullman [1974] for a code optimization
problem.
The tree structure of Section 5.5, using path compression and merging smaller into
larger, was first used by M. D. McIlroy and R. Morris to construct minimum-cost
spanning trees. The performance of the tree implementation of MFSET's was
analyzed by Fischer [1972] and by Hopcroft and Ullman [1973]. Exercise 5.21(b) is
from Tarjan [1975].
The solution to the LCS problem of Section 5.6 is from Hunt and Szymanski
[1977]. An efficient data structure for FIND, SPLIT, and the restricted MERGE
(where all elements of one set are less than those of the other) is described in van
Emde Boas, Kaas, and Zijlstra [1977].
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (45 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Exercise 5.6 is based on an efficient algorithm for matching patterns developed by
Weiner [1973]. The 2-3 tree variant of Exercise 5.16 is discussed in detail in Wirth
[1976]. The AVL tree structure in Exercise 5.18 is from Adel'son-Vel'skii and Landis
[1962].
† Recall the left child of the root is a descendant of itself, so we have not ruled out the
possibility that x is at the left child of the root.
† The highest-valued node among the descendants of the left child would do as well.
† Recall that all logarithms are to the base 2 unless otherwise noted.
† Trie was originally intended to be a homonym of "tree" but to distinguish these two
terms many people prefer to pronounce trie as though it rhymes with "pie."
† VALUEOF is a function version of COMPUTE in Section 2.5.
† There is another version of 2-3 trees that places whole records at interior nodes, as a
binary search tree does.
† All nodes, however, take the largest amount of space needed for any variant types,
so Pascal is not really the best language of implementing 2-3 trees in practice.M
† A useful variant would take only a key value and delete any element with that key.
† We say a is congruent to b modulo n if a and b have the same remainders when
divided by n, or put another way, a-b is a multiple of n.
† Note that n- 1 is the largest number of merges that can be performed before all
elements are in one set.
‡ Note that our ability to call the resulting component by the name of either of its
constituents is important here, although in the simpler implementation, the name of
the first argument was always picked.
† Strictly speaking we should use a different name for the MERGE operation, as the
implementation we propose will not work to compute the arbitrary union of disjoint
sets, while keeping the elements sorted so operations like SPLIT and FIND can be
performed.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (46 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Table of Contents Go to Chapter 6
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (47 of 47) [1.7.2001 19:09:33]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_1.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_1.gif [1.7.2001 19:09:42]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_3.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_3.gif [1.7.2001 19:09:57]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_7.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_7.gif [1.7.2001 19:10:04]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_10.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_10.gif [1.7.2001 19:10:13]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_13.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_13.gif [1.7.2001 19:10:19]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_15.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_15.gif [1.7.2001 19:10:28]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_16.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_16.gif [1.7.2001 19:10:44]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_20.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_20.gif [1.7.2001 19:10:48]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_21.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_21.gif [1.7.2001 19:10:53]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_22.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_22.gif [1.7.2001 19:10:59]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_24.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_24.gif [1.7.2001 19:11:14]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_25.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_25.gif [1.7.2001 19:11:36]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_27.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/fig4_27.gif [1.7.2001 19:11:53]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Directed Graphs
In problems arising in computer science, mathematics, engineering, and many other
disciplines we often need to represent arbitrary relationships among data objects.
Directed and undirected graphs are natural models of such relationships. This chapter
presents the basic data structures that can be used to represent directed graphs. Some
basic algorithms for determining the connectivity of directed graphs and for finding
shortest paths are also presented.
6.1 Basic Definitions
A directed graph (digraph for short) G consists of a set of vertices V and a set of arcs
E. The vertices are also called nodes or points; the arcs could be called directed edges
or directed lines. An arc is an ordered pair of vertices (v, w); v is called the tail and w
the head of the arc. The arc (v, w) is often expressed by v ® w and drawn as
Notice that the "arrowhead" is at the vertex called the "head" and the tail of the arrow
is at the vertex called the "tail." We say that arc v ® w is from v to w, and that w is
adjacent to v.
Example 6.1. Figure 6.1 shows a digraph with four vertices and five arcs.
The vertices of a digraph can be used to represent objects, and the arcs
relationships between the objects. For example, the vertices might represent cities
and the arcs airplane flights from one city to another. As another example, which we
introduced in Section 4.2, a digraph can be used to represent the flow of control in a
computer program. The vertices represent basic blocks and the arcs possible transfers
of flow of control.
A path in a digraph is a sequence of vertices v1, v2, . . . , vn, such that v1 ® v2, v2
® v3, . . . , vn-1 ® vn are arcs. This path is from vertex v1 to vertex vn, and passes
through vertices v2, v3, . . . , vn-1, and ends at vertex vn. The length of a path is the
number of arcs on the path, in this case, n-1. As a
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (1 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Fig. 6.1. Directed graph.
special case, a single vertex v by itself denotes a path of length zero from v to v. In
Fig. 6.1, the sequence 1, 2, 4 is a path of length 2 from vertex 1 to vertex 4.
A path is simple if all vertices on the path, except possibly the first and last, are
distinct. A simple cycle is a simple path of length at least one that begins and ends at
the same vertex. In Fig. 6.1, the path 3, 2, 4, 3 is a cycle of length three.
In many applications it is useful to attach information to the vertices and arcs of
a digraph. For this purpose we can use a labeled digraph, a digraph in which each arc
and/or each vertex can have an associated label. A label can be a name, a cost, or a
value of any given data type.
Example 6.2. Figure 6.2 shows a labeled digraph in which each arc is labeled by a
letter that causes a transition from one vertex to another. This labeled digraph has the
interesting property that the arc labels on every cycle from vertex 1 back to vertex 1
spell out a string of a's and b's in which both the number of a's and b's is even.
In a labeled digraph a vertex can have both a name and a label. Quite frequently,
we shall use the vertex label as the name of the vertex. Thus, the numbers in Fig. 6.2
could be interpreted as vertex names or vertex labels.
6.2 Representations for Directed Graphs
Several data structures can be used to represent a directed graph. The appropriate
choice of data structure depends on the operations that will be applied to the vertices
and arcs of the digraph. One common representation for a digraph G = (V,E) is the
adjacency matrix. Suppose V = {1, 2 , . . . , n}. The adjacency matrix for G is an n x n
matrix A of booleans, where A[i, j] is true if and only if there is an arc from vertex i
to j. Often, we shall exhibit adjacency matrices with 1 for true and 0 for false;
adjacency matrices may even be implemented that way. In the adjacency matrix
representation the time required to access an element of an adjacency matrix is
independent of the size of V and E. Thus the adjacency matrix
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (2 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Fig. 6.2. Transition digraph.
representation is useful in those graph algorithms in which we frequently need to
know whether a given arc is present.
Closely related is the labeled adjacency matrix representation of a digraph,
where A[i, j] is the label on the arc going from vertex i to vertex j. If there is no arc
from i to j, then a value that cannot be a legitimate label must be used as the entry for
A[i, j].
Example 6.3. Figure 6.3 shows the labeled adjacency matrix for the digraph of Fig.
6.2. Here, the label type is char, and a blank represents the absence of an arc.
Fig. 6.3. Labeled adjacency matrix for digraph of Fig. 6.2.
The main disadvantage of using an adjacency matrix to represent a digraph is
that the matrix requires W(n2) storage even if the digraph has many fewer than n2
arcs. Simply to read in or examine the matrix would require O(n2) time, which would
preclude O(n) algorithms for manipulating digraphs with O(n) arcs.
To avoid this disadvantage we can use another common representation for a
digraph G = (V,E) called the adjacency list representation. The adjacency list for a
vertex i is a list, in some order, of all vertices adjacent to i. We can represent G by an
array HEAD, where HEAD[i] is a pointer to the adjacency list for vertex i. The
adjacency list representation of a digraph requires storage proportional to sum of the
number of vertices plus the number of arcs; it is often used when the number of arcs
is much less than n2. However, a potential disadvantage of the adjacency list
representation is that it may take O(n) time to determine whether there is an arc from
vertex i to vertex j, since there can be O(n) vertices on the adjacency list for vertex i.
Example 6.4. Figure 6.4 shows an adjacency list representation for the digraph of
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (3 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Fig. 6.1, where singly linked lists are used. If arcs had labels, these could be included
in the cells of the linked list.
Fig. 6.4. Adjacency list representation for digraph of Fig. 6.1.
If we did insertions and deletions from the adjacency lists, we might prefer to
have the HEAD array point to header cells that did not contain adjacent vertices.†
Alternatively, if the graph were expected to remain fixed, with no (or very few)
changes to be made to the adjacency lists, we might prefer HEAD[i] to be a cursor to
an array ADJ, where ADJ[HEAD[i]], ADJ[HEAD[i]+ 1] , . . . , and so on, contained
the vertices adjacent to vertex i, up to that point in ADJ where we first encounter a 0,
which marks the end of the list of adjacent vertices for i. For example, Fig. 6.1 could
be represented as in Fig. 6.5.
Directed Graph ADT's
We could define an ADT corresponding to the directed graph formally and study
implementations of its operations. We shall not pursue this direction extensively,
because there is little of a surprising nature, and the principal data structures for
graphs have already been covered. The most common operations on directed graphs
include operations to read the label of a vertex or arc, to insert or delete vertices and
arcs, and to navigate by following arcs
Fig. 6.5. Another adjacency list representation of Fig. 6.1.
from tail to head.
The latter operations require a little thought. Most frequently, we shall encounter
in informal programs statements like
for each vertex w adjacent to vertex v do (6.1)
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (4 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
{ some action on w }
To implement such a step, we need the notion of an index type for the set of vertices
adjacent to some one vertex v. For example, if adjacency lists are used to represent
the graph, then an index is really a position on the adjacency list for v. If an
adjacency matrix is used, an index is an integer representing an adjacent vertex. We
need the following three operations on directed graphs.
1. FIRST(v) returns the index for the first vertex adjacent to v. The index for the
null vertex L is returned if there is no vertex adjacent to v.
2. NEXT(v, i) returns the index after index i for the vertices adjacent to v. L is
returned if i is the last index for vertices adjacent to v.
3. VERTEX(v, i) returns the vertex with index i among the vertices adjacent to v.
Example 6.5. If the adjacency matrix representation is chosen, VERTEX(v, i) returns
i. FIRST(v) and NEXT(v, i) can be written as in Fig. 6.6 to operate on an externally
defined n x n boolean matrix A. We assume A is declared
array [1..n, 1..n] of boolean
and that 0 is used for L. We can then implement the statement (6.1) as in Fig. 6.7.
Note that FIRST(v) can also be implemented as NEXT(v, 0).
function FIRST ( v: integer ): integer;
var
i: integer;
begin
for i := 1 to n do
if A[v, i] then
return (i);
return (0) { if we reach here, v has no adjacent vertex }
end; { FIRST }
function NEXT ( v: integer; i: integer ): integer;
var
j: integer;
begin
for j := i + 1 to n do
if A[v, j] then
return (j);
return (0)
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (5 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
end; { NEXT }
Fig. 6.6. Operations to scan adjacent vertices.
i := FIRST(v);
while i <> L do begin
w := VERTEX(v, i);
{ some action on w }
i := NEXT(v, i)
end
Fig. 6.7. Iteration over vertices adjacent to v.
6.3 The Single Source Shortest Paths
Problem
In this section we consider a common path-finding problem on directed graphs. We
are given a directed graph G = (V, E) in which each arc has a nonnegative label, and
one vertex is specified as the source. Our problem is to determine the cost of the
shortest path from the source to every other vertex in V, where the length of a path is
just the sum of the costs of the arcs on the path. This problem is often called the
single-source shortest paths problem.† Note that we shall talk of paths as having
"length" even if costs represent something different, like time.
We might think of G as a map of airline flights, in which each vertex represents a
city and each arc v ® w an airline route from city v to city w. The label on arc v ® w
is the time to fly from v to w.‡ Solving the single-source shortest paths problem for
this directed graph would determine the minimum travel time from a given city to
every other city on the map.
To solve this problem we shall use a "greedy" technique, often known as
Dijkstra's algorithm. The algorithm works by maintaining a set S of vertices whose
shortest distance from the source is already known. Initially, S contains only the
source vertex. At each step, we add to S a remaining vertex v whose distance from
the source is as short as possible. Assuming all arcs have nonnegative costs, we can
always find a shortest path from the source to v that passes only through vertices in S.
Call such a path special. At each step of the algorithm, we use an array D to record
the length of the shortest special path to each vertex. Once S includes all vertices, all
paths are "special," so D will hold the shortest distance from the source to each
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (6 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
vertex.
The algorithm itself is given in Fig. 6.8. It assumes that we are given a directed
graph G = (V, E) where V = {1, 2, . . . , n} and vertex 1 is the source. C is a twodimensional
array of costs, where C[i, j] is the cost of going from vertex i to vertex j
on arc i ® j. If there is no arc i ® j, then we assume C[i, j] is ¥, some value much
larger than any actual cost. At each step D [i] contains the length of the current
shortest special path to vertex i.
Example 6.6. Let us apply Dijkstra to the directed graph of Fig. 6.9. Initially, S =
{1}, D[2] = 10, D[3] = ¥, D[4] = 30 and D[5] = 100. In the first iteration of the forloop
of lines (4)-(8), w = 2 is selected as the vertex with the minimum D value. Then
we set D[3] = min(¥, 10+50)= 60. D(4) and D(5) do not change, because reaching
them from 1 directly is shorter than going through vertex 2. The sequence of Dvalues
after each iteration of the for-loop is shown in Fig. 6.10.
If we wish to reconstruct the shortest path from the source to each vertex, then
we can maintain another array P of vertices, such that P [v] contains the vertex
immediately before vertex v in the shortest path. Initialize P[v] to 1 for all v ¹ 1. The
P-array can be updated right after line (8) of Dijkstra. If D[w]+C[w,v]<D[v] at line
(8), then we set P[v]:= w. Upon termination
procedure Dijkstra;
{ Dijkstra computes the cost of the shortest paths
from vertex 1 to every vertex of a directed graph }
begin
(1) S := {1};
(2) for i := 2 to n do
(3) D[i] := C[1, i]; { initialize D }
(4) for i := 1 to n-1 do begin
(5) choose a vertex w in V-S such that
D[w] is a minimum;
(6) add w to S;
(7) for each vertex v in V-S do
(8) D[v] := min(D[v], D[w] +
C[w, v])
end
end; { Dijkstra }
Fig. 6.8. Dijkstra's algorithm.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (7 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Fig. 6.9. Digraph with labeled arcs.
of Dijkstra the path to each vertex can be found by tracing backward the predecessor
vertices in the P-array.
Example 6.7. For the digraph in Example 6.6 the P-array would have the values P[2]
= 1, P[3] = 4, P[4] = 1, and P[5] = 3. To find the shortest path from vertex 1 to vertex
5, for example, we would trace the predecessors in reverse order beginning at vertex
5. From the P-array we determine 3 is the predecessor of 5, 4 the predecessor of 3,
and 1 the predecessor of 4. Thus the shortest path from vertex 1 to vertex 5 is 1, 4, 3,
5.
Fig. 6.10. Computation of Dijkstra on digraph of Fig. 6.9.
Why Dijkstra's Algorithm Works
Dijkstra's algorithm is an example where "greed" pays off, in the sense that what
appears locally as the best thing to do turns out to be the best over all. In this case, the
locally "best" thing to do is to find the distance to the vertex w that is outside S but
has the shortest special path. To see why in this case there cannot be a shorter
nonspecial path from the source to w, observe Fig. 6.11. There we show a
hypothetical shorter path to w that first leaves S to go to vertex x, then (perhaps)
wanders into and out of S several times before ultimately arriving at w.
But if this path is shorter than the shortest special path to w, then the initial
segment of the path from the source to x is a special path to x shorter than the shortest
special path to w. (Notice how important the fact that costs are nonnegative is here;
without it our argument wouldn't work, and in fact Dijkstra's algorithm would not
work correctly.) In that case, when we selected w at line (5) of Fig. 6.8, we should
have selected x instead, because D [x] was less than D [w].
To complete a proof that Fig. 6.8 works, we should verify that at all times D [v]
is truly the shortest distance of a special path to vertex v. The crux of this argument is
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (8 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
in observing that when we add a new vertex w to S at line (6), lines (7) and (8) adjust
D to take account of the possibility that there is now a shorter special path to v going
through w. If that path goes through the old S to w and then immediately to v, its cost,
D [w]+C [w, v], will be compared with D [v] at line (8), and D [v] will be reduced if
the new special path is shorter. The only other possibility for a shorter special path is
shown in Fig. 6.12, where the path travels to w, then back into the old S, to some
member x of the old S, then to v.
But there really cannot be such a path. Since x was placed in S before w, the
shortest of all paths from the source to x runs through the old S alone. Therefore, the
path to x through w shown in Fig. 6.12 is no shorter than the path directly to x
through S. As a result, the length of the path in Fig. 6.12 from the source to w, x, and
v is no less from the old value of D [v], since D [v] was no greater than the length of
the shortest path to x through S and then directly to v. Thus D [v] cannot be reduced
at line (8) by a path through w and x as in Fig. 6.12, and we need not consider the
length of such paths.
Fig. 6.11. Hypothetical shorter path to w.
Fig. 6.12. Impossible shortest special path.
Running Time of Dijkstra's Algorithm
Suppose Fig. 6.8 operates on a digraph with n vertices and e edges. If we use an
adjacency matrix to represent the digraph, then the loop of lines (7) and (8) takes
O(n) time, and it is executed n-1 times for a total time of O(n2). The rest of the
algorithm is easily seen to require no more time than this.
If e is much less than n2, we might do better by using an adjacency list
representation of the digraph and using a priority queue implemented as a partially
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (9 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
ordered tree to organize the vertices in V-S. The loop of lines (7) and (8) can then be
implemented by going down the adjacency list for w and updating the distances in the
priority queue. A total of e updates will be made, each at a cost of O(logn) time, so
the total time spent in lines (7) and (8) is now O(elogn), rather than O(n2).
Lines (1)-(3) clearly take O(n) time, as do lines (4) and (6). Using the priority
queue to represent V-S, lines (5)-(6) implement exactly the DELETEMIN operation,
and each of the n-1 iterations of these lines requires O(logn) time.
As a result, the total time spent on this version of Dijkstra's algorithm is bounded
by O(elogn). This running time is considerably better than O(n2) if e is very small
compared with n2.
6.4 The All-Pairs Shortest Paths Problem
Suppose we have a labeled digraph that gives the flying time on certain routes
connecting cities, and we wish to construct a table that gives the shortest time
required to fly from any one city to any other. We now have an instance of the allpairs
shortest paths (APSP) problem. To state the problem precisely, we are given a
directed graph G = (V, E) in which each arc v ® w has a non-negative cost C[v, w].
The APSP problem is to find for each ordered pair of vertices (v, w) the smallest
length of any path from v to w.
We could solve this problem using Dijkstra's algorithm with each vertex in turn
as the source. A more direct way of solving the problem is to use the following
algorithm due to R. W. Floyd. For convenience, let us again assume the vertices in V
are numbered 1, 2 , . . . , n. Floyd's algorithm uses an n x n matrix A in which to
compute the lengths of the shortest paths. We initially set A[i, j] = C[i, j] for all i ¹ j.
If there is no arc from i to j, we assume C[i, j] = ¥. Each diagonal element is set to 0.
We then make n iterations over the A matrix. After the kth iteration, A[i, j] will
have for its value the smallest length of any path from vertex i to vertex j that does
not pass through a vertex numbered higher than k. That is to say, i and j, the end
vertices on the path, may be any vertex, but any intermediate vertex on the path must
be less than or equal to k.
In the kth iteration we use the following formula to compute A.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (10 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
The subscript k denotes the value of the A matrix after the kth iteration, and it should
not be assumed that there are n different matrices. We shall eliminate these subscripts
shortly. This formula has the simple interpretation shown in Fig. 6.13.
To compute Ak[i, j] we compare Ak- 1[i, j], the cost of going from i to j without
going through k or any higher-numbered vertex, with Ak-1[i, k] + Ak- 1[k, j], the cost
of going first from i to k and then from k to j, without passing through a vertex
numbered higher than k. If passing through vertex k produces a cheaper path than
what we had for Ak- 1[i, j], then we choose that cheaper cost for Ak[i, j].
Fig. 6.13. Including k among the vertices to go from i to j.
Example 6.8. Consider the weighted digraph shown in Fig. 6.14. The values of the A
matrix initially and after the three iterations are shown in Fig. 6.15.
Fig. 6.14. Weighted digraph.
Since Ak[i, k] = Ak-1[i, k] and Ak[k, j] = Ak-1[k, j], no entry with either subscript
equal to k changes during the kth iteration. Therefore, we can perform the
computation with only one copy of the A matrix. A program to perform this
computation on n x n matrices is shown in Fig. 6.16.
The running time of this program is clearly O(n3), since the program is basically
nothing more than a triply nested for-loop. To verify that this program works, it is
easy to prove by induction on k that after k passes through the triple for-loop, A[i, j]
holds the length of the shortest path from vertex i to vertex j that does not pass
through a vertex numbered higher than k.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (11 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Fig. 6.15. Values of successive A matrices.
procedure Floyd ( var A: array[1..n, 1..n]
of real;
C: array[1..n, 1..n] of real );
{ Floyd computes shortest path matrix A given arc cost matrix C }
var
i, j, k: integer;
begin
for i := 1 to n do
for j := 1 to n do
A[i, j] := C[i, j];
for i:= 1 to n do
A[i, i] := 0;
for k:= 1 to n do
for i := 1 to n do
for j:= 1 to n do
if A[i, k] + A[k, j] < A
[i, j] then
A[i, j] := A[i, k] + A[k,
j]
end; { Floyd }
Fig. 6.16. Floyd's algorithm.
Comparison Between Floyd's and
Dijkstra's Algorithms
Since the adjacency-matrix version of Dijkstra finds shortest paths from one vertex in
O(n2) time, it, like Floyd's algorithm, can find all shortest paths in O(n3) time. The
compiler, machine, and implementation details will determine the constants of
proportionality. Experimentation and measurement are the easiest way to ascertain
the best algorithm for the application at hand.
If e, the number of edges, is very much less than n2, then despite the relatively
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (12 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
low constant factor in the O(n3) running time of Floyd, we would expect the
adjacency list version of Dijkstra, taking O(ne logn) time to solve the APSP, to be
superior, at least for large sparse graphs.
Recovering the Paths
In many situations we may want to print out the cheapest path from one vertex to
another. One way to accomplish this is to use another matrix P, where P[i, j] holds
that vertex k that led Floyd to find the smallest value of A[i, j]. If P[i, j]=0, then the
shortest path from i to j is direct, following the arc from i to j. The modified version
of Floyd in Fig. 6.17 stores the appropriate intermediate vertices into P.
procedure shortest ( var A: array[1..n, 1..n]
of real;
C: array[1..n, 1..n] of real; P:
array[1..n, 1..n] of integer );
{ shortest takes an n X n matrix C of arc costs and produces an
n X n matrix A of lengths of shortest paths and an n X n
matrix
P giving a point in the "middle" of each shortest path }
var
i, j, k: integer;
begin
for i:= 1 to n do
for j := 1 to n do begin
A[i, j] := C[i, j];
P[i, j] := 0
end;
for i:= 1 to n do
A[i, i] := 0;
for k := 1 to n do
for i:= 1 to n do
for j:= 1 to n do
if A[i, k] + A[k, j] <
A[i, j] then begin
A[i, j] := A[i, k] + A[k,
j];
P[i, j] := k
end
end; { shortest }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (13 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Fig. 6.17. Shortest paths program.
To print out the intermediate vertices on the shortest path from vertex i to vertex
j, we invoke the procedure path(i, j) where path is given in Fig. 6.18. While on an
arbitrary matrix P, path could loop forever, if P comes from shortest, we could not,
say, have k on the shortest path from i to j and also have j on the shortest path from i
to k. Note how our assumption of nonnegative weights is again crucial.
procedure path ( i, j: integer );
var
k: integer;
begin
k := P[i, j];
if k = 0 then
return;
path(i, k);
writeln(k);
path(k, j)
end; { path }
Fig. 6.18. Procedure to print shortest path.
Example 6.9. Figure 6.19 shows the final P matrix for the digraph of Fig. 6.14.
Fig. 6.19. P matrix for digraph of Fig. 6.14.
Transitive Closure
In some problems we may be interested in determining only whether there exists a
path of length one or more from vertex i to vertex j. Floyd's algorithm can be
specialized readily to this problem; the resulting algorithm, which predates Floyd's, is
called Warshall's algorithm.
Suppose our cost matrix C is just the adjacency matrix for the given digraph.
That is, C[i, j] = 1 if there is an arc from i to j, and 0 otherwise. We wish to compute
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (14 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
the matrix A such that A[i, j] = 1 if there is a path of length one or more from i to j,
and 0 otherwise. A is often called the transitive closure of the adjacency matrix.
Example 6.10. Figure 6.20 shows the transitive closure for the adjacency matrix of
the digraph of Fig. 6.14.
The transitive closure can be computed using a procedure similar to Floyd by
applying the following formula in the kth pass over the boolean A matrix.
Ak[i, j] = Ak-1[i,
j] or (Ak-1[i, k] and Ak-
1[k, j])
This formula states that there is a path from i to j not passing through a
Fig. 6.20. Transitive closure.
vertex numbered higher than k if
1. there is already a path from i to j not passing through a vertex numbered
higher than k- 1 or
2. there is a path from i to k not passing through a vertex numbered higher than k
- 1 and a path from k to j not passing through a vertex numbered higher than k-
1.
As before Ak[i, k] = Ak-1[i, k] and Ak[k, j] = Ak-1[k, j] so we can perform the
computation with only one copy of the A matrix. The resulting Pascal program,
named Warshall after its discoverer, is shown in Fig. 6.21.
procedure Warshall ( var A: array[1..n, 1..n
] of boolean;
C: array[1..n, 1..n] of boolean );
{ Warshall makes A the transitive closure of C }
var
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (15 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
i, j, k: integer;
begin
for i := 1 to n do
for j := 1 to n do
A[i, j] := C[i, j];
for k := 1 to n do
for i := 1 to n do
for j := 1 to n do
if A[i, j ] = false then
A[i, j] := A[i, k] and
A[k, j]
end; { Warshall }
Fig. 6.21. Warshall's algorithm for transitive closure.
An Example: Finding the Center of a
Digraph
Suppose we wish to determine the most central vertex in a digraph. This problem can
be readily solved using Floyd's algorithm. First, let us make more precise the term
"most central vertex." Let v be a vertex in a digraph G = (V, E). The eccentricity of v
is
The center of G is a vertex of minimum eccentricity. Thus, the center of a digraph is
a vertex that is closest to the vertex most distant from it.
Example 6.11. Consider the weighted digraph shown in Fig. 6.22.
Fig. 6.22. Weighted digraph.
The eccentricities of the vertices are
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (16 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
vertex eccentricity
a ¥
b 6
c 8
d 5
e 7
Thus the center is vertex d.
Finding the center of a digraph G is easy. Suppose C is the cost matrix for G.
1. First apply the procedure Floyd of Fig. 6.16 to C to compute the all-pairs
shortest paths matrix A.
2. Find the maximum cost in each column i. This gives us the eccentricity of
vertex i.
3. Find a vertex with minimum eccentricity. This is the center of G.
This running time of this process is dominated by the first step, which takes O(n3)
time. Step (2) takes O(n2) time and step (3) O(n) time.
Example 6.12. The APSP cost matrix for Fig. 6.22 is shown in Fig. 6.23. The
maximum value in each column is shown below.
Fig. 6.23. APSP cost matrix.
6.5 Traversals of Directed Graphs
To solve many problems dealing with directed graphs efficiently we need to visit the
vertices and arcs of a directed graph in a systematic fashion. Depth-first search, a
generalization of the preorder traversal of a tree, is one important technique for doing
so. Depth-first search can serve as a skeleton around which many other efficient
graph algorithms can be built. The last two sections of this chapter contain several
algorithms that use depth-first search as a foundation.
Suppose we have a directed graph G in which all vertices are initially marked
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (17 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
unvisited. Depth-first search works by selecting one vertex v of G as a start vertex; v
is marked visited. Then each unvisited vertex adjacent to v is searched in turn, using
depth-first search recursively. Once all vertices that can be reached from v have been
visited, the search of v is complete. If some vertices remain unvisited, we select an
unvisited vertex as a new start vertex. We repeat this process until all vertices of G
have been visited.
This technique is called depth-first search because it continues searching in the
forward (deeper) direction as long as possible. For example, suppose x is the most
recently visited vertex. Depth-first search selects some unexplored arc x ® y
emanating from x. If y has been visited, the procedure looks for another unexplored
arc emanating from x. If y has not been visited, then the procedure marks y visited
and initiates a new search at y. After completing the search through all paths
beginning at y, the search returns to x, the vertex from which y was first visited. The
process of selecting unexplored arcs emanating from x is then continued until all arcs
from x have been explored.
An adjacency list L[v] can be used to represent the vertices adjacent to vertex v,
and an array mark, whose elements are chosen from (visited, unvisited), can be used
to determine whether a vertex has been previously visited. The recursive procedure
dfs is outlined in Fig. 6.24. To use it on an n-vertex graph, we initialize mark to
unvisited and then commence a depth-first search from each vertex that is still
unvisited when its turn comes, by
for v := 1 to n do
mark[v] := unvisited;
for v := 1 to n do
if mark[v] = unvisited then
dfs( v )
Note that Fig. 6.24 is a template to which we shall attach other actions later, as we
apply depth-first search. The code in Fig. 6.24 doesn't do anything but set the mark
array.
Analysis of Depth-First Search
All the calls to dfs in the depth-first search of a graph with e arcs and n £ e vertices
take O(e) time. To see why, observe that on no vertex is dfs called more than once,
because as soon as we call dfs(v) we set mark[v] to visited at line (1), and we never
call dfs on a vertex that previously had its mark set to visited. Thus, the total time
spent at lines (2)-(3) going down the adjacency lists is proportional to the sum of the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (18 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
lengths of those lists, that is, O(e). Thus, assuming n£e, the total time spent on the
depth- first search of an entire graph is O(e), which is, to within a constant factor, the
time needed merely to "look at" each arc.
procedure dfs ( v: vertex );
var
w: vertex;
begin
(1) mark[v]: = visited;
(2) for each vertex w on L[v] do
(3) if mark[w] = unvisited then
(4) dfs(w)
end; { dfs }
Fig. 6.24. Depth-first search.
Example 6.13. Assume the procedure dfs(v) is applied to the directed graph of Fig.
6.25 with v = A. The algorithm marks A visited and selects vertex B from the
adjacency list of vertex A. Since B is unvisited, the search continues by calling dfs(B).
The algorithm now marks B visited and selects the first vertex from the adjacency list
for vertex B. Depending on the order of the vertices on the adjacency list of B the
search will go to C or D next.
Assuming that C appears ahead of D, dfs(C) is invoked. Vertex A is on the
adjacency list of C. However, A is already visited at this point so the
Fig. 6.25. Directed graph.
search remains at C. Since all vertices on the adjacency list at C have now been
exhausted, the search returns to B, from which the search proceeds to D. Vertices A
and C on the adjacency list of D were already visited, so the search returns to B and
then to A.
At this point the original call of dfs(A) is complete. However, the digraph has not
been entirely searched; vertices E, F and G are still unvisited. To complete the
search, we can call dfs(E).
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (19 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
The Depth-first Spanning Forest
During a depth-first traversal of a directed graph, certain arcs, when traversed, lead to
unvisited vertices. The arcs leading to new vertices are called tree arcs and they form
a depth-first spanning forest for the given digraph. The solid arcs in Fig. 6.26 form a
depth-first spanning forest for the digraph of Fig. 6.25. Note that the tree arcs must
indeed form a forest, since a vertex cannot be unvisited when two different arcs to it
are traversed.
In addition to the tree arcs, there are three other types of arcs defined by a depthfirst
search of a directed graph. These are called back arcs, forward arcs, and cross
arcs. An arc such as C ® A is called a back arc, since it goes from a vertex to one of
its ancestors in the spanning forest. Note that an arc from a vertex to itself is a back
arc. A nontree arc that goes from a vertex to a proper descendant is called a forward
arc. There are no forward arcs in Fig. 6.26.
Arcs such as D ® C or G ® D, which go from a vertex to another vertex that is
neither an ancestor nor a descendant, are called cross arcs. Observe that all cross arcs
in Fig. 6.26 go from right to left, on the assumption that we add children to the tree in
the order they were visited, from left to right, and that we add new trees to the forest
from left to right. This pattern is not accidental. Had the arc G ® D been the arc D ®
G, then G would have been unvisited when the search at D was in progress, and thus
on encountering arc D ® G, vertex G would have been made a descendant of D, and
D ® G would have become a tree arc.
Fig. 6.26. Depth-first spanning forest for Fig. 6.25.
How do we distinguish among the four types of arcs? Clearly tree arcs are
special since they lead to unvisited vertices during the depth-first search. Suppose we
number the vertices of a directed graph in the order in which we first mark them
visited during a depth-first search. That is, we may assign to an array
dfnumber [v] := count;
count : = count + 1;
after line (1) of Fig. 6.24. Let us call this the depth-first numbering of a directed
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (20 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
graph; notice how depth-first numbering generalizes the preorder numbering
introduced in Section 3.1.
All descendants of a vertex v are assigned depth-first search numbers greater
than or equal to the number assigned v. In fact, w is a descendant of v if and only if
dfnumber(v) £ dfnumber(w) £ dfnumber(v) + number of descendants of v. Thus,
forward arcs go from low-numbered to high-numbered vertices and back arcs go
from high-numbered to low-numbered vertices.
All cross arcs go from high-numbered vertices to low-numbered vertices. To see
this, suppose that x ® y is an arc and dfnumber(x) £ dfnumber(y). Thus x is visited
before y. Every vertex visited between the time dfs(x) is first invoked and the time
dfs(x) is complete becomes a descendant of x in the depth-first spanning forest. If y is
unvisited at the time arc x ® y is explored, x ® y becomes a tree arc. Otherwise, x ®
y is a forward arc. Thus there can be no cross arc x ® y with dfnumber (x) £
dfnumber (y).
In the next two sections we shall show how depth- first search can be used in
solving various graph problems.
6.6 Directed Acyclic Graphs
A directed acyclic graph, or dag for short, is a directed graph with no cycles.
Measured in terms of the relationships they can represent, dags are more general than
trees but less general than arbitrary directed graphs. Figure 6.27 gives an example of
a tree, a dag, and a directed graph with a cycle.
Fig. 6.27. Three directed graphs.
Among other things, dags are useful in representing the syntactic structure of
arithmetic expressions with common subexpressions. For example, Fig. 6.28 shows a
dag for the expression
((a+b)*c + ((a+b)+e) * (e+f)) * ((a+b)*c)
The terms a+b and (a+b)*c are shared common subexpressions that are represented
by vertices with more than one incoming arc.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (21 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Dags are also useful in representing partial orders. A partial order R on a set S is
a binary relation such that
1. for all a in S, a R a is false (R is irreflexive)
2. for all a, b, c in S, if a R b and b R c, then a R c (R is transitive)
Two natural examples of partial orders are the "less than" (<) relation on
integers, and the relation of proper containment (Ì) on sets.
Example 6.14. Let S = {1, 2, 3} and let P(S) be the power set of S, that is, the set of
all subsets of S. P(S) = {Ø, {1}, {2}, {3}, {1,2}, {1,3}, {2,3}, {1,2,3}}. Ì is a partial
order on P(S). Certainly, A Ì A is false for any set A (irreflexivity), and if A Ì B and
B Ì C, then A Ì C (transitivity).
Dags can be used to portray partial orders graphically. To begin, we can view a
relation R as a set of pairs (arcs) such that (a, b) is in the set if and only if a R b is
true. If R is a partial order on a set S, then the directed graph G = (S ,R) is a dag.
Conversely, suppose G = (S ,R) is a dag and R+ is the relation defined by a R+ b if
and only if there is a path of length one or
Fig. 6.28. Dag for arithmetic expression.
more from a to b. (R+ is the transitive closure of the relation R.) Then, R+ is a partial
order on S.
Example 6.15. Figure 6.29 shows a dag (P(S), R), where S = {1, 2 ,3}. The relation
R+ is proper containment on the power set P(S).
Fig. 6.29. Dag of proper containments.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (22 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Test for Acyclicity
Suppose we are given a directed graph G = (V, E), and we wish to determine whether
G is acyclic, that is, whether G has no cycles. Depth-first search can be used to
answer this question. If a back arc is encountered during a depth-first search of G,
then clearly the graph has a cycle. Conversely, if a directed graph has a cycle, then a
back arc will always be encountered in any depth-first search of the graph.
To see this fact, suppose G is cyclic. If we do a depth-first search of G, there will
be one vertex v having the lowest depth-first search number of any vertex on a cycle.
Consider an arc u ® v on some cycle containing v. Since u is on the cycle, u must be
a descendant of v in the depth-first spanning forest. Thus, u ® v cannot be a cross
arc. Since the depth- first number of u is greater than the depth-first number of v, u ®
v cannot be a tree arc or a forward arc. Thus, u ® v must be a back arc, as illustrated
in Fig. 6.30.
Fig. 6.30. Every cycle contains a back arc.
Topological Sort
A large project is often divided into a collection of smaller tasks, some of which have
to be performed in certain specified orders so that we may complete the entire
project. For example, a university curriculum may have courses that require other
courses as prerequisites. Dags can be used to model such situations naturally. For
example, we could have an arc from course C to course D if C is a prerequisite for D.
Example 6.16. Figure 6.31 shows a dag giving the prerequisite structure on five
courses. Course C3, for example, requires courses C1 and C2 as prerequisites.
Fig. 6.31. Dag of prerequisites.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (23 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Topological sort is a process of assigning a linear ordering to the vertices of a
dag so that if there is an arc from vertex i to vertex j, then i appears before j in the
linear ordering. For example, C1, C2, C3, C4, C5 is a topological sort of the dag in
Fig. 6.31. Taking the courses in this sequence would satisfy the prerequisite structure
given in Fig. 6.31.
Topological sort can be easily accomplished by adding a print statement after
line (4) to the depth-first search procedure in Fig. 6.24:
procedure topsort ( v: vertex );
{ print vertices accessible from v in reverse topological order }
var
w: vertex;
begin
mark[v] := visited;
for each vertex w on L[v] do
if mark[w] = unvisited then
topsort(w) ;
writeln(v)
end; { topsort }
When topsort finishes searching all vertices adjacent to a given vertex x, it prints x.
The effect of calling topsort (v) is to print in a reverse topological order all vertices of
a dag accessible from v by a path in the dag.
This technique works because there are no back arcs in a dag. Consider what
happens when depth-first search leaves a vertex x for the last time. The only arcs
emanating from v are tree, forward, and cross arcs. But all these arcs are directed
towards vertices that have already been completely visited by the search and
therefore precede x in the order being constructed.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (24 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
6.7 Strong Components
A strongly connected component of a directed graph is a maximal set of vertices in
which there is a path from any one vertex in the set to any other vertex in the set.
Depth-first search can be used to determine strongly connected components of a
directed graph efficiently.
Let G = (V, E) be a directed graph. We can partition V into equivalence
classes Vi, 1£i£r, such that vertices v and w are equivalent if and only if there is a
path from v to w and a path from w to v. Let Ei, 1£i£r, be the set of arcs with head
and tail in Vi. The graphs Gi = (Vi, Ei) are called the strongly connected components
(or just strong components) of G. A directed graph with only one strong component
is said to be strongly connected.
Example 6.17. Figure 6.32 illustrates a directed graph with the two strong
components shown in Fig. 6.33.
Fig. 6.32. Directed graph.
Fig. 6.33. The strong components of the digraph of Fig. 6.32.
Note that every vertex of a directed graph G is in some strong component, but
that certain arcs may not be in any component. Such arcs, called cross-component
arcs, go from a vertex in one component to a vertex in another. We can represent the
interconnections among the components by constructing a reduced graph for G. The
vertices of the reduced graph are the strongly connected components of G. There is
an arc from vertex C to a different vertex C' of the reduced graph if there is an arc in
G from some vertex in the component C to some vertex in component C'. The
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (25 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
reduced graph is always a dag, because if there were a cycle, then all the components
in the cycle would really be one strong component, meaning that we didn't compute
strong components properly. Figure 6.34 shows the reduced graph for the digraph in
Figure 6.32.
We shall now present an algorithm to find the strongly connected
Fig. 6.34. Reduced graph.
components of a given directed graph G.
1. Perform a depth-first search of G and number the vertices in order of
completion of the recursive calls; i.e., assign a number to vertex v after line (4)
of Fig. 6.24.
2. Construct a new directed graph Gr by reversing the direction of every arc in
G.
3. Perform a depth-first search on Gr starting the search from the highestnumbered
vertex according to the numbering assigned at step (1). If the depthfirst
search does not reach all vertices, start the next depth-first search from
the highest-numbered remaining vertex.
4. Each tree in the resulting spanning forest is a strongly connected component
of G.
Example 6.18. Let us apply this algorithm to the directed graph of Fig. 6.32, starting
at a and progressing first to b. After step (1) we number the vertices as shown in Fig.
6.35. Reversing the direction of the arcs we obtain the directed graph Gr in Fig. 6.36.
Performing the depth-first search on Gr we obtain the depth-first spanning forest
shown in Fig. 6.37. We begin with a as a root, because a has the highest number.
From a we can only reach c and then b. The next tree has root d, since that is the
highest numbered (and only) remaining vertex. Each tree in this forest forms a
strongly connected component of the original directed graph.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (26 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
We have claimed that the vertices of a strongly connected component correspond
precisely to the vertices of a tree in the spanning forest of the second depth-first
search. To see why, observe that if v and w are vertices in the same strongly
connected component, then there are paths in G from v to w and from w to v. Thus
there are also paths from v to w and from w to v in Gr.
Suppose that in the depth-first search of Gr, we begin a search at some root x and
reach either v or w. Since v and w are reachable from each other,
Fig. 6.35. After step 1.
Fig. 6.36. Gr.
Fig. 6.37.Depth-first spanning forest for Gr.
both v and w will end up in the spanning tree with root x.
Now suppose v and w are in the same spanning tree of the depth-first spanning
forest of Gr. We must show that v and w are in the same strongly connected
component. Let x be the root of the spanning tree containing v and w. Since v is a
descendant of x, there exists a path in Gr from x to v. Thus there exists a path in G
from v to x.
In the construction of the depth-first spanning forest of Gr, vertex v was still
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (27 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
unvisited when the depth-first search at x was initiated. Thus x has a higher number
than v, so in the depth-first search of G, the recursive call at v terminated before the
recursive call at x did. But in the depth-first search of G, the search at v could not
have started before x, since the path in G from v to x would then imply that the search
at x would start and end before the search at v ended.
We conclude that in the search of G, v is visited during the search of x and hence
v is a descendant of x in the first depth-first spanning forest for G. Thus there exists a
path from x to v in G. Therefore x and v are in the same strongly connected
component. An identical argument shows that x and w are in the same strongly
connected component and hence v and w are in the same strongly connected
component, as shown by the path from v to x to w and the path from w to x to v.
Exercises
a. adjacency matrices,
b. linked adjacency lists, and
c. adjacency lists represented as in Fig. 6.5.
a. Use the algorithm Dijkstra to find the shortest paths from a to
the other vertices.
b. Use the algorithm Floyd to find the shortest distances between
all pairs of points. Also construct the matrix P that lets us
recover the shortest paths.
6.1
Represent the digraph of Fig. 6.38
a. by an adjacency matrix giving arc costs.
b. by a linked adjacency list with arc costs indicated.
Fig. 6.38. Directed graph with arc costs.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (28 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
6.2
Describe a mathematical model for the following scheduling problem.
Given tasks T1, T2, . . . , Tn, which require times t1, t2, . . . , tn to
complete, and a set of constraints, each of the form "Ti must be completed
prior to the start of Tj," find the minimum time necessary to complete all
tasks.
6.3
Implement the operations FIRST, NEXT, and VERTEX for digraphs
represented by
6.4 In the digraph of Fig. 6.38
6.5
Write a complete program for Dijkstra's algorithm using a partially
ordered tree as a priority queue and linked adjacency lists.
*6.6
Show that the program Dijkstra does not work correctly if arc costs can
be negative.
**6.7
Show that the program Floyd still works if some of the arcs have negative
cost but no cycle has negative cost.
6.8
Assuming the order of the vertices is a, b , . . . , f in Fig. 6.38, construct a
depth-first spanning forest; indicate the tree, forward, back and cross arcs,
and indicate the depth-first numbering of the vertices.
*6.9
Suppose we are given a depth-first spanning forest, and we list in
postorder each of the spanning trees (trees composed of spanning edges),
from the leftmost to the rightmost. Show that this order is the same as the
order in which the calls of dfs ended when the spanning forest was
constructed.
6.10
A root of a dag is a vertex r such that every vertex of the dag can be
reached by a directed path from r. Write a program to determine whether
a dag is rooted.
*6.11
Consider a dag with e arcs and with two distinguished vertices s and t.
Construct an O (e) algorithm to find a maximal set of vertex disjoint paths
from s to t. By maximal we mean no additional path may be added, but it
does not mean that it is the largest size such set.
6.12
Construct an algorithm to convert an expression tree with operators + and
* into a dag by sharing common subexpressions. What is the time
complexity of your algorithm?
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (29 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
6.13
Construct an algorithm for evaluating an arithmetic expression
represented as a dag.
6.14
Write a program to find the longest path in a dag. What is the time
complexity of your program?
6.15 Find the strong components of Fig. 6.38.
*6.16
Prove that the reduced graph of the strong components of Section 6.7
must be a dag.
6.17
Draw the first spanning forest, the reverse graph, and the second spanning
forest developed by the strong components algorithm applied to the
digraph of Fig. 6.38.
6.18 Implement the strong components algorithm discussed in Section 6.7.
*6.19
Show that the strong components algorithm takes time O (e) on a directed
graph of e arcs and n vertices, assuming n £ e.
*6.20
Write a program that takes as input a digraph and two of its vertices. The
program is to print all simple paths from one vertex to the other. What is
the time complexity of your program?
*6.21
A transitive reduction of a directed graph G = (V, E) is any graph G' with
the same vertices but with as few arcs as possible, such that the transitive
closure of G' is the same as the transitive closure of G. how that if G is a
dag, then the transitive reduction of G is unique.
*6.22
Write a program to compute the transitive reduction of a digraph. What is
the time complexity of your program?
*6.23
G' = (V, E') is called a minimal equivalent digraph for a digraph G = (V,
E) if E' is a smallest subset of E and the transitive closure of both G and
G' are the same. Show that if G is acyclic, then there is only one minimal
equivalent digraph, namely, the transitive reduction.
*6.24
Write a program to find a minimal equivalent digraph for a given digraph.
What is the time complexity of your program?
*6.25
Write a program to find the longest simple path from a given vertex of a
digraph. What is the time complexity of your program?
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (30 of 31) [1.7.2001 19:12:41]
Data Structures and Algorithms: CHAPTER 6: Directed Graphs
Bibliographic Notes
Berge [1958] and Harary [1969] are two good sources for additional material on
graph theory. Some books treating graph algorithms are Deo [1975], Even [1980],
and Tarjan [1983].
The single-source shortest paths algorithm in Section 6.3 is from Dijkstra [1959].
The all-pairs shortest paths algorithm is from Floyd [1962] and the transitive closure
algorithm is from Warshall [1962]. Johnson [1977] discusses efficient algorithms for
finding shortest paths in sparse graphs. Knuth [1968] contains additional material on
topological sort.
The strong components algorithm of Section 6.7 is similar to one suggested by
R. Kosaraju in 1978 (unpublished), and to one published by Sharir [1981]. Tarjan
[1972] contains another strong components algorithm that needs only one depth-first
search traversal.
Coffman [1976] contains many examples of how digraphs can be used to model
scheduling problems as in Exercise 6.2. Aho, Garey, and Ullman [1972] show that
the transitive reduction of a dag is unique, and that computing the transitive reduction
of a digraph is computationally equivalent to computing the transitive closure
(Exercises 6.21 and 6.22). Finding the minimal equivalent digraph (Exercises 6.23
and 6.24), on the other hand, appears to be computationally much harder; this
problem is NP-complete [Sahni (1974)].
† This is another manifestation of the old Pascal problem of doing insertion and
deletion at arbitrary positions of singly linked lists.
† One might expect that a more natural problem is to find the shortest path from the
source to one particular destination vertex. However, that problem appears just as
hard in general as the single-source shortest paths problem (except that by luck we
may find the path to the destination before some of the other vertices and thereby
terminate the algorithm a bit earlier than if we wanted paths to all the vertices).
‡ We might assume that an undirected graph would be used here, since the label on
arcs v ® w and w ® v would be the same. However, in fact, travel times are different
in different directions because of prevailing winds. Anyway, assuming that labels on
v ® w and w ® v are identical doesn't seem to help solve the problem.
Table of Contents Go to Chapter 7
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1206.htm (31 of 31) [1.7.2001 19:12:41]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_1.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_1.gif [1.7.2001 19:12:46]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_6.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_6.gif [1.7.2001 19:12:50]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_7.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_7.gif [1.7.2001 19:12:55]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_9.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_9.gif [1.7.2001 19:13:03]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_12.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_12.gif [1.7.2001 19:13:06]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_13.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_13.gif [1.7.2001 19:13:14]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_14.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_14.gif [1.7.2001 19:13:22]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_15.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_15.gif [1.7.2001 19:13:35]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_16.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_16.gif [1.7.2001 19:13:50]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_24.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_24.gif [1.7.2001 19:14:05]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_25.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_25.gif [1.7.2001 19:14:09]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_26.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_26.gif [1.7.2001 19:14:17]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_27.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_27.gif [1.7.2001 19:14:23]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_28.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_28.gif [1.7.2001 19:14:31]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_30.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_30.gif [1.7.2001 19:14:44]
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_31.gif
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/images/f5_31.gif [1.7.2001 19:15:09]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
Undirected Graphs
An undirected graph G = (V, E) consists of a finite set of vertices V and a set of edges
E. It differs from a directed graph in that each edge in E is an unordered pair of
vertices.† If (v, w) is an undirected edge, then (v, w) = (w, v). Hereafter we refer to an
undirected graph as simply a graph.
Graphs are used in many different disciplines to model a symmetric relationship
between objects. The objects are represented by the vertices of the graph and two
objects are connected by an edge if the objects are related. In this chapter we present
several data structures that can be used to represent graphs. We then present
algorithms for three typical problems involving undirected graphs: constructing
minimal spanning trees, biconnected components, and maximal matchings.
7.1 Definitions
Much of the terminology for directed graphs is also applicable to undirected graphs.
For example, vertices v and w are adjacent if (v, w) is an edge [or, equivalently, if (w,
v) is an edge]. We say the edge (v, w) is incident upon vertices v and w.
A path is a sequence of vertices v1, v2, . . . , vn such that (vi, vi+1) is an edge for 1
£ i < n. A path is simple if all vertices on the path are distinct, with the exception that
v1 and vn may be the same. The length of the path is n-1, the number of edges along
the path. We say the path v1, v2, . . . , vn connects v1 and vn. A graph is connected if
every pair of its vertices is connected.
Let G = (V, E) be a graph with vertex set V and edge set E. A subgraph of G is a
graph G' = (V', E') where
1. V' is a subset of V.
2. E' consists of edges (v, w) in E such that both v and w are in V'.
If E' consists of all edges (v, w) in E, such that both v and w are in V', then G' is called
an induced subgraph of G.
Example 7.1. In Fig. 7.1(a) we see a graph G = (V, E) with V = {a, b, c, d} and E =
{(a, b), (a, d), (b, c), (b, d), (c, d)}. In Fig. 7.1(b) we see one of its induced subgraphs,
the one defined by the set of vertices {a, b, c} and all the edges in Fig. 7.1(a) that are
not incident upon vertex d.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (1 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
Fig. 7.1. A graph and one of its subgraphs.
A connected component of a graph G is a maximal connected induced subgraph,
that is, a connected induced subgraph that is not itself a proper subgraph of any other
connected subgraph of G.
Example 7.2 Figure 7.1 is a connected graph. It has only one connected component,
namely itself. Figure 7.2 is a graph with two connected components.
Fig. 7.2. An unconnected graph.
A (simple) cycle in a graph is a (simple) path of length three or more that
connects a vertex to itself. We do not consider paths of the form v (path of length 0),
v, v (path of length 1), or v, w, v (path of length 2) to be cycles. A graph is cyclic if it
contains at least one cycle. A connected, acyclic graph is sometimes called a free tree.
Figure 7.2 shows a graph consisting of two connected components where each
connected component is a free tree. A free tree can be made into an ordinary tree if
we pick any vertex we wish as the root and orient each edge from the root.
Free trees have two important properties, which we shall use in the next section.
1. Every free tree with n ³ 1 vertices contains exactly n-1 edges.
2. If we add any edge to a free tree, we get a cycle.
We can prove (1) by induction on n, or what is equivalent, by an argument concerning
the "smallest counterexample." Suppose G = (V, E) is a counterexample to (1) with
the fewest vertices, say n vertices. Now n cannot be 1, because the only free tree on
one vertex has zero edges, and (1) is satisfied. Therefore, n must be greater than 1.
We now claim that in the free tree there must be some vertex with exactly one
incident edge. In proof, no vertex can have zero incident edges, or G would not be
connected. Suppose every vertex has at least two edges incident. Then, start at some
vertex v1, and follow any edge from v1. At each step, leave a vertex by a different
edge from the one used to enter it, thereby forming a path v1, v2, v3, . . ..
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (2 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
Since there are only a finite number of vertices in V, all vertices on this path
cannot be distinct; eventually, we find vi = vj for some i < j. We cannot have i = j-1
because there are no loops from a vertex to itself, and we cannot have i = j-2 or else
we entered and left vertex vi+1 on the same edge. Thus, i £ j-3, and we have a cycle vi,
vi+1, . . . , vj = vi. Thus, we have contradicted the hypothesis that G had no vertex with
only one edge incident, and therefore conclude that such a vertex v with edge (v, w)
exists.
Now consider the graph G' formed by deleting vertex v and edge (v, w) from G.
G' cannot contradict (1), because if it did, it would be a smaller counterexample than
G. Therefore, G' has n-1 vertices and n-2 edges. But G has one more edge and one
more vertex than G', so G has n-1 edges, proving that G does indeed satisfy (1). Since
there is no smallest counterexample to (1), we conclude there can be no
counterexample at all, so (1) is true.
Now we can easily prove statement (2), that adding an edge to a free tree forms a
cycle. If not, the result of adding the edge to a free tree of n vertices would be a graph
with n vertices and n edges. This graph would still be connected, and we supposed
that adding the edge left the graph acyclic. Thus we would have a free tree whose
vertex and edge count did not satisfy condition (1).
Methods of Representation
The methods of representing directed graphs can be used to represent undirected
graphs. One simply represents an undirected edge between v and w by two directed
edges, one from v to w and the other from w to v.
Example 7.3. The adjacency matrix and adjacency list representations for the graph
of Fig. 7.1(a) are shown in Fig. 7.3.
Clearly, the adjacency matrix for a graph is symmetric. In the adjacency list
representation if (i, j) is an edge, then vertex j is on the list for vertex i and vertex i is
on the list for vertex j.
Fig. 7.3. Representations.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (3 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
7.2 Minimum-Cost Spanning Trees
Suppose G = (V, E) is a connected graph in which each edge (u, v) in E has a cost c(u,
v) attached to it. A spanning tree for G is a free tree that connects all the vertices in V.
The cost of a spanning tree is the sum of the costs of the edges in the tree. In this
section we shall show how to find a minimum-cost spanning tree for G.
Example 7.4. Figure 7.4 shows a weighted graph and its minimum-cost spanning
tree.
A typical application for minimum-cost spanning trees occurs in the design of
communications networks. The vertices of a graph represent cities and the edges
possible communications links between the cities. The cost associated with an edge
represents the cost of selecting that link for the network. A minimum-cost spanning
tree represents a communications network that connects all the cities at minimal cost.
Fig. 7.4. A graph and spanning tree.
The MST Property
There are several different ways to construct a minimum-cost spanning tree. Many of
these methods use the following property of minimum-cost spanning trees, which we
call the MST property. Let G = (V, E) be a connected graph with a cost function
defined on the edges. Let U be some proper subset of the set of vertices V. If (u, v) is
an edge of lowest cost such that u Î U and v Î V-U, then there is a minimum-cost
spanning tree that includes (u, v) as an edge.
The proof that every minimum-cost spanning tree satisfies the MST property is
not hard. Suppose to the contrary that there is no minimum-cost spanning tree for G
that includes (u, v). Let T be any minimum-cost spanning tree for G. Adding (u, v) to
T must introduce a cycle, since T is a free tree and therefore satisfies property (2) for
free trees. This cycle involves edge (u, v). Thus, there must be another edge (u', v') in
T such that u' Î U and v' Î V-U, as illustrated in Fig. 7.5. If not, there would be no
way for the cycle to get from u to v without following the edge (u, v) a second time.
Deleting the edge (u', v') breaks the cycle and yields a spanning tree T' whose
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (4 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
cost is certainly no higher than the cost of T since by assumption c(u, v) £ c(u', v').
Thus, T' contradicts our assumption that there is no minimum-cost spanning tree that
includes (u, v).
Prim's Algorithm
There are two popular techniques that exploit the MST property to construct a
minimum-cost spanning tree from a weighted graph G = (V, E). One such method is
known as Prim's algorithm. Suppose V = {1, 2, . . . , n}. Prim's algorithm begins with
a set U initialized to {1}. It then "grows" a spanning tree, one edge at a time. At each
step, it finds a shortest edge (u, v) that connects U and V-U and then adds v, the vertex
in V-U, to U. It repeats this
Fig. 7.5. Resulting cycle.
step until U = V. The algorithm is summarized in Fig. 7.6 and the sequence of edges
added to T for the graph of Fig. 7.4(a) is shown in Fig. 7.7.
procedure Prim ( G: graph; var T: set of edges );
{ Prim constructs a minimum-cost spanning tree T for G }
var
U: set of vertices;
u, v: vertex;
begin
T:= Ø;
U := {1};
while U ¹ V do begin
let (u, v) be a lowest cost edge such that
u is in U and v is in V-U;
T := T È {(u, v)};
U := U È {v}
end
end; { Prim }
Fig. 7.6. Sketch of Prim's algorithm.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (5 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
One simple way to find the lowest-cost edge between U and V-U at each step is
to maintain two arrays. One array CLOSEST[i] gives the vertex in U that is currently
closest to vertex i in V-U. The other array LOWCOST[i] gives the cost of the edge (i,
CLOSEST[i]).
At each step we can scan LOWCOST to find the vertex, say k, in V-U that is
closest to U. We print the edge (k, CLOSEST[k]). We then update the LOWCOST and
CLOSEST arrays, taking into account the fact that k has been
Fig. 7.7. Seqeunces of edges added by Prim's algorithm.
added to U. A Pascal version of this algorithm is given in Fig. 7.8. We assume C is an
n x n array such that C[i, j] is the cost of edge (i, j). If edge (i, j) does not exist, we
assume C[i, j] is some appropriate large value.
Whenever we find another vertex k for the spanning tree, we make LOWCOST[k]
be infinity, a very large value, so this vertex will no longer be considered in
subsequent passes for inclusion in U. The value infinity is greater than the cost of any
edge or the cost associated with a missing edge.
The time complexity of Prim's algorithm is O(n2), since we make n-1 iterations
of the loop of lines (4)-(16) and each iteration of the loop takes O(n) time, due to the
inner loops of lines (7)-(10) and (13)-(16). As n gets large the performance of this
algorithm may become unsatisfactory. We now give another algorithm due to Kruskal
for finding minimum-cost spanning trees whose performance is at most O(eloge),
where e is the number of edges in the given graph. If e is much less than n2, Kruskal's
algorithm is superior, although if e is about n2, we would prefer Prim's algorithm.
procedure Prim ( C: array[l..n, 1..n] of
real );
{ Prim prints the edges of a minimum-cost spanning tree for a graph
with vertices {1, 2, . . . , n} and cost matrix C on edges }
var
LOWCOST: array[1..n] of real;
CLOSEST: array[1..n] of integer;
i, j, k, min; integer;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (6 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
{ i and j are indices. During a scan of the LOWCOST array,
k is the index of the closest vertex found so far, and
min = LOWCOST[k] }
begin
(1) for i := 2 to n do begin
{ initialize with only vertex 1 in the set U }
(2) LOWCOST[i] := C[1, i];
(3) CLOSEST[i] := 1
end;
(4) for i := 2 to n do begin
{ find the closest vertex k outside of U to
some vertex in U }
(5) min := LOWCOST[2];
(6) k := 2;
(7) for j := 3 to n do
(8) if LOWCOST[j] < min then begin
(9) min := LOWCOST[j];
(10) k := j
end;
(11) writeln(k, CLOSEST[k]); { print edge }
(12) LOWCOST[k] := infinity; { k is added to U }
(13) for j := 2 to n do { adjust costs to U }
(14) if (C[k, j] < LOWCOST[j]) and
(LOWCOST[j] < infinity) then begin
(15) LOWCOST[j] := C[k, j];
(16) CLOSEST[j] := k
end
end
end; { Prim }
Fig. 7.8. Prim's algorithm.
Kruskal's Algorithm
Suppose again we are given a connected graph G = (V, E), with V = {1, 2, . . . , n} and
a cost function c defined on the edges of E. Another way to construct a minimum-cost
spanning tree for G is to start with a graph T = (V, Ø) consisting only of the n vertices
of G and having no edges. Each vertex is therefore in a connected component by
itself. As the algorithm proceeds, we shall always have a collection of connected
components, and for each component we shall have selected edges that form a
spanning tree.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (7 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
To build progressively larger components, we examine edges from E, in order of
increasing cost. If the edge connects two vertices in two different connected
components, then we add the edge to T. If the edge connects two vertices in the same
component, then we discard the edge, since it would cause a cycle if we added it to
the spanning tree for that connected component. When all vertices are in one
component, T is a minimum-cost spanning tree for G.
Example 7.5. Consider the weighted graph of Fig. 7.4(a). The sequence of edges
added to T is shown in Fig. 7.9. The edges of cost 1, 2, 3, and 4 are considered first,
and all are accepted, since none of them causes a cycle. The edges (1, 4) and (3, 4) of
cost 5 cannot be accepted, because they connect vertices in the same component in
Fig. 7.9(d), and therefore would complete a cycle. However, the remaining edge of
cost 5, namely (2, 3), does not create a cycle. Once it is accepted, we are done.
We can implement this algorithm using sets and set operations discussed in
Chapters 4 and 5. First, we need a set consisting of the edges in E. We then apply the
DELETEMIN operator repeatedly to this set to select edges in order of increasing
cost. The set of edges therefore forms a priority queue, and a partially ordered tree is
an appropriate data structure to use here.
We also need to maintain a set of connected components C. The operations we
apply to it are:
1. MERGE(A, B, C) to merge the components A and B in C and to call the result
either A or B arbitrarily.†
2. FIND(v, C) to return the name of the component of C of which vertex v is a
member. This operation will be used to determine whether the two vertices of
an edge are in the same or different components.
3. INITIAL(A, v, C) to make A the name of a component in C containing only
vertex v initially.
These are the operations of the MERGE-FIND ADT called MFSET, which we
encountered in Section 5.5. A sketch of a program called Kruskal to find a minimumcost
spanning tree using these operations is shown in Fig. 7.10.
We can use the techniques of Section 5.5 to implement the operations used in this
program. The running time of this program is dependent on two factors. If there are e
edges, it takes O(eloge) time to insert the edges into the priority queue.‡ In each
iteration of the while-loop, finding the least cost
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (8 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
Fig. 7.9. Sequence of edges added by Kruskal's algorithm.
edge in edges takes O(log e) time. Thus, the priority queue operations take O(e log e)
time in the worst case. The total time required to perform the MERGE and FIND
operations depends on the method used to implement the MFSET. As shown in
Section 5.5, there are O(e log e) and O(e a (e)) methods. In either case, Kruskal's
algorithm can be implemented to run in O(e log e) time.
7.3 Traversals
In a number of graph problems, we need to visit the vertices of a graph
systematically. Depth-first search and breadth-first search, the subjects of this section,
are two important techniques for doing this. Both techniques can be used to determine
efficiently all vertices that are connected to a given vertex.
procedure Kruskal ( V: SET of vertex;
E: SET of edges;
var T: SET of edges );
var
ncomp: integer; { current number of components }
edges: PRIORITYQUEUE; { the set of edges }
components: MFSET; { the set V grouped into
a MERGE-FIND set of components }
u, v: vertex;
e: edge;
nextcomp: integer; { name for new component }
ucomp, vcomp: integer; { component names }
begin
MAKENULL(T);
MAKENULL(edges);
nextcomp := 0;
ncomp := number of members of V;
for v in V do begin { initialize a
component to contain one vertex of V }
nextcomp := nextcomp + 1;
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (9 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
INITIAL( nextcomp, v, components)
end;
for e in E do { initialize priority queue of edges }
INSERT(e, edges);
while ncomp > 1 do begin { consider next edge }
e := DELETEMIN(edges);
let e = (u, v);
ucomp := FIND(u, components);
vcomp := FIND(v, components);
if ucomp <> vcomp then begin
{ e connects two different components }
MERGE (ucomp, vcomp, components);
ncomp := ncomp - 1;
INSERT(e, T)
end
end
end; { Kruskal }
Fig. 7.10. Kruskal's algorithm.
Depth-First Search
Recall from Section 6.5 the algorithm dfs for searching a directed graph. The same
algorithm can be used to search undirected graphs, since the undirected edge (v, w)
may be thought of as the pair of directed edges v ® w and w ® v.
In fact, the depth-first spanning forests constructed for undirected graphs are even
simpler than for digraphs. We should first note that each tree in the forest is one
connected component of the graph, so if a graph is connected, it has only one tree in
its depth-first spanning forest. Second, for digraphs we identified four kinds of arcs:
tree, forward, back, and cross. For undirected graphs there are only two kinds: tree
edges and back edges.
Because there is no distinction between forward edges and backward edges for
undirected graphs, we elect to refer to them all as back edges. In an undirected graph,
there can be no cross edges, that is, edges (v, w) where v is neither an ancestor nor
descendant of w in the spanning tree. Suppose there were. Then let v be a vertex that
is reached before w in the search. The call to dfs(v) cannot end until w has been
searched, so w is entered into the tree as some descendant of v. Similarly, if dfs(w) is
called before dfs(v), then v becomes a descendant of w.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (10 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
As a result, during a depth-first search of an undirected graph G, all edges
become either
1. tree edges, those edges (v, w) such that dfs(v) directly calls dfs(w) or vice
versa, or
2. back edges, those edges (v, w) such that neither dfs(v) nor dfs(w) called the
other directly, but one called the other indirectly (i.e., dfs(w) calls dfs (x),
which calls dfs (v), so w is an ancestor of v).
Example 7.6. Consider the connected graph G in Fig. 7.11(a). A depth-first spanning
tree T resulting from a depth-first search of G is shown in Fig. 7.11(b). We assume
the search began at vertex a, and we have adopted the convention of showing tree
edges solid and back edges dashed. The tree has been drawn with the root on the top
and the children of each vertex have been drawn in the left-to-right order in which
they were first visited in the procedure of dfs.
To follow a few steps of the search, the procedure dfs(a) calls dfs(b) and adds
edge (a, b) to T since b is not yet visited. At b, dfs calls dfs(d) and adds edge (b, d) to
T. At d, dfs calls dfs(e) and adds edge (d, e) to T. At e, vertices a, b, and d are marked
visited so dfs(e) returns without adding any edges to T. At d, dfs now sees that
vertices a and b are marked visited so dfs(d) returns without adding any more edges to
T. At b, dfs now sees that the remaining adjacent vertices a and e are marked visited,
so dfs(b) returns. The search then continues to c, f, and g.
Fig. 7.11. A graph and its depth-first search.
Breadth-First Search
Another systematic way of visiting the vertices is called breadth-first search. The
approach is called "breadth-first" because from each vertex v that we visit we search
as broadly as possible by next visiting all the vertices adjacent to v. We can also apply
this strategy of search to directed graphs.
As for depth-first search, we can build a spanning forest when we perform a
breadth-first search. In this case, we consider edge (x, y) a tree edge if vertex y is first
visited from vertex x in the inner loop of the search procedure bfs of Fig. 7.12.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (11 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
It turns out that for the breadth-first search of an undirected graph, every edge
that is not a tree edge is a cross edge, that is, it connects two vertices neither of which
is an ancestor of the other.
The breadth-first search algorithm given in Fig. 7.12 inserts the tree edges into a
set T, which we assume is initially empty. Every entry in the array mark is assumed to
be initialized to the value unvisited; Figure 7.12 works on one connected component.
If the graph is not connected, bfs must be called on a vertex of each component. Note
that in a breadth-first search we must mark a vertex visited before enqueuing it, to
avoid placing it on the queue more than once.
Example 7.7. The breadth-first spanning tree for the graph G in Fig. 7.11(a) is shown
in Fig. 7.13. We assume the search began at vertex a. As before, we have shown tree
edges solid and other edges dashed. We have also drawn the tree with the root at the
top and the children in the left-to-right order in which they were first visited.
The time complexity of breadth-first search is the same as that of depthprocedure
bfs ( v );
{ bfs visits all vertices connected to v using breadth-first search }
var
Q: QUEUE of vertex;
x, y: vertex;
begin
mark[v] := visited;
ENQUEUE(v, Q);
while not EMPTY(Q) do begin
x := FRONT(Q);
DEQUEUE(Q);
for each vertex y adjacent to x do
if mark[y] = unvisited then begin
mark[y] := visited;
ENQUEUE(y, Q);
INSERT((x, y), T)
end
end
end; { bfs }
Fig. 7.12. Breadth-first search.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (12 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
Fig. 7.13. Breadth-first search of G.
first search. Each vertex visited is placed in the queue once, so the body of the while
loop is executed once for each vertex. Each edge (x, y) is examined twice, once from
x and once from y. Thus, if a graph has n vertices and e edges, the running time of bfs
is O(max(n, e)) if we use an adjacency list representation for the edges. Since e ³ n is
typical, we shall usually refer to the running time of breadth-first search as O(e), just
as we did for depth-first search.
Depth-first search and breadth-first search can be used as frameworks around
which to design efficient graph algorithms. For example, either method can be used to
find the connected components of a graph, since the connected components are the
trees of either spanning forest.
We can test for cycles using breadth-first search in O(n) time, where n is the
number of vertices, independent of the number of edges. As we discussed in Section
7.1, any graph with n vertices and n or more edges must have a cycle. However, a
graph could have n-1 or fewer edges and still have a cycle, if it had two or more
connected components. One sure way to find the cycles is to build a breadth-first
spanning forest. Then, every cross edge (v, w) must complete a simple cycle with the
tree edges leading to v and w from their closest common ancestor, as shown in Fig.
7.14.
Fig. 7.14 A cycle found by breadth-first search.
7.4 Articulation Points and Biconnected
Components
An articulation point of a graph is a vertex v such that when we remove v and all
edges incident upon v, we break a connected component of the graph into two or
more pieces. For example, the articulation points of Fig. 7.11(a) are a and c. If we
delete a, the graph, which is one connected component, is divided into two triangles:
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (13 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
{b, d, e} and {c, f, g}. If we delete c, we divide the graph into {a, b, d, e} and {f, g}.
However, if we delete any one of the other vertices from the graph of Fig. 7.11(a), we
do not split the connected component. A connected graph with no articulation points
is said to be biconnected. Depth-first search is particularly useful in finding the
biconnected components of a graph.
The problem of finding articulation points is the simplest of many important
problems concerning the connectivity of graphs. As an example of applications of
connectivity algorithms, we may represent a communication network as a graph in
which the vertices are sites to be kept in communication with one another. A graph
has connectivity k if the deletion of any k-1 vertices fails to disconnect the graph. For
example, a graph has connectivity two or more if and only if it has no articulation
points, that is, if and only if it is biconnected. The higher the connectivity of a graph,
the more likely the graph is to survive the failure of some of its vertices, whether by
failure of the processing units at the vertices or external attack.
We shall here give a simple depth-first search algorithm to find all the
articulation points of a connected graph, and thereby test by their absence whether the
graph is biconnected.
1. Perform a depth-first search of the graph, computing dfnumber [v] for each
vertex v as discussed in Section 6.5. In essence, dfnumber orders the vertices
as in a preorder traversal of the depth-first spanning tree.
2. For each vertex v, compute low [v], which is the smallest dfnumber of v or of
any vertex w reachable from v by following down zero or more tree edges to a
descendant x of v (x may be v) and then following a back edge (x, w). We
compute low [v] for all vertices v by visiting the vertices in a postorder
traversal. When we process v, we have computed low [y] for every child y of v.
We take low [v] to be the minimum of
a. dfnumber [v],
b. dfnumber [z] for any vertex z for which there is a back
edge (v, z) and
c. low [y] for any child y of v.
l Now we find the articulation points as follows.
a. The root is an articulation point if and only if it has two or more
children. Since there are no cross edges, deletion of the root
must disconnect the subtrees rooted at its children, as a
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (14 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
disconnects {b, d, e} from {c, f, g} in Fig. 7.11(b).
b. A vertex v other than the root is an articulation point if and only
if there is some child w of v such that low [w] ³ dfnumber [v]. In
this case, v disconnects w and its descendants from the rest of
the graph. Conversely, if low [w] < dfnumber [v], then there
must be a way to get from w down the tree and back to a proper
ancestor of v (the vertex whose dfnumber is low [w]), and
therefore deletion of v does not disconnect w or its descendants
from the rest of the graph.
Example 7.8. dfnumber and low are computed for the graph of Fig. 7.11(a) in Fig.
7.15. As an example of the calculation of low, our postorder traversal visits e first. At
e, there are back edges (e, a) and (e, b), so low [e] is set to min(dfnumber [e],
dfnumber [a], dfnumber [b]) = 1. Then d is visited, and low [d] is set to the minimum
of dfnumber [d], low [e], and dfnumber [a ]. The second of these arises because e is a
child of d and the third because of the back edge (d, a).
Fig. 7.15. Depth-first and low numberings.
After computing low we consider each vertex. The root, a, is an articulation point
because it has two children. Vertex c is an articulation point because it has a child f
with low [f] ³ dfnumber [c]. The other vertices are not articulation points.
The time taken by the above algorithm on a graph of e edges and n £ e vertices is
O(e). The reader should check that the time spent in each of the three phases can be
attributed either to the vertex visited or to an edge emanating from that vertex, there
being only a constant amount of time attributable to any vertex or edge in any pass.
Thus, the total time is O(n+e), which is O(e) under the assumption n £ e.
7.5 Graph Matching
In this section we outline an algorithm to solve "matching problems" on graphs. A
simple example of a matching problem occurs when we have a set of teachers to
assign to a set of courses. Each teacher is qualified to teach certain courses but not
others. We wish to assign a course to a qualified teacher so that no two teachers are
assigned the same course. With certain distributions of teachers and courses it is
impossible to assign every teacher a course; in those situations we wish to assign as
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (15 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
many teachers as possible.
We can represent this situation by a graph as in Fig. 7.16 where the vertices are
divided into two sets V1 and V2, such that vertices in the set V1 represent teachers and
vertices in the set V2 courses. That teacher v is qualified to teach course w is
represented by an edge (v, w). A graph such as this whose vertices can be divided into
two disjoint groups with each edge having one end in each group is called bipartite.
Assigning a teacher a course is equivalent to selecting an edge between a teacher
vertex and a course vertex.
Fig. 7.16. A bipartite graph.
The matching problem can be formulated in general terms as follows. Given a
graph G =(V, E), a subset of the edges in E with no two edges incident upon the same
vertex in V is called a matching. The task of selecting a maximum subset of such
edges is called the maximal matching problem. The heavy edges in Fig. 7.16 are an
example of one maximal matching in that graph. A complete matching is a matching
in which every vertex is an endpoint of some edge in the matching. Clearly, every
complete matching is a maximal matching.
There is a straightforward way to find maximal matchings. We can systematically
generate all matchings and then pick one that has the largest number of edges. The
difficulty with this method is that it has a running time that is an exponential function
of the number of edges.
There are more efficient algorithms for finding maximal matchings. These
algorithms generally use a technique known as "augmenting paths." Let M be a
matching in a graph G. A vertex v is matched if it is the endpoint of an edge in M. A
path connecting two unmatched vertices in which alternate edges in the path are in M
is called an augmenting path relative to M. Observe that an augmenting path must be
of odd length, and must begin and end with edges not in M. Also observe that given
an augmenting path P we can always find a bigger matching by removing from M
those edges that are in P, and then adding to M the edges of P that were initially not
in M. This new matching is M Å P where Å denotes "exclusive or" on sets. That is,
the new matching consists of those edges that are in M or P, but not in both.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (16 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
Example 7.9. Figure 7.17(a) shows a graph and a matching M consisting of the heavy
edges (1, 6), (3, 7), and (4, 8). The path 2, 6, 1, 8, 4, 9 in Fig. 7.17(b) is an
augmenting path relative to M. Figure 7.18 shows the matching (l, 8), (2, 6), (3, 7), (4,
9) obtained by removing from M those edges that are in the path, and then adding to
M the other edges in the path.
Fig. 7.17. A matching and an augmenting path.
The key observation is that M is a maximal matching if and only if there is no
augmenting path relative to M. This observation is the basis of our maximal matching
algorithm.
Suppose M and N are matchings with ½M½ < ½N½. (½M½ denotes the number of
edges in M.) To see that M Å N contains an augmenting path relative to M consider
the graph G' = (V, M Å N). Since M and N are both matchings, each vertex of V is an
endpoint of at most one edge from M and an endpoint of at most one edge from N.
Thus each connected component of G' forms a simple path (possibly a cycle) with
edges alternating between M and N. Each path that is not a cycle is either an
augmenting path relative to M or an augmenting path relative to N depending on
whether it has more edges from N or
Fig. 7.18. The larger matching.
from M. Each cycle has an equal number of edges from M and N. Since ½M½ < ½N½,
M Å N has more edges from N than M, and hence has at least one augmenting path
relative to M.
We can now outline our procedure to find a maximal matching M for a graph G =
(V, E).
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1207.htm (17 of 22) [1.7.2001 19:15:36]
Data Structures and Algorithms: CHAPTER 7: Undirected Graphs
1. Start with M = Ø.
2. Find an augmenting path P relative to M and replace M by M Å P.
3. Repeat step (2) until no further augmenting paths exist, at which point M is a
maximal matching.
It remains only to show how to find an augmenting path relative to a matching M. We
shall do this for the simpler case where G is a bipartite graph with vertices partitioned
into sets V1 and V2. We shall build an augmenting path graph for G for levels i = 0, 1,
2, . . . using a process similar to breadth-first search. Level 0 consists of all
unmatched vertices from V1. At odd level i, we add new vertices that are adjacent to a
vertex at level i-1, by a non-matching edge, and we also add that edge. At even level
i, we add new vertices that are adjacent to a vertex at level i-1 because of an edge in
the matching M, together with that edge.
We continue building the augmenting path graph level-by-level until an
unmatched vertex is added at an odd level, or until no more vertices can be added. If
there is an augmenting path relative to M, an unmatched vertex v will eventually be
added at an odd level. The path from v to any vertex at level 0 is an augmenting path
relative to M.
Example 7.10. Figure 7.19 illustrates the augmenting path graph for the graph in Fig.
7.17(a) relative to the matching in Fig. 7.18, in which we have chosen vertex 5 as the
unmatched vertex at level 0. At level 1 we add the non-matching edge (5, 6). At level
2 we add the matching edge (6, 2). At level 3 we can add either of the non-matching
edges (2, 9) or (2, 10). Since both vertices 9 and 10 are currently unmatched, we can
terminate the construction of the augmenting path graph after the addition of either
one of these vertices. Both paths 9, 2, 6, 5 and 10, 2, 6, 5 are augmenting paths
relative to the malowback
end
end
end
end; { insert 1 }
Fig. 5.19. The procedure insert 1.
Now we can write the procedure INSERT, which calls insert 1. If insert 1
"returns" a new node, then INSERT must create a new root. The code is shown in
Fig. 5.20 on the assumption that the type SET is ­ twothreenode, i.e., a pointer to the
root of a 2-3 tree whose leaves contain the members of the set.
procedure INSERT ( x: elementtype; var S: SET );
var
pback: ­ twothreenode; { pointer to new node
returned by insert 1 }
lowback: real; { low value in subtree of pback }
saveS: SET; { place to store a temporary copy of the pointer S }
begin
{ checks for S being empty or a single node should occur here,
and an appropriate insertion procedure should be included }
insert 1(S, x, pback , lowback ) ;
if pback < > nil then begin
{ create new root; its children are now pointed to by S and pback }
saveS := S;
new(S);
S ­.firstchild := saveS;
S ­.secondchild := pback;
S ­.lowofsecond := lowback;
S ­.thirdchild := nil;
end
end; { INSERT }
Fig. 5.20. INSERT for sets represented by 2-3 trees.
Implementation of DELETE
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (25 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
We shall sketch a function delete 1 that takes a pointer to a node node and an element
x, and deletes a leaf descended from node having value x, if there is one.† Function
delete 1 returns true if after deletion node has only one child, and it returns false if
node still has two or three children. A sketch of the code for delete 1 is shown in Fig.
5.21.
We leave the detailed code for function delete 1 for the reader. Another exercise is
to write a procedure DELETE(S, x) that checks for the special cases that the set S
consists only of a single leaf or is empty, and otherwise calls delete 1(S, x); if delete 1
returns true, the procedure removes the root (the node pointed to by S) and makes S
point to its lone child.
function delete 1 ( node : ­ twothreenode;
x: elementtype ) : boolean;
var
onlyone: boolean; { to hold the value returned by a call to delete 1 }
begin
delete 1 := false;
if the children of node are leaves then begin
if x is among those leaves then begin
remove x;
shift children of node to the right of x one position left;
if node now has one child then
delete 1 := true
end
end
else begin { node is at level two or higher }
determine which child of node could have x as a descendant;
onlyone := delete 1(w, x); ( w stands for node
­.firstchild,
node ­.secondchild, or node
­.thirdchild, as appropriate }
if onlyone then begin ( fix children of node )
if w is the first child of node then
if y, the second child of node, has three children then
make the first child of y be the second child of w
else begin { y has two children }
make the child of w be the first child of y;
remove w from among the children of node;
if node now has one child then
delete 1 := true
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (26 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
end;
if w is the second child of node then
if y, the first child of node, has three children then
make the third child of y be the first child of w
else { y has two children }
if z, the third child of node, exists
and has three children then
make first child of z be the second child of w
else begin { no other child of node has three children }
make the child of w be the third child of y;
remove w from among the children of node;
if node now has one child then
delete 1 := true
end;
if w is the third child of node then
if y, the second child of node, has three children then
make the third child of y be the first child of w
else begin { y has two children }
make the child of w be the third child of y;
remove w from among the children of node
end { note node surely has two children left in this case }
end
end
end; { delete 1 }
Fig. 5.21. Recursive deletion procedure.
5.5 Sets with the MERGE and FIND
Operations
In certain problems we start with a collection of objects, each in a set by itself; we
then combine sets in some order, and from time to time ask which set a particular
object is in. These problems can be solved using the operations MERGE and FIND.
The operation MERGE(A, B, C) makes C equal to the union of sets A and B, provided
A and B are disjoint (have no member in common); MERGE is undefined if A and B
are not disjoint. FIND(x) is a function that returns the set of which x is a member; in
case x is in two or more sets, or in no set, FIND is not defined.
Example 5.6. An equivalence relation is a reflexive, symmetric, and transitive
relation. That is, if º is an equivalence relation on set S, then for any (not necessarily
distinct) members a, b, and c in S, the following properties hold:
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (27 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
1. a º a (reflexivity)
2. If a º b, then b º a (symmetry).
3. If a º b and b º c, then a º c (transitivity).
The relation "is equal to" (=) is the paradigm equivalence relation on any set S.
For a, b, and c in S, we have (1) a = a, (2) if a = b, then b = a, and (3) if a = b and b =
c, then a = c. There are many other equivalence relations, however, and we shall
shortly see several additional examples.
In general, whenever we partition a collection of objects into disjoint groups, the
relation a º b if and only if a and b are in the same group is an equivalence relation.
"Is equal to" is the special case where every element is in a group by itself.
More formally, if a set S has an equivalence relation defined on it, then the set S
can be partitioned into disjoint subsets S1, S2, ..., called equivalence classes, whose
union is S. Each subset Si consists of equivalent members of S. That is, a º b for all a
and b in Si, and a ?? b if a and b are in different subsets. For example, the relation
congruence modulo n† is an equivalence relation on the set of integers. To check that
this is so, note that a-a = 0, which is a multiple of n (reflexivity); if a-b = dn, then b-a
= (-d)n (symmetry); and if a-b = dn and b-c = en, then a-c = (d+e)n (transitivity). In
the case of congruence modulo n there are n equivalence classes, which are the set of
integers congruent to 0, the set of integers congruent to 1, ... , the set of integers
congruent to n - 1.
The equivalence problem can be formulated in the following manner. We are
given a set S and a sequence of statements of the form "a is equivalent to b." We are
to process the statements in order in such a way that at any time we are able to
determine in which equivalence class a given element belongs. For example, suppose
S = {1, 2, ... , 7} and we are given the sequence of statements
1º2 5º6 3º4 1º4
to process. The following sequence of equivalence classes needs to be constructed,
assuming that initially each element of S is in an equivalence class by itself.
1º2 {1,2} {3} {4} {5} {6} {7}
5º6 {1,2} {3} {4} {5,6} {7}
3º4 {1,2} {3,4} {5,6} {7}
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (28 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
1º4 {1,2,3,4} {5,6} {7}
We can "solve" the equivalence problem by starting with each element in a named
set. When we process statement aºb, we FIND the equivalence classes of a and b and
then MERGE them. We can at any time use FIND to tell us the current equivalence
class of any element.
The equivalence problem arises in several areas of computer science. For example,
one form occurs when a Fortran compiler has to process "equivalence declarations"
such as
EQUIVALENCE (A(1),B(1,2),C(3)), (A(2),D,E), (F,G)
Another example, presented in Chapter 7, uses solutions to the equivalence
problem to help find minimum-cost spanning trees.
A Simple Implementation of MFSET
Let us begin with a simplified version of the MERGE-FIND ADT. We shall define an
ADT, called MFSET, consisting of a set of subsets, which we shall call components,
together with the following operations:
1. MERGE(A, B) takes the union of the components A and B and calls the result
either A or B, arbitrarily.
2. FIND(x) is a function that returns the name of the component of which x is a
member.
3. INITIAL(A, x) creates a component named A that contains only the element x.
To make a reasonable implementation of MFSET, we must restrict our underlying
types, or alternatively, we should recognize that MFSET really has two other types as
"parameters" - the type of set names and the type of members of these sets. In many
applications we can use integers as set names. If we take n to be the number of
elements, we may also use integers in the range [1..n] for the members of
components. For the implementation we have in mind, it is important that the type of
set members be a subrange type, because we want to index into an array defined over
that subrange. The type of set names is not important, as this type is the type of array
elements, not their indices. Observe, however, that if we wanted the member type to
be other than a subrange type, we could create a mapping, with a hash table, for
example, that assigned these to unique integers in a subrange. We only need to know
the total number of elements in advance.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (29 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
The implementation we have in mind is to declare
const
n = { number of elements };
type
MFSET = array[1..n] of integer;
as a special case of the more general type
array[subrange of members] of (type of set names);
Suppose we declare components to be of type MFSET with the intention that
components[x] holds the name of the set currently containing x. Then the three
MFSET operations are easy to write. For example, the operation MERGE is shown in
Fig. 5.22. INITIAL(A, x) simply sets components[x] to A, and FIND(x) returns
components [x].
The time performance of this implementation of MFSET is easy to
procedure MERGE ( A, B: integer; var C: MFSET );
var
x: 1..n;
begin
for x:= 1 to n do
if C[x] = B then
C[x] := A
end; { MERGE }
Fig. 5.22. The procedure MERGE.
analyze. Each execution of the procedure MERGE takes O(n) time. On the other
hand, the obvious implementations of INITIAL(A, x) and FIND(x) have constant
running times.
A Faster Implementation of MFSET
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (30 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Using the algorithm in Fig. 5.22, a sequence of n-1 MERGE instructions will take
O(n2) time.† One way to speed up the MERGE operation is to link together all
members of a component in a list. Then, instead of scanning all members when we
merge component B into A, we need only run down the list of members of B. This
arrangement saves time on the average. However, it could happen that the ith merge is
of the form MERGE(A, B) where A is a component of size 1 and B is a component of
size i, and that the result is named A. This merge operation would require O(i) steps,
and a sequence of n-1 such merge instructions would take on the order of
time.
One way to avoid this worst case situation is to keep track of the size of each
component and always merge the smaller into the larger.‡ Thus, every time a member
is merged into a bigger component, it finds itself in a component at least twice as big.
Thus, if there are initially n components, each with one member, none of the n
members can have its component changed more than 1+log n times. As the time spent
by this new version of MERGE is proportional to the number of members whose
component names are changed, and the total number of such changes is at most
n(1+log n), we see that O(n log n) work suffices for all merges.
Now let us consider the data structure needed for this implementation. First, we
need a mapping from set names to records consisting of
1. a count giving the number of members in the set and
2. the index in the array of the first element of that set.
We also need another array of records, indexed by members, to indicate
1. the set of which each element is a member and
2. the next array element on the list for that set.
We use 0 to serve as NIL, the end-of-list marker. In a language that lent itself to such
constructs, we would prefer to use pointers in this array, but Pascal does not permit
pointers into arrays.
In the special case where set names, as well as members, are chosen from the
subrange 1..n, we can use an array for the mapping described above. That is, we
define
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (31 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
type
nametype: 1..n;
elementtype = 1..n;
MFSET = record
setheaders: array[ 1..n] of record
{ headers for set lists }
count: 0..n;
firstelement: 0..n
end;
names; array[1..n] of record
{ table giving set containing each member }
setname: nametype;
nextelement: 0..n
end
end;
The procedures INITIAL, MERGE, and FIND are shown in Fig. 5.23.
Figure 5.24 shows an example of the data structure used in Fig. 5.23, where set 1
is {1, 3, 4}, set 2 is {2}, and set 5 is {5, 6}.
A Tree Implementation of MFSET's
Another, completely different, approach to the implementation of MFSET's uses trees
with pointers to parents. We shall describe this approach informally. The basic idea is
that nodes of trees correspond to set members, with an array or other implementation
of a mapping leading from set members to their nodes. Each node, except the root of
each tree, has a pointer to its parent. The roots hold the name of the set, as well as an
element. A mapping from set names to roots allows access to any given set, when
merges are done.
Figure 5.25 shows the sets A = {1, 2, 3, 4}, B = {5, 6}, and C = {7} represented in
this form. The rectangles are assumed to be part of the root node, not separate nodes.
procedure INITIAL ( A: nametype; x: elementtype; var C:
MFSET );
{ initialize A to a set containing x only }
begin
C.names [x].setname : = A;
C.names [x].nextelement := 0;
{ null pointer at end of list of members of A }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (32 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
C.setheaders [A].count := 1;
C.setheaders [A].firstelement := x
end; { INITIAL }
procedure MERGE ( A, B: nametype; var C: MFSET );
{ merge A and B, calling the result A or B, arbitrarily }
var
i: 0..n; { used to find end of smaller list }
begin
if C.setheaders [A ].count > C.setheaders
[B].count then begin
{ A is the larger set; merge B into A }
{ find end of B, changing set names to A as we go }
i := C.setheaders[B].firstelement;
while C.names [i].nextelement <> 0 do
begin
C.names [i].setname := A;
i := C.names [i].nextelement
end;
{ append list A to the end of B and call the result A }
{ now i is the index of the last member of B }
C.names [i].setname := A;
C.names [i].nextelement := C.setheaders [A
].firstelement;
C.setheaders [A ].firstelement := C.setheaders [B
].firstelement;
C.setheaders [A ].count := C.setheaders [A
].count +
C.setheaders [B ].count;
C.setheaders[B ].count := 0;
C.setheaders[B ].firstelement := 0
{ above two steps not really necessary, as set B no longer exists }
end
else { B is at least as large as A }
{ code similar to case above, but with A and B interchanged }
end; { MERGE }
function FIND ( x: 1..n; var C: MFSET );
{ return the name of the set of which x is a member }
begin
return ( C. names [x ].setname)
end; { FIND }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (33 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Fig. 5.23. The operations of an MFSET.
To find the set containing an element x, we first consult a mapping (e.g., an array)
not shown in Fig. 5.25, to obtain a pointer to the node for x. We then follow the path
from that node to the root of its tree and read the name of the set there.
The basic merge operation is to make the root of one tree be a child of the root of
the other. For example, we could merge A and B of Fig. 5.25 and call the result A, by
making node 5 a child of node 1. The result is shown in Fig. 5.26. However,
indiscriminate merging could result in a tree of n nodes that is a single chain. Then
doing a FIND operation on each of those nodes would take O(n2) time. Observe that
although a merge can be done in O(1) steps, the cost of a reasonable number of
FIND's will dominate the total cost, and this approach is not necessarily better than
the simplest one for executing n merges and n finds.
However, a simple improvement guarantees that if n is the number of elements,
then no FIND will take more than O(log n) steps. We simply keep at each root a
count of the number of elements in the set, and when called upon to merge two sets,
we make the root of the smaller tree be a child of the root of the larger. Thus, every
time a node is moved to a new tree, two things happen: the distance from the node to
its root increases by one, and the node will be in a set with at least twice as many
elements as before. Thus, if n is the total number of elements, no node can be moved
more than logn times; hence, the distance to its root can never exceed logn. We
conclude that each FIND requires at most O(log n) time.
Path Compression
Another idea that may speed up this implementation of MFSET's is path
compression. During a FIND, when following a path from some node to the root,
make each node encountered along the path be a child of the root. The easiest way to
do this is in two passes. First, find the root, and then retraverse the same path, making
each node a child of the root.
Example 5.7. Figure 5.27(a) shows a tree before executing a FIND operation on the
node for element 7 and Fig. 5.27(b) shows the result after 5 and 7 are made children
of the root. Nodes 1 and 2 on the path are not moved because 1 is the root, and 2 is
already a child of the root.
Path compression does not affect the cost of MERGE's; each MERGE still takes a
constant amount of time. There is, however, a subtle speedup in FIND's since path
compression tends to shorten a large number of paths from various nodes to the root
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (34 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
with relatively little effort.
Unfortunately, it is very difficult to analyze the average cost of FIND's when path
compression is used. It turns out that if we do not require that smaller trees be merged
into larger ones, we require no more than O(n log n) time to do n FIND's. Of course,
the first FIND may take O(n) time by itself for a tree consisting of one chain. But
path compression can change a tree very rapidly and no matter in what order we
apply FIND to elements of any tree no more than O(n) time is spent on n FIND's.
However, there are
Fig. 5.24. Example of the MFSET data structure.
Fig. 5.25. MFSET represented by a collection of trees.
sequences of MERGE and FIND instructions that require W(nlog n) time.
The algorithm that both uses path compression and merges the smaller tree into
the larger is asymptotically the most efficient method known for implementing
MFSET's. In particular, n FIND's require no more than O(na(n)) time, where a(n) is
a function that is not constant, yet grows much more slowly than logn. We shall
define a(n) below, but the analysis that leads to this bound is beyond the scope of this
book.
Fig. 5.26. Merging B into A.
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (35 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Fig. 5.27. An example of path compression.
The Function a(n)
The function a(n) is closely related to a very rapidly growing function A(x, y), known
as Ackermann's function. A(x, y) is defined recursively by:
A(0,y) = 1 for y ³ 0
A(1, 0) = 2
A(x, 0) = x+2 for x ³ 2
A(x,y) = A(A(x-1, y),y-1) for x, y ³ 1
Each value of y defines a function of one variable. For example, the third line above
tells us that for y=0, this function is "add 2." For y = 1, we have A(x, 1) = A(A(x-1,
1),0) = A(x-1, 1) + 2, for x > 1, with A(1, 1) = A(A(0, 1),0) = A(1, 0) = 2. Thus A(x, 1)
= 2x for all x ³ 1. In other words, A(x, 1) is "multiply by 2." Then, A(x, 2) = A(A(x-1,
2), 1) = 2A(x-1, 2) for x > 1. Also, A(1, 2) = A(A(0,2), 1) = A(1, 1) = 2. Thus A(x, 2) =
2x. Similarly, we can show that A(x, 3) = 22...2 (stack of x 2's), while A(x, 4) is so
rapidly growing there is no accepted mathematical notation for such a function.
A single-variable Ackermann's function can be defined by letting A(x) = A(x, x).
The function a(n) is a pseudo-inverse of this single variable function. That is, a(n) is
the least x such that n £ A(x). For example, A(1) = 2, so a(1) = a(2) = 1. A(2) = 4, so
a(3) = a(4) = 2. A(3) = 8, so a(5) = . . . = a(8) = 3. So far, a(n) seems to be growing
rather steadily.
However, A(4) is a stack of 65536 2's. Since log(A(4)) is a stack of 65535 2's, we
cannot hope even to write A(4) explicitly, as it would take log(A(4)) bits to do so.
Thus a(n) £ 4 for all integers n one is ever likely to encounter. Nevertheless, a(n)
eventually reaches 5, 6, 7, . . . on its unimaginably slow course toward infinity.
5.6 An ADT with MERGE and SPLIT
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (36 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
Let S be a set whose members are ordered by the relation <. The operation SPLIT(S,
S1, S2, x) partitions S into two sets: S1={ a ½ a is in S and a < x} and S2 = {a ½ a is in
S and a ³ x}. The value of S after the split is undefined, unless it is one of S1 or S2.
There are several situations where the operation of splitting sets by comparing each
member with a fixed value x is essential. We shall consider one such problem here.
The Longest Common Subsequence
Problem
A subsequence of a sequence x is obtained by removing zero or more (not necessarily
contiguous) elements from x. Given two sequences x and y, a longest common
subsequence (LCS) is a longest sequence that is a subsequence of both x and y.
For example, an LCS of 1, 2, 3, 2, 4, 1, 2 and 2, 4, 3, 1, 2, 1 is the subsequence 2,
3, 2, 1, formed as shown in Fig. 5.28. There are other LCS's as well, such as 2, 4, 1, 2,
but there are no common subsequences of length 5.
Fig. 5.28. A longest common subsequence.
There is a UNIX command called diff that compares files line-by-line, finding a
longest common subsequence, where a line of a file is considered an element of the
subsequence. That is, whole lines are analogous to the integers 1, 2, 3, and 4 in Fig.
5.28. The assumption behind the command diff is that the lines of each file that are
not in this LCS are lines inserted, deleted or modified in going from one file to the
other. For example, if the two files are versions of the same program made several
days apart, diff will, with high probability, find the changes.
There are several general solutions to the LCS problem that work in O(n2) steps
on sequences of length n. The command diff uses a different strategy that works well
when the files do not have too many repetitions of any line. For example, programs
will tend to have lines "begin" and "end" repeated many times, but other lines are not
likely to repeat.
The algorithm used by diff for finding an LCS makes use of an efficient
implementation of sets with operations MERGE and SPLIT, to work in time
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (37 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
O(plogn), where n is the maximum number of lines in a file and p is the number of
pairs of positions, one from each file, that have the same line. For example, p for the
strings in Fig. 5.28 is 12. The two 1's in each string contribute four pairs, the 2's
contribute six pairs, and 3 and 4 contribute one pair each. In the worst case, p,. could
be n2, and this algorithm would take O(n2logn) time. However, in practice, p is
usually closer to n, so we can expect an O(nlogn) time complexity.
To begin the description of the algorithm let A = a1a2 × × × an and B = b1b2 × × × bm
be the two strings whose LCS we desire. The first step is to tabulate for each value a,
the positions of the string A at which a appears. That is, we define PLACES(a)= { i ½
a = ai }. We can compute the sets PLACES(a) by constructing a mapping from
symbols to headers of lists of positions. By using a hash table, we can create the sets
PLACES(a) in O(n) "steps" on the average, where a "step" is the time it takes to
operate on a symbol, say to hash it or compare it with another. This time could be a
constant if symbols are characters or integers, say. However, if the symbols of A and
B are really lines of text, then steps take an amount of time that depends on the
average length of a line of text.
Having computed PLACES(a) for each symbol a that occurs in string A, we are
ready to find an LCS. To simplify matters, we shall only show how to find the length
of the LCS, leaving the actual construction of the LCS as an exercise. The algorithm
considers each bj, for j = 1, 2, ... , m, in turn. After considering bj, we need to know,
for each i between 0 and n, the length of the LCS of strings a1 × × × ai and b1 × × × bj.
We shall group values of i into sets Sk, for k = 0, 1, . . . , n, where Sk consists of all
those integers i such that the LCS of a1 × × × ai and b1 × × × bj has length k. Note that Sk
will always be a set of consecutive integers, and the integers in Sk+1 are larger than
those in Sk, for all k.
Example 5.8. Consider Fig. 5.28, with j = 5. If we try to match zero symbols from
the first string with the first five symbols of the second (24312), we naturally have an
LCS of length 0, so 0 is in S0. If we use the first symbol from the first string, we can
obtain an LCS of length 1, and if we use the first two symbols, 12, we can obtain an
LCS of length 2. However, using 123, the first three symbols, still gives us an LCS of
length 2 when matched against 24312. Proceeding in this manner, we discover S0 =
{0}, S1 = {1}, S2 = {2, 3}, S3 = {4, 5, 6}, and S4 = {7}.
Suppose that we have computed the Sk's for position j-1 of the second string and
we wish to modify them to apply to position j. We consider the set PLACES(bj). For
each r in PLACES(bj), we consider whether we can improve some of the LCS's by
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (38 of 47) [1.7.2001 19:09:32]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
adding the match between ar and bj to the LCS of a1 × × × ar- 1 and b1 × × × bj. That is, if
both r-1 and r are in Sk, then all s ³ r in Sk really belong in Sk+1 when bj is
considered. To see this we observe that we can obtain k matches between a1 × × × ar-1
and bl × × × bj-1, to which we add a match between ar and bj. We can modify Sk and
Sk+1 by the following steps.
1. FIND(r) to get Sk.
2. If FIND(r-1) is not Sk, then no benefit can be had by matching bj with ar. Skip
the remaining steps and do not modify Sk or Sk+1.
3. If FIND(r-1) = Sk, apply SPLIT(Sk, Sk, S'k, r) to separate from Sk those
members greater than or equal to r.
4. MERGE(S'k, Sk+1, Sk+1) to move these elements into Sk+1.
It is important to consider the members of PLACES(bj) largest first. To see why,
suppose for example that 7 and 9 are in PLACES(bj), and before bj is considered,
S3={6, 7, 8, 9} and S4={10, 11}.
If we consider 7 before 9, we split S3 into S3 = {6} and S'3 = {7, 8, 9}, then make
S4 = {7, 8, 9, 10, 11}. If we then consider 9, we split S4 into S4={7, 8} and S'4 = {9,
10, 11}, then merge 9, 10 and 11 into S5. We have thus moved 9 from S3 to S5 by
considering only one more position in the second string, representing an
impossibility. Intuitively, what has happened is that we have erroneously matched bj
against both a7 and a9 in creating an imaginary LCS of length 5.
In Fig. 5.29, we see a sketch of the algorithm that maintains the sets Sk as we scan
the second string. To determine the length of an LCS, we need only execute FIND(n)
at the end.
procedure LCS;
begin
(1) initialize S0 = {0, 1, ... , n} and Si = ­ for i = 1, 2, ... , n;
(2) for j := 1 to n do { compute Sk's for position
j }
(3) for r in PLACES(bj), largest first do begin
(4) k := FIND(r);
(5) if k = FIND(r-1) then begin { r is not
smallest in Sk }
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (39 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
(6) SPLIT(Sk, Sk, S'k,
r);
(7) MERGE(S'k, Sk+1,
Sk+1)
end
end
end; { LCS }
Fig. 5.29. Sketch of longest common subsequence program.
Time Analysis of the LCS Algorithm
As we mentioned earlier, the algorithm of Fig. 5.29 is a useful approach only if there
are not too many matches between symbols of the two strings. The measure of the
number of matches is
where |PLACES(bj)| denotes the number of elements in set PLACES(bj). In other
words, p is the sum over all bj of the number of positions in the first string that match
bj. Recall that in our discussion of file comparison, we expect p to be of the same
order as m and n, the lengths of the two strings (files).
It turns out that the 2-3 tree is a good structure for the sets Sk. We can initialize
these sets, as in line (1) of Fig. 5.29, in O(n) steps. The FIND operation requires an
array to serve as a mapping from positions r to the leaf for r and also requires
pointers to parents in the 2-3 tree. The name of the set, i.e., k for Sk, can be kept at the
root, so we can execute FIND in O(logn) steps by following parent pointers until we
reach the root. Thus all executions of lines (4) and (5) together take O(plogn) time,
since those lines are each executed exactly once for each match found.
The MERGE operation of line (5) has the special property that every member of
S'k is lower than every member of Sk+1, and we can take advantage of this fact when
using 2-3 trees for an implementation.† To begin the MERGE, place the 2-3 tree for
S'k to the left of that for Sk+1. If both are of the same height, create a new root with
the roots of the two trees as children. If S'k is shorter, insert the root of that tree as the
http://www.ourstillwaters.org/stillwaters/csteaching/DataStructuresAndAlgorithms/mf1205.htm (40 of 47) [1.7.2001 19:09:33]
Data Structures and Algorithms: CHAPTER 5: Advanced Set Representation Methods
leftmost child of the leftmost node of Sk+1 at the appropriate level. If this node now
has four children, we modify the tree exactly as in the INSERT procedure of Fig.
5.20. An example is shown in Fig. 5.30. Similarly, if Sk+1 is shorter, make its root the
rightmost child of the rightmost node of S'k at the appropriate level.
Fig. 5.30. Example of MERGE.
The SPLIT operation at r requires that we travel up the tree from leaf r,
duplicating every interior node along the path and giving one copy to each of the two
resulting trees. Nodes with no children are eliminated, and nodes with one child are
removed and have that child inserted into the proper tree at the proper level.
Example 5.9. Suppose we split the tree of Fig. 5.30(b) at node 9. The two trees, with
duplicated nodes, are shown in Fig. 5.31(a). On the left, the parent of 8 has only one
child, so 8 becomes a child of the parent of 6 and 7. This parent now has three
children, so all is as it should be; if it had four children, a new node would have been
created and inserted
